{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 94-775/95-865: Topic Modeling with Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "\n",
    "# data: 20 news groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load in 10,000 posts from the 20 Newsgroups dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "# randomly take 10,000 articles from the 20 news groups\n",
    "num_articles = 10000\n",
    "# Loaded into data, data is a list of strings\n",
    "data = fetch_20newsgroups(shuffle=True, random_state=0,\n",
    "                          remove=('headers', 'footers', 'quotes')).data[:num_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there are 10,000 posts, and we can look at an example post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is a list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Koberg,\n",
      "\n",
      "\tJust a couple of minor corrections here...\n",
      "\n",
      "\t1)  The Churches of Christ do not usually believe in speaking in\n",
      "tongues, in fact many of them are known for being strongly opposed to\n",
      "Pentecostal teaching.  You are probably thinking of Church of God in\n",
      "Christ, the largest African-American Pentecostal denomination.\n",
      "\n",
      "\t2)  I'm not sure what you mean by \"signifying believers\"  but it\n",
      "should be pointed out that the Assemblies of God does not now, nor has it\n",
      "ever, held that speaking in tongues is the sign that one is a Christian. \n",
      "The doctrine that traditional Pentecostals (including the A/G) maintain is\n",
      "that speaking in tongues is the sign of a second experience after becoming\n",
      "a Christian in which one is \"Baptized in the Holy Spirit\"  That may be\n",
      "what you were referring to, but I point this out because Pentecostals are\n",
      "frequently labeled as believing that you have to speak in tongues in order\n",
      "to be a Christian.  Such a position is only held by some groups and not the\n",
      "majority of Pentecostals.   Many Pentecostals will quote the passage in\n",
      "Mark 16 about \"these signs following them that believe\" but they generally\n",
      "do not interpret this as meaning if you don't pactice the signs you aren't\n",
      "\"saved\".\n",
      "\n",
      "\t3)  I know it's hard to summarize the beliefs of a movement that\n",
      "has such diversity, but I think you've made some pretty big\n",
      "generalizations here.  Do \"Neo-Pentecostals\" only believe in tongues as a\n",
      "sign and tongues as prayer but NOT tongues as revelatory with a message? \n",
      "I've never heard of that before.  In fact I would have characterized them\n",
      "as believing the same as Pentecostals except less likely to see tongues as\n",
      "a sign of Spirit Baptism.  Also, while neo-Pentecostals may not be\n",
      "inclined to speak in tongues in the non-Pentecostal churches they attend,\n",
      "they do have their own meetings and, in many cases, a whole church will be\n",
      "charismatic.\n"
     ]
    }
   ],
   "source": [
    "# you can take a look at what individual documents look like by replacing what index we look at\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit a `CountVectorizer` model that will compute, for each post, what its raw word count histograms are (the \"term frequencies\" we saw in week 1).\n",
    "\n",
    "The output of the following cell is the term-frequencies matrix, where rows index different posts/text documents, and columns index 1000 different vocabulary words. A note about the arguments to `CountVectorizer`:\n",
    "\n",
    "- `max_df`: we only keep words that appear in at most this fraction of the documents\n",
    "- `min_df`: we only keep words that appear in at least this many documents\n",
    "- `stop_words`: whether to remove stop words\n",
    "- `max_features`: among words that don't get removed due to the above 3 arguments, we keep the top `max_features` number of most frequently occuring words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 1000\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer does tokenization and can remove terms that occur too frequently, not frequently enough, or that are stop words\n",
    "\n",
    "# countvectorizer is scikit learns way of doing the histogram thing we did with space: less manual control than spacy, but faster\n",
    "# itll build the feature vectores (document-word matrix, where each row is a ...)\n",
    "# parameters: max_df filters for the words were keeping track of:words that show up in more than 95% of document, we throw away, not\n",
    "# semantically interesting; we can use it as an int (50: then every word appearing in omre than 50 docs, goes away)\n",
    "\n",
    "# min_df: words in less than 2 documents, we throw it away\n",
    "\n",
    "# stop_words: discard from scikit english stopwords \n",
    "\n",
    "# max_features: here is 1,000: after I do the max_df, min_df and stopwords filetrs, I look at how many words I have left: if its greater than\n",
    "# max_features, I only keep the most popular. BUT, if the words left after the 3 filters is less than max_features, then it will be ignored. \n",
    "\n",
    "# document frequency (df) means number of documents a word appears in\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "                                min_df=2,\n",
    "                                stop_words='english',\n",
    "                                max_features=vocab_size)\n",
    "# tf: term frequency vectorizer\n",
    "\n",
    "# the fitting is learning the vocabulary: goes thru all the data and know the words it needs to keep track of\n",
    "# transform: Ive done the fitting, Ill go back to data points and convert each one into a feat vector representation using the vocab i learnt during fitting\n",
    "tf_fit = tf_vectorizer.fit(data)\n",
    "tf = tf_vectorizer.fit_transform(data)\n",
    "\n",
    "# I get a 2D table: raw count histogram\n",
    "\n",
    "# After transform: It has learnt what the vocabulary should be, from the data, but we havent done LDA yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 254416 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature vector representation\n",
    "tf[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492568"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().flatten().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0:10].toarray().flatten().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0:10].toarray().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00', '000', '02', '03', '04', '0d', '0t', '10', '100', '11']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['00', '000', '02', '03', '04', '0d', '0t', '10', '100', '11'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_fit.get_feature_names_out()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  980,   631,   239,   195,   310,   230,   454,  1513,   492,\n",
       "         783,   857,   202,   598,   856,   751,   927,   906,   602,\n",
       "         605,   447,   193,   195,   353,   657,   617,   198,   471,\n",
       "         972,   225,   452,   396,   395,   634,   898,   197,   355,\n",
       "         380,   354,   268,   383,   325,   750,   198,   286,   482,\n",
       "         313,   430,   399,   317,   227,   232,   231,   200,   191,\n",
       "         403,   547,   185,   217,   214,   235,   413,   182,   188,\n",
       "         722,   225,   428,   322,   200,   187,   224,   320,   325,\n",
       "         261,   421,   284,   204,   284,   213,   363,   496,   196,\n",
       "         891,   190,   668,   228,   247,   480,   270,   271,   245,\n",
       "         761,   317,   212,   567,   283,   324,   205,   496,   408,\n",
       "         228,   342,   282,   243,   309,   199,   207,   222,   523,\n",
       "         216,   413,   463,   226,   316,   368,   195,   253,   324,\n",
       "         420,   265,   191,   262,   200,   197,   194,   476,   275,\n",
       "         418,   189,   279,   373,   190,   557,   456,   183,   200,\n",
       "         564,   191,   516,   385,   245,   301,   204,   244,   188,\n",
       "         199,   234,   186,  1181,   239,   222,   557, 56383,  1078,\n",
       "         591,   219,   769,   238,   182,   198,   228,  1208,   831,\n",
       "        1037,   208,   431,   477,   529,   295,  1091,   238,   187,\n",
       "         322,   198,   207,   367,   351,   573,   294,   225,   359,\n",
       "         231,   202,   182,   194,   189,   341,   285,   317,   341,\n",
       "         298,   401,   227,   398,   209,   198,   770,   182,   561,\n",
       "         223,   717,   735,   259,   409,   230,   253,   968,   295,\n",
       "         425,   301,   436,   395,   402,   217,   558,   218,   183,\n",
       "         421,   203,   191,   506,   610,   226,   223,   349,   497,\n",
       "         204,   352,   420,   225,   370,   378,   206,   210,   342,\n",
       "         233,   248,   355,   282,   601,   490,  1206,   937,   427,\n",
       "         288,   261,   283,   203,   185,   361,   205,   305,   291,\n",
       "         263,   244,   726,   265,   229,   267,   376,   280,   450,\n",
       "         204,   210,   222,   676,   296,   355,   349,   420,   286,\n",
       "         363,   363,   841,   264,   259,   260,   223,   311,   184,\n",
       "         462,   560,   296,   253,   297,  1134,   301,   189,   397,\n",
       "         883,   481,   619,   190,   338,   345,   342,   184,   203,\n",
       "         292,   203,   226,   223,   233,   258,   237,   261,   241,\n",
       "         269,   196,  1573,  1001,   348,   836,   212,   225,   206,\n",
       "         332,   320,   597,   454,   194,   242,   195,  2452,   936,\n",
       "         598,  3454,   188,   640,   214,   264,  1015,   370,   326,\n",
       "         340,   202,   311,   365,   235,   185,   310,   215,  2122,\n",
       "         306,   212,   698,   513,   763,   225,   242,   228,   430,\n",
       "         189,   319,   183,   340,   201,   207,   462,   345,   732,\n",
       "         233,   329,   226,   195,   278,   336,   218,   223,   215,\n",
       "         317,   889,   358,   184,   258,   417,   770,   378,   219,\n",
       "         301,   289,   233,   185,   266,   435,   243,   232,  1619,\n",
       "         761,   250,   229,   344,   229,   213,   201,   236,   713,\n",
       "         248,   356,   313,   357,   648,   205,   264,   717,   242,\n",
       "         193,   315,  1108,   764,   511,   196,   248,   666,   224,\n",
       "         293,   550,   218,   613,   197,   387,   201,   220,   246,\n",
       "        1719,   335,  1267,  2240,  1033,   290,  1111,   579,   776,\n",
       "         210,   262,   714,   385,   369,   671,   320,   256,   277,\n",
       "         469,   270,   310,   184,   784,   413,   293,   652,   335,\n",
       "         346,   308,   552,   188,   297,  1073,   418,   792,   243,\n",
       "         416,   293,   350,   234,   534,   478,   185,   376,   274,\n",
       "         478,   348,   210,   578,   241,   262,   666,   317,   195,\n",
       "         501,   490,   296,   375,   495,   247,   629,  1345,   275,\n",
       "         205,   211,   410,   189,   187,   490,   328,   303,   203,\n",
       "         257,   461,   278,   595,   502,   320,   492,   244,   793,\n",
       "         321,   507,   192,   285,   221,   502,   269,  3320,  1060,\n",
       "         184,   375,   301,   337,   530,   220,  3160,   263,   424,\n",
       "         259,   213,   224,   247,   509,   193,   486,   932,   298,\n",
       "         348,   200,   254,   616,   282,   985,   219,   363,   254,\n",
       "         727,   322,  3568,   375,   203,   807,   352,   854,   892,\n",
       "         435,   183,   221,  1196,   535,   997,   259,   987,   235,\n",
       "         611,   279,   189,   295,   772,   214,   348,   405,   186,\n",
       "         542,   459,   250,  1083,   249,   351,  1827,   472,   400,\n",
       "         628,   281,   223,   321,   208,   192,   182,   191,   454,\n",
       "        4140,   613,   248,   664,   209,   595,   227,   278,   266,\n",
       "         504,   358,   185,   262,   530,   206,   184,   341,   285,\n",
       "         393,   348,   364,   304,   226,   523,   301,   188,   288,\n",
       "         209,   183,   354,   259,   880,   359,   203,   417,   552,\n",
       "         229,   305,   271,  1446,   295,   298,   488,   290,  2340,\n",
       "         557,   239,   225,   368,   292,   714,   238,   629,   191,\n",
       "        1172,   358,   189,   183,   203,   318,   306,   199,   327,\n",
       "         319,   762,   321,   522,   266,   192,   182,   694,   200,\n",
       "         217,   482,   330,   523,   261,   490,   220,   212,   344,\n",
       "         303,   206,   303,   274,   313,   568,   208,  3676,   187,\n",
       "         263,   423,   649,   304,   437,   215,   189,   205,   184,\n",
       "         745,   670,   214,   235,   524,   185,   247,   335,   305,\n",
       "        1195,   337,   370,   236,   240,   253,   240,   320,   756,\n",
       "         188,   687,   343,   446,   941,   212,   273,   608,   377,\n",
       "         473,   188,   475,   204,   351,   322,   206,   904,  1368,\n",
       "         656,   315,   237,  1263,   473,   205,   215,   407,   186,\n",
       "         519,   790,   211,   283,   184,   276,   964,   530,   584,\n",
       "         251,   292,   183,   342,  1014,   316,   745,  1265,   614,\n",
       "         198,   208,   238,   208,   288,   240,   239,   265,   183,\n",
       "         304,   251,   386,   274,   526,   268,   256,   196,   318,\n",
       "         221,   239,   187,   493,   185,   285,   332,   282,   251,\n",
       "         377,  1642,   442,   263,   225,   231,   183,   301,   788,\n",
       "         520,   335,   288,   209,  1597,   307,   283,   260,   381,\n",
       "        1653,   463,   616,   308,   245,   426,   231,   375,   590,\n",
       "         189,   419,   825,   222,   401,   238,   438,   594,   327,\n",
       "         309,   781,   338,   305,   252,   295,   560,   390,   902,\n",
       "         211,   198,   337,   230,   187,   406,   369,   439,   191,\n",
       "         415,   282,   215,   292,   397,   525,   283,   865,   207,\n",
       "         213,   310,   306,   368,   344,   211,   607,   321,   202,\n",
       "         196,  1011,   292,   287,   496,   188,   341,   536,   564,\n",
       "         415,   836,   297,   534,   213,   345,   186,   320,   255,\n",
       "         184,   214,   310,   303,   477,   606,   213,   484,   768,\n",
       "         192,   197,  1040,   560,   340,   214,   283,   369,   323,\n",
       "         260,   229,   222,   719,   239,   246,   404,   802,   264,\n",
       "         193,   336,   351,   245,  1111,   236,   918,  1145,  2695,\n",
       "         257,   611,  2700,   608,   229,   183,   525,   517,   423,\n",
       "         199,   261,   191,   202,   417,   874,   346,   790,   582,\n",
       "         244,   423,   307,   216,   523,   259,   421,   214,   234,\n",
       "         331,   586,   335,   375,   227,  2784,  1680,   224,   220,\n",
       "         412,   335,   372,  1268,   348,   373,   203,   208,   302,\n",
       "        1653,   792,   193,   430,   310,   207,   227,   302,   190,\n",
       "        1530,   359,   230,   486,   291,   283,   182,   227,  1796,\n",
       "         208,   267,   361,   236,   522,   341,   212,   323,   244,\n",
       "         456,   679,  1047,   213,   347,   316,   567,   505,   344,\n",
       "        1430,   190,   468,   496,   948,   289,   374,   428,   191,\n",
       "         343,   572,   189,   209,   208,  1299,  1258,   634,   265,\n",
       "         284])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().sum(axis  = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ribarragi/miniconda3/envs/UDA_RIG/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_vectorizer.get_feature_names()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2152"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf[:, tf_fit.vocabulary_['just']].toarray().flatten() > 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_fit.vocabulary_['just']  #index for just\n",
    "tf[:, tf_fit.vocabulary_['just']].toarray() # all rows just this one column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2152"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "(tf[:, tf_vectorizer.vocabulary_['just']].toarray().flatten() > 0).sum() # this gives you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_vectorizer.vocabulary_ # gives youi index of each word kept in the tf table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that there are 10,000 rows (corresponding to posts), and 1000 columns (corresponding to words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape\n",
    "# 10,000 posts\n",
    "# 1,000 words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note about the `tf` matrix: this actually is stored as what's called a sparse matrix (rather than a 2D NumPy array that you're more familiar with). The reason is that often these matrices are really large and the vast majority of entries are 0, so it's possible to save space by not storing where the 0's are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf)\n",
    "# this is not a numpy array, but a scipy matrix: keep only the non zero entries of the matrix: stored as a sparse matrix, or ill get a memory error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To convert `tf` to a 2D NumPy table, you can run `tf.toarray()` (this does not modify the original `tf` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf.toarray())\n",
    "# here I can covnert into array: a dense matrix, because the shape is not that large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what words the different columns correspond to by using the `get_feature_names()` function; the output is in the same order as the column indices. In particular, we can index into the following list (i.e., so given a column index, we can figure out which word it corresponds to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '02', '03', '04', '0d', '0t', '10', '100', '11', '12', '128', '13', '14', '145', '15', '16', '17', '18', '19', '1990', '1991', '1992', '1993', '1d9', '1st', '1t', '20', '200', '21', '22', '23', '24', '25', '250', '26', '27', '28', '29', '2di', '2tm', '30', '300', '31', '32', '33', '34', '34u', '35', '36', '37', '38', '39', '3d', '3t', '40', '41', '42', '43', '44', '45', '46', '48', '50', '500', '55', '60', '64', '6um', '70', '75', '75u', '7ey', '80', '800', '86', '90', '91', '92', '93', '9v', 'a86', 'ability', 'able', 'ac', 'accept', 'access', 'according', 'act', 'action', 'actually', 'add', 'addition', 'address', 'administration', 'advance', 'age', 'ago', 'agree', 'ah', 'air', 'al', 'algorithm', 'allow', 'allowed', 'alt', 'america', 'american', 'analysis', 'anonymous', 'answer', 'answers', 'anti', 'anybody', 'apparently', 'appears', 'apple', 'application', 'applications', 'appreciate', 'appreciated', 'approach', 'appropriate', 'apr', 'april', 'archive', 'area', 'areas', 'aren', 'argument', 'armenia', 'armenian', 'armenians', 'arms', 'army', 'article', 'articles', 'ask', 'asked', 'asking', 'assume', 'atheism', 'attack', 'attempt', 'au', 'author', 'authority', 'available', 'average', 'avoid', 'away', 'ax', 'b8f', 'bad', 'base', 'based', 'basic', 'basically', 'basis', 'belief', 'believe', 'best', 'better', 'bh', 'bhj', 'bible', 'big', 'bike', 'bit', 'bits', 'bj', 'black', 'block', 'blood', 'board', 'body', 'book', 'books', 'bought', 'box', 'break', 'bring', 'brought', 'btw', 'buf', 'build', 'building', 'built', 'bus', 'business', 'buy', 'bxn', 'ca', 'cable', 'california', 'called', 'calls', 'came', 'canada', 'car', 'card', 'cards', 'care', 'carry', 'cars', 'case', 'cases', 'cause', 'cd', 'center', 'certain', 'certainly', 'chance', 'change', 'changed', 'changes', 'check', 'chicago', 'child', 'children', 'chip', 'chips', 'choice', 'christ', 'christian', 'christianity', 'christians', 'church', 'citizens', 'city', 'claim', 'claims', 'class', 'clear', 'clearly', 'clinton', 'clipper', 'close', 'code', 'color', 'com', 'come', 'comes', 'coming', 'command', 'comments', 'commercial', 'committee', 'common', 'community', 'comp', 'company', 'complete', 'completely', 'computer', 'condition', 'conference', 'congress', 'consider', 'considered', 'contact', 'contains', 'context', 'continue', 'control', 'controller', 'copy', 'correct', 'cost', 'couldn', 'country', 'couple', 'course', 'court', 'cover', 'create', 'created', 'crime', 'cross', 'cs', 'current', 'currently', 'cut', 'cx', 'data', 'date', 'dave', 'david', 'day', 'days', 'db', 'dc', 'dead', 'deal', 'death', 'dec', 'decided', 'defense', 'define', 'deleted', 'department', 'des', 'design', 'designed', 'details', 'development', 'device', 'devices', 'did', 'didn', 'difference', 'different', 'difficult', 'digital', 'directly', 'directory', 'discussion', 'disk', 'display', 'distribution', 'division', 'dod', 'does', 'doesn', 'doing', 'don', 'door', 'dos', 'doubt', 'dr', 'drive', 'driver', 'drivers', 'drives', 'drug', 'early', 'earth', 'easily', 'east', 'easy', 'ed', 'edu', 'effect', 'electronic', 'email', 'encryption', 'end', 'enforcement', 'engine', 'entire', 'entry', 'environment', 'error', 'escrow', 'especially', 'event', 'events', 'evidence', 'exactly', 'example', 'excellent', 'exist', 'existence', 'exists', 'expect', 'experience', 'explain', 'export', 'extra', 'face', 'fact', 'faith', 'false', 'family', 'faq', 'far', 'fast', 'faster', 'father', 'fax', 'fbi', 'features', 'federal', 'feel', 'field', 'figure', 'file', 'files', 'final', 'finally', 'fine', 'firearms', 'floppy', 'folks', 'follow', 'following', 'food', 'force', 'form', 'format', 'free', 'freedom', 'friend', 'ftp', 'function', 'functions', 'future', 'g9v', 'game', 'games', 'gas', 'gave', 'general', 'generally', 'gets', 'getting', 'gif', 'given', 'gives', 'giz', 'gk', 'gm', 'goal', 'god', 'goes', 'going', 'good', 'got', 'gov', 'government', 'graphics', 'great', 'greek', 'ground', 'group', 'groups', 'guess', 'gun', 'guns', 'guy', 'half', 'hand', 'happen', 'happened', 'happens', 'hard', 'hardware', 'haven', 'having', 'head', 'health', 'hear', 'heard', 'held', 'hell', 'help', 'hi', 'high', 'higher', 'history', 'hit', 'hockey', 'hold', 'home', 'hope', 'hours', 'house', 'hp', 'human', 'ibm', 'ide', 'idea', 'ideas', 'ii', 'image', 'images', 'imagine', 'important', 'include', 'included', 'includes', 'including', 'individual', 'info', 'information', 'input', 'inside', 'installed', 'instead', 'insurance', 'int', 'interested', 'interesting', 'interface', 'internal', 'international', 'internet', 'involved', 'isn', 'israel', 'israeli', 'issue', 'issues', 'jesus', 'jewish', 'jews', 'jim', 'job', 'jobs', 'john', 'jpeg', 'just', 'key', 'keyboard', 'keys', 'kill', 'killed', 'kind', 'knew', 'know', 'knowledge', 'known', 'knows', 'la', 'land', 'language', 'large', 'late', 'later', 'law', 'laws', 'league', 'learn', 'leave', 'left', 'legal', 'let', 'letter', 'level', 'library', 'life', 'light', 'like', 'likely', 'limited', 'line', 'lines', 'list', 'little', 'live', 'lives', 'living', 'll', 'local', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'lord', 'lost', 'lot', 'lots', 'love', 'low', 'lower', 'mac', 'machine', 'machines', 'mail', 'main', 'major', 'make', 'makes', 'making', 'man', 'manager', 'manual', 'mark', 'market', 'mass', 'master', 'material', 'matter', 'max', 'maybe', 'mb', 'mean', 'meaning', 'means', 'media', 'medical', 'members', 'memory', 'men', 'mention', 'mentioned', 'message', 'mike', 'miles', 'military', 'million', 'mind', 'mit', 'mode', 'model', 'modem', 'money', 'monitor', 'month', 'months', 'moral', 'mother', 'motif', 'mouse', 'mr', 'ms', 'multiple', 'nasa', 'national', 'nature', 'near', 'necessary', 'need', 'needed', 'needs', 'net', 'network', 'new', 'news', 'newsgroup', 'nhl', 'nice', 'night', 'non', 'normal', 'note', 'nsa', 'number', 'numbers', 'object', 'obvious', 'obviously', 'offer', 'office', 'official', 'oh', 'ok', 'old', 'ones', 'open', 'opinion', 'opinions', 'orbit', 'order', 'org', 'organization', 'original', 'os', 'output', 'outside', 'package', 'page', 'paper', 'particular', 'parts', 'party', 'past', 'paul', 'pay', 'pc', 'peace', 'people', 'perfect', 'performance', 'period', 'person', 'personal', 'phone', 'pick', 'picture', 'pin', 'pittsburgh', 'pl', 'place', 'places', 'plan', 'play', 'played', 'player', 'players', 'plus', 'point', 'points', 'police', 'policy', 'political', 'population', 'port', 'position', 'possible', 'possibly', 'post', 'posted', 'posting', 'power', 'pp', 'present', 'president', 'press', 'pretty', 'previous', 'price', 'printer', 'privacy', 'private', 'pro', 'probably', 'problem', 'problems', 'process', 'product', 'program', 'programs', 'project', 'protect', 'provide', 'provides', 'pub', 'public', 'published', 'purpose', 'qq', 'quality', 'question', 'questions', 'quite', 'radio', 'ram', 'range', 'rate', 'read', 'reading', 'real', 'really', 'reason', 'reasonable', 'reasons', 'received', 'recent', 'recently', 'record', 'red', 'reference', 'regular', 'related', 'release', 'religion', 'religious', 'remember', 'reply', 'report', 'reports', 'request', 'require', 'required', 'requires', 'research', 'resources', 'response', 'rest', 'result', 'results', 'return', 'right', 'rights', 'road', 'rom', 'room', 'round', 'rules', 'run', 'running', 'runs', 'russian', 'safety', 'said', 'sale', 'san', 'save', 'saw', 'say', 'saying', 'says', 'school', 'sci', 'science', 'scientific', 'screen', 'scsi', 'search', 'season', 'second', 'secret', 'section', 'secure', 'security', 'seen', 'self', 'sell', 'send', 'sense', 'sent', 'serial', 'series', 'server', 'service', 'set', 'shall', 'shipping', 'short', 'shot', 'shuttle', 'similar', 'simple', 'simply', 'sin', 'single', 'site', 'sites', 'situation', 'size', 'small', 'society', 'software', 'solution', 'son', 'soon', 'sorry', 'sort', 'sound', 'sounds', 'source', 'sources', 'south', 'soviet', 'space', 'special', 'specific', 'speed', 'spirit', 'st', 'standard', 'start', 'started', 'state', 'statement', 'states', 'station', 'stephanopoulos', 'steve', 'stop', 'story', 'stream', 'street', 'strong', 'study', 'stuff', 'subject', 'suggest', 'sun', 'support', 'supports', 'supposed', 'sure', 'systems', 'taken', 'takes', 'taking', 'talk', 'talking', 'tape', 'tar', 'tax', 'team', 'teams', 'technical', 'technology', 'tell', 'term', 'terms', 'test', 'text', 'thank', 'thanks', 'theory', 'thing', 'things', 'think', 'thinking', 'thought', 'time', 'times', 'title', 'tm', 'today', 'told', 'took', 'tools', 'total', 'trade', 'transfer', 'tried', 'true', 'truth', 'try', 'trying', 'turkey', 'turkish', 'turn', 'tv', 'type', 'uk', 'understand', 'unfortunately', 'unit', 'united', 'university', 'unix', 'unless', 'usa', 'use', 'used', 'useful', 'usenet', 'user', 'users', 'uses', 'using', 'usually', 'value', 'values', 'van', 'various', 've', 'version', 'vga', 'video', 'view', 'voice', 'volume', 'vs', 'wait', 'want', 'wanted', 'wants', 'war', 'washington', 'wasn', 'watch', 'water', 'way', 'ways', 'weapons', 'week', 'weeks', 'went', 'white', 'wide', 'widget', 'willing', 'win', 'window', 'windows', 'wish', 'wm', 'women', 'won', 'word', 'words', 'work', 'worked', 'working', 'works', 'world', 'worth', 'wouldn', 'write', 'writing', 'written', 'wrong', 'wrote', 'x11', 'xt', 'year', 'years', 'yes', 'york', 'young']\n"
     ]
    }
   ],
   "source": [
    "print(tf_vectorizer.get_feature_names())\n",
    "# this will print out the vocabulary, here 1,000 words, sorted alphabetically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go in reverse: given a word, we can figure out which column index it corresponds to. To do this, we use the `vocabulary_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "199"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.vocabulary_['car']\n",
    "# .vocabulary_ get the word index in the vocabulary\n",
    "# IN OTHER WORDS: what column do I have to look at to find the word car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what the raw counts are for the 0-th post as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 2, 3, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0].toarray()\n",
    "\n",
    "# I can index in to the zeroeth row of the matrix and convert into an array, I am anazlizing just the zeroeth row:\n",
    "# feature vector representation for the zeroeth document: \n",
    "# We should be able to write code to know what word corresponds each non zero number there\n",
    "\n",
    "# tf_vectorizer.get_feature_names_out()[(tf[0].toarray() != 0)[0]] # LIKE THIS!\n",
    "\n",
    "# run a for loop that goes thru the entries, and check to which indeces this corresponds in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit an LDA model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(random_state=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now LDA. This will take a while ong\n",
    "\n",
    "num_topics = 10\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting procedure determines the every topic's distribution over words; this information is stored in the `components_` attribute. There's a catch: we actually have to normalize to get the probability distributions (without this normalization, instead what the model has are pseudocounts for how often different words appear per topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape\n",
    "# The shaope of this is 10 by 1000, 10 topics and 1000 words\n",
    "# These are not probability distributions yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74979.92756, 37471.24963, 33866.78341, 40079.8962 , 41754.73437,\n",
       "       44022.38561, 36169.17215, 70224.62485, 41944.93058, 73054.29565])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we access: sum ACROSS COLUMNS (get rid of columns), I get 10 numbers: a sum for each row.\n",
    "# They dont sum uop to 1, because not probability distributions\n",
    "\n",
    "# .components_ is a raw count histogram, but why are there fractions?\n",
    "# Because of how it works: when it estimates how much a word belongs to a topic it does a prob assignment, not a deterministic assignment.\n",
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])\n",
    "# We can get the proba distributions: take each row and divide by sum, then we get what was in the purple box in the lecture slide:\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that each topic's word distribution sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.sum(axis=1)\n",
    "# then we have distributions: every row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out what the probabilities for the different words are for a specific topic. This isn't very easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00011 0.00191 0.      0.      0.      0.      0.      0.00247 0.00257\n",
      " 0.00003 0.00033 0.      0.00003 0.00015 0.00002 0.00059 0.      0.\n",
      " 0.      0.      0.00049 0.00014 0.00019 0.00004 0.      0.00026 0.\n",
      " 0.00164 0.00075 0.      0.00004 0.      0.00001 0.00005 0.00018 0.\n",
      " 0.      0.      0.      0.      0.      0.00162 0.00095 0.      0.\n",
      " 0.      0.      0.      0.0001  0.      0.      0.      0.      0.\n",
      " 0.      0.00053 0.      0.      0.      0.      0.00011 0.      0.00003\n",
      " 0.00235 0.00082 0.00001 0.00081 0.      0.      0.00048 0.00022 0.\n",
      " 0.      0.0006  0.00024 0.      0.00108 0.00015 0.00025 0.00025 0.\n",
      " 0.      0.00082 0.00259 0.00014 0.00021 0.      0.00058 0.00006 0.00022\n",
      " 0.0042  0.00115 0.00021 0.      0.00069 0.00016 0.00076 0.00388 0.00136\n",
      " 0.00023 0.00212 0.00103 0.      0.00029 0.00068 0.      0.0009  0.00128\n",
      " 0.00006 0.      0.00054 0.00008 0.00012 0.00072 0.00069 0.00023 0.\n",
      " 0.00003 0.0001  0.00019 0.00008 0.00027 0.0001  0.      0.00031 0.\n",
      " 0.00167 0.00045 0.00156 0.00023 0.      0.      0.      0.      0.\n",
      " 0.00004 0.      0.00067 0.00052 0.00081 0.0007  0.      0.00001 0.00014\n",
      " 0.00001 0.      0.00001 0.00066 0.00247 0.00014 0.00206 0.      0.\n",
      " 0.0037  0.00214 0.00123 0.00032 0.001   0.00025 0.      0.00294 0.00556\n",
      " 0.00731 0.      0.      0.      0.00451 0.      0.0018  0.      0.\n",
      " 0.0011  0.00011 0.00003 0.00008 0.00062 0.0004  0.00003 0.00162 0.00109\n",
      " 0.00059 0.00079 0.00024 0.00134 0.      0.00133 0.00058 0.0011  0.\n",
      " 0.00172 0.00267 0.      0.0002  0.00027 0.00026 0.00131 0.00027 0.00163\n",
      " 0.00093 0.00686 0.      0.00004 0.00156 0.00047 0.00126 0.00251 0.00021\n",
      " 0.00038 0.00001 0.00058 0.00074 0.00172 0.00155 0.00122 0.00053 0.00064\n",
      " 0.00098 0.      0.      0.      0.00011 0.00042 0.00062 0.      0.\n",
      " 0.      0.      0.      0.      0.00063 0.00027 0.00015 0.00099 0.00074\n",
      " 0.00059 0.00106 0.      0.00196 0.      0.      0.00001 0.00335 0.00191\n",
      " 0.00132 0.      0.0005  0.00041 0.      0.00068 0.00066 0.      0.00186\n",
      " 0.00028 0.00081 0.00005 0.00194 0.00021 0.00035 0.0016  0.0003  0.00036\n",
      " 0.      0.00012 0.00066 0.00082 0.      0.00004 0.00021 0.00333 0.00096\n",
      " 0.00088 0.00241 0.00313 0.      0.00078 0.00057 0.00046 0.      0.00023\n",
      " 0.      0.00191 0.00061 0.00075 0.      0.00005 0.00011 0.00085 0.\n",
      " 0.003   0.0021  0.      0.00095 0.00032 0.00236 0.      0.      0.00077\n",
      " 0.00141 0.      0.00042 0.0002  0.      0.00119 0.00121 0.00025 0.00075\n",
      " 0.      0.      0.00572 0.00382 0.00196 0.00213 0.00064 0.00005 0.0006\n",
      " 0.      0.00024 0.      0.00001 0.      0.00079 0.      0.00391 0.00365\n",
      " 0.00257 0.01337 0.00058 0.      0.00102 0.00006 0.00062 0.      0.\n",
      " 0.      0.      0.00195 0.0009  0.00107 0.00052 0.00089 0.00002 0.00043\n",
      " 0.00091 0.00019 0.      0.      0.00336 0.      0.00161 0.0006  0.\n",
      " 0.00035 0.00002 0.      0.00129 0.00026 0.      0.      0.00122 0.00108\n",
      " 0.00227 0.0001  0.      0.      0.00122 0.00159 0.00016 0.      0.00208\n",
      " 0.00135 0.00161 0.      0.      0.00023 0.      0.0034  0.00128 0.00023\n",
      " 0.      0.      0.00001 0.00034 0.      0.00133 0.00086 0.00101 0.\n",
      " 0.      0.00084 0.00115 0.00088 0.      0.      0.0008  0.00031 0.00022\n",
      " 0.0014  0.00067 0.00037 0.      0.00123 0.00003 0.0004  0.      0.00001\n",
      " 0.      0.00146 0.      0.00842 0.00374 0.00132 0.00076 0.00049 0.00082\n",
      " 0.00236 0.0032  0.      0.00171 0.00054 0.      0.      0.00082 0.00192\n",
      " 0.      0.00193 0.00736 0.01592 0.0071  0.00001 0.00006 0.      0.00513\n",
      " 0.      0.00264 0.00024 0.00004 0.00239 0.      0.      0.00192 0.00167\n",
      " 0.00124 0.00083 0.00065 0.00052 0.00247 0.00015 0.00137 0.00157 0.00154\n",
      " 0.00003 0.00097 0.00213 0.00003 0.00092 0.00124 0.      0.0046  0.00133\n",
      " 0.00025 0.00296 0.00249 0.00074 0.00217 0.00154 0.00076 0.00056 0.00014\n",
      " 0.      0.      0.      0.0024  0.00049 0.00018 0.      0.      0.00075\n",
      " 0.00247 0.00029 0.00053 0.00011 0.00048 0.00022 0.      0.      0.00018\n",
      " 0.0013  0.00008 0.00151 0.00064 0.      0.00105 0.00117 0.      0.00004\n",
      " 0.      0.      0.00057 0.00281 0.      0.      0.00071 0.00007 0.\n",
      " 0.      0.      0.00007 0.00261 0.0028  0.0005  0.      0.01571 0.00022\n",
      " 0.      0.      0.00004 0.      0.00243 0.00046 0.00787 0.0004  0.00077\n",
      " 0.00051 0.0006  0.0004  0.00002 0.0017  0.00108 0.00102 0.      0.\n",
      " 0.0019  0.00109 0.00034 0.00198 0.00013 0.00388 0.00009 0.00224 0.\n",
      " 0.00106 0.00221 0.01585 0.00119 0.00057 0.00185 0.00054 0.00055 0.00491\n",
      " 0.0011  0.      0.00052 0.00686 0.00196 0.00509 0.00107 0.00415 0.00124\n",
      " 0.00209 0.00151 0.      0.00103 0.00541 0.00138 0.00074 0.00325 0.00123\n",
      " 0.      0.00023 0.00002 0.00012 0.00072 0.00178 0.00832 0.00151 0.00157\n",
      " 0.00089 0.00066 0.00031 0.00091 0.00177 0.00098 0.00009 0.00014 0.00096\n",
      " 0.00002 0.00319 0.      0.00174 0.      0.00097 0.00037 0.00007 0.00003\n",
      " 0.00017 0.      0.00076 0.00065 0.00002 0.00189 0.00192 0.00042 0.00123\n",
      " 0.00141 0.      0.00001 0.00107 0.      0.00554 0.00004 0.00092 0.00185\n",
      " 0.      0.00005 0.      0.      0.00015 0.00019 0.      0.00012 0.00027\n",
      " 0.00018 0.00146 0.00016 0.00412 0.00056 0.00144 0.00074 0.00024 0.00877\n",
      " 0.00043 0.      0.00147 0.00229 0.00142 0.00074 0.00074 0.00097 0.\n",
      " 0.00111 0.00168 0.      0.00035 0.00085 0.00186 0.00063 0.00068 0.00213\n",
      " 0.00105 0.00395 0.00155 0.00082 0.00062 0.00044 0.00112 0.00107 0.\n",
      " 0.00039 0.0007  0.      0.      0.00043 0.00192 0.00001 0.00033 0.0003\n",
      " 0.00144 0.00001 0.00193 0.00026 0.00255 0.      0.      0.00608 0.00076\n",
      " 0.00139 0.00052 0.00026 0.00061 0.00041 0.00191 0.0001  0.00001 0.00083\n",
      " 0.      0.00183 0.00062 0.00138 0.00435 0.00163 0.00283 0.00394 0.00154\n",
      " 0.00433 0.00154 0.      0.0001  0.00004 0.      0.00004 0.00109 0.00146\n",
      " 0.00057 0.00087 0.00006 0.00005 0.00546 0.      0.00011 0.00137 0.00016\n",
      " 0.00367 0.00069 0.00325 0.      0.      0.00028 0.00052 0.00558 0.00221\n",
      " 0.00192 0.0004  0.00053 0.00193 0.00066 0.0007  0.      0.00047 0.\n",
      " 0.      0.00003 0.      0.00022 0.      0.00153 0.00158 0.00056 0.00296\n",
      " 0.00198 0.      0.00102 0.0011  0.00052 0.00037 0.00291 0.00728 0.00164\n",
      " 0.00064 0.00078 0.      0.00066 0.00072 0.0011  0.00132 0.00003 0.00097\n",
      " 0.0001  0.00028 0.      0.      0.00285 0.      0.00013 0.00046 0.\n",
      " 0.00079 0.00036 0.00012 0.00003 0.00015 0.00015 0.00126 0.00033 0.00035\n",
      " 0.00041 0.00539 0.      0.00133 0.      0.00056 0.00054 0.00017 0.00274\n",
      " 0.00112 0.00245 0.00069 0.00042 0.00259 0.00068 0.00017 0.00073 0.00063\n",
      " 0.00426 0.00094 0.0003  0.0013  0.      0.      0.      0.      0.\n",
      " 0.      0.00312 0.00295 0.00019 0.      0.      0.      0.00241 0.\n",
      " 0.00246 0.00013 0.0007  0.00011 0.      0.0017  0.      0.0011  0.00104\n",
      " 0.      0.00036 0.00189 0.00155 0.00018 0.00099 0.00049 0.00106 0.\n",
      " 0.00173 0.00007 0.00002 0.00044 0.00088 0.00262 0.00004 0.      0.00016\n",
      " 0.      0.00101 0.00097 0.0015  0.00084 0.0011  0.00017 0.00013 0.00085\n",
      " 0.00002 0.00192 0.00043 0.00022 0.002   0.      0.00001 0.00038 0.00267\n",
      " 0.00129 0.00094 0.00005 0.00047 0.00095 0.      0.0008  0.00096 0.00058\n",
      " 0.      0.00051 0.00079 0.00016 0.00188 0.00009 0.0006  0.00079 0.00192\n",
      " 0.      0.00054 0.00489 0.0004  0.00107 0.00084 0.0008  0.001   0.00136\n",
      " 0.00002 0.      0.00121 0.0069  0.00236 0.00039 0.00034 0.00187 0.00133\n",
      " 0.00058 0.00082 0.      0.00062 0.00022 0.00029 0.00526 0.00455 0.01466\n",
      " 0.00091 0.00236 0.01216 0.00204 0.      0.00009 0.0015  0.00119 0.00127\n",
      " 0.      0.00079 0.00162 0.      0.00028 0.00135 0.00002 0.00171 0.00149\n",
      " 0.      0.      0.00099 0.00186 0.00102 0.      0.0008  0.00053 0.00067\n",
      " 0.00008 0.00023 0.      0.00186 0.00013 0.00293 0.00366 0.00058 0.\n",
      " 0.      0.      0.00027 0.00118 0.00139 0.00089 0.00007 0.      0.00039\n",
      " 0.00806 0.      0.      0.00007 0.0002  0.00025 0.00028 0.00033 0.00107\n",
      " 0.00508 0.00136 0.00066 0.00022 0.0003  0.00157 0.00158 0.00098 0.00702\n",
      " 0.00043 0.      0.00132 0.00138 0.00144 0.00115 0.0005  0.      0.00095\n",
      " 0.00249 0.      0.      0.00041 0.      0.      0.00325 0.00043 0.00045\n",
      " 0.00577 0.00086 0.00217 0.00091 0.00206 0.00192 0.00176 0.00004 0.00002\n",
      " 0.      0.00154 0.00004 0.      0.      0.01144 0.00844 0.00248 0.00049\n",
      " 0.00144]\n"
     ]
    }
   ],
   "source": [
    "print(topic_word_distributions[0])\n",
    "# lets look at 0th topic, it is saying that the 0th word appears 0.00011, word 1 appears with prob 0.00191\n",
    "# Lets view the data differently to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, usually people do something like looking at the most probable words per topic, and try to use these words to interpret what the different topics correspond to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "good : 0.015922549081293896\n",
      "like : 0.015847630671172157\n",
      "just : 0.015714974597809822\n",
      "think : 0.01465803514815006\n",
      "don : 0.013366025027767674\n",
      "time : 0.012159230893303003\n",
      "year : 0.011442050656933971\n",
      "new : 0.008768217977593936\n",
      "years : 0.008439220778250291\n",
      "game : 0.008416482579473788\n",
      "make : 0.008318270139852613\n",
      "ve : 0.008056133818726018\n",
      "know : 0.007865529016907296\n",
      "going : 0.007357414502894843\n",
      "better : 0.007305177940555175\n",
      "really : 0.007282768897233141\n",
      "got : 0.007100242166187468\n",
      "way : 0.007020258221618514\n",
      "team : 0.006901091494924349\n",
      "car : 0.006860678090522127\n",
      "\n",
      "[Topic 1]\n",
      "drive : 0.02511445975596719\n",
      "card : 0.019045045227142902\n",
      "scsi : 0.01574807346309643\n",
      "disk : 0.01508615194924129\n",
      "use : 0.013112057755912501\n",
      "output : 0.012487568705565059\n",
      "file : 0.011474974819227329\n",
      "bit : 0.011450491727323105\n",
      "hard : 0.010426435918865872\n",
      "entry : 0.009962381704950406\n",
      "memory : 0.009892936703385195\n",
      "mac : 0.009531449582937765\n",
      "video : 0.009451338641933645\n",
      "drives : 0.009074000962777743\n",
      "pc : 0.009070328611216795\n",
      "windows : 0.008135023862197369\n",
      "16 : 0.007988238149752367\n",
      "bus : 0.007927283819698573\n",
      "controller : 0.00790205787618957\n",
      "program : 0.00784268458596016\n",
      "\n",
      "[Topic 2]\n",
      "10 : 0.03202922038890933\n",
      "00 : 0.02696433054139319\n",
      "25 : 0.02182969124534406\n",
      "15 : 0.020606357749565506\n",
      "11 : 0.020604350399724462\n",
      "20 : 0.02049576092374078\n",
      "12 : 0.020376684447947477\n",
      "14 : 0.01805547083487532\n",
      "16 : 0.01646026562968167\n",
      "13 : 0.016023012407497667\n",
      "17 : 0.01601890318720602\n",
      "18 : 0.01593141608024834\n",
      "30 : 0.013487129884524506\n",
      "50 : 0.01332308313005534\n",
      "24 : 0.013126904580198847\n",
      "19 : 0.012520561557602231\n",
      "55 : 0.012500233146391187\n",
      "21 : 0.012264247922282928\n",
      "40 : 0.01192815257556638\n",
      "22 : 0.011207231765196161\n",
      "\n",
      "[Topic 3]\n",
      "key : 0.022866565042557685\n",
      "government : 0.02085312498843717\n",
      "gun : 0.016677031123281286\n",
      "db : 0.015171423239595614\n",
      "law : 0.01464907573017526\n",
      "use : 0.014111126676381425\n",
      "people : 0.012470526208203802\n",
      "encryption : 0.0102852246227526\n",
      "public : 0.010243636203369254\n",
      "state : 0.0099473416019665\n",
      "chip : 0.009825392329557047\n",
      "keys : 0.008779074363170257\n",
      "clipper : 0.008603245846227368\n",
      "states : 0.008318226325668433\n",
      "control : 0.008226551411556766\n",
      "number : 0.008019292972255114\n",
      "used : 0.007703329865061303\n",
      "guns : 0.007611516117671697\n",
      "health : 0.0074015183803603935\n",
      "right : 0.006935759695920407\n",
      "\n",
      "[Topic 4]\n",
      "people : 0.024311295078161155\n",
      "said : 0.02424559321880432\n",
      "mr : 0.014104981910179986\n",
      "know : 0.013468288248487325\n",
      "armenian : 0.01334219940918647\n",
      "did : 0.012413013162784349\n",
      "don : 0.011539776119380054\n",
      "didn : 0.010944034432906063\n",
      "armenians : 0.010923312074368838\n",
      "turkish : 0.010132982460837896\n",
      "going : 0.009685278853463292\n",
      "went : 0.00948231476359466\n",
      "time : 0.009318116756590052\n",
      "president : 0.00927153862990113\n",
      "just : 0.009064415387593217\n",
      "war : 0.008272822237163075\n",
      "stephanopoulos : 0.008264930815089593\n",
      "jews : 0.007943422443838052\n",
      "say : 0.007785799204031933\n",
      "like : 0.007637486984592759\n",
      "\n",
      "[Topic 5]\n",
      "edu : 0.02018312550138163\n",
      "available : 0.014622086775877735\n",
      "window : 0.01451759669750938\n",
      "windows : 0.01450809472801771\n",
      "image : 0.014202932981631156\n",
      "version : 0.01356723393325451\n",
      "use : 0.013559184672232338\n",
      "software : 0.01229184546011519\n",
      "program : 0.011764165563758299\n",
      "graphics : 0.011586390886004608\n",
      "ftp : 0.010473416938225278\n",
      "server : 0.010462468444821687\n",
      "file : 0.01030098888454638\n",
      "using : 0.010279537804707143\n",
      "dos : 0.010186128274177574\n",
      "com : 0.010041868375764216\n",
      "display : 0.009508250698966074\n",
      "sun : 0.008411849482997407\n",
      "set : 0.008165843948144168\n",
      "motif : 0.008043634521501525\n",
      "\n",
      "[Topic 6]\n",
      "space : 0.021229375975651145\n",
      "information : 0.020605813849808006\n",
      "edu : 0.013086098355937825\n",
      "mail : 0.01212242592672959\n",
      "data : 0.012063443176285967\n",
      "new : 0.011922325100362579\n",
      "internet : 0.01180566449915215\n",
      "university : 0.011648063401924157\n",
      "nasa : 0.011260324762512442\n",
      "research : 0.011214434673688996\n",
      "computer : 0.010579251202203373\n",
      "send : 0.01008235236748465\n",
      "file : 0.009827860114832828\n",
      "anonymous : 0.009393114153924206\n",
      "list : 0.00884288720549654\n",
      "email : 0.008556680088168827\n",
      "technology : 0.00850254009023313\n",
      "address : 0.008453084833023802\n",
      "available : 0.008312068272885711\n",
      "com : 0.008010436642025573\n",
      "\n",
      "[Topic 7]\n",
      "god : 0.024214358169839616\n",
      "people : 0.01983115631224014\n",
      "think : 0.012548053861881264\n",
      "does : 0.011687743323611244\n",
      "jesus : 0.011293758739613025\n",
      "believe : 0.011272045532292012\n",
      "don : 0.010834089093902339\n",
      "say : 0.010114162234632523\n",
      "just : 0.009461924824189775\n",
      "know : 0.00827621606008663\n",
      "true : 0.007825806533253573\n",
      "like : 0.007419673532456669\n",
      "way : 0.007242156428134807\n",
      "life : 0.007196868607195392\n",
      "christian : 0.007055010808531864\n",
      "time : 0.0070227629792582215\n",
      "israel : 0.006934392612716927\n",
      "bible : 0.006745791914802957\n",
      "question : 0.006407541720117259\n",
      "things : 0.006373984459111454\n",
      "\n",
      "[Topic 8]\n",
      "know : 0.0261341322608149\n",
      "like : 0.02131701814847326\n",
      "don : 0.02005316567858998\n",
      "thanks : 0.0199926164318845\n",
      "just : 0.017657856830905934\n",
      "does : 0.01503962829791828\n",
      "ve : 0.012652637710485531\n",
      "help : 0.011282977547682703\n",
      "want : 0.010651057192301494\n",
      "use : 0.01053777436836704\n",
      "mail : 0.010021837288920428\n",
      "problem : 0.009663091871713079\n",
      "edu : 0.009128531452268558\n",
      "good : 0.009109138814944425\n",
      "post : 0.008996488682684106\n",
      "file : 0.008922092098023292\n",
      "need : 0.008706364795622644\n",
      "looking : 0.007313642795819985\n",
      "time : 0.0071744436606974655\n",
      "bike : 0.007034238505179879\n",
      "\n",
      "[Topic 9]\n",
      "ax : 0.7717971886082466\n",
      "max : 0.055921756933680296\n",
      "g9v : 0.015168170327094601\n",
      "b8f : 0.014757516856658558\n",
      "a86 : 0.012197776886748507\n",
      "pl : 0.010147908394792498\n",
      "145 : 0.009912072552667456\n",
      "1d9 : 0.008447141779981918\n",
      "1t : 0.0064486281754207175\n",
      "0t : 0.006215924619143329\n",
      "bhj : 0.005901090271008161\n",
      "3t : 0.005517813662944113\n",
      "34u : 0.005463059915719653\n",
      "giz : 0.0052987986062219715\n",
      "2di : 0.005244044809233216\n",
      "wm : 0.004677791543976195\n",
      "2tm : 0.004450114768534645\n",
      "75u : 0.00445011475803213\n",
      "7ey : 0.003574054032872233\n",
      "0d : 0.0031497118103811124\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ribarragi/miniconda3/envs/UDA_RIG/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# we can sort the probability by big to small\n",
    "# for each topic, listing top 20 most popular words (by probability)\n",
    "\n",
    "num_top_words = 20\n",
    "\n",
    "def print_top_words(topic_word_distributions, num_top_words, vectorizer):\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    print('Displaying the top %d words per topic and their probabilities within the topic...' % num_top_words)\n",
    "    print()\n",
    "\n",
    "    for topic_idx in range(num_topics):\n",
    "        print('[Topic ', topic_idx, ']', sep='')\n",
    "        # This is the interesting part: take the topic_word_distributions only for the topic_idxth topic, and get the indexes sorted from highest to lowest\n",
    "        sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "        # then for the range 0,num_words, print the vocab of each of the words and its entry in the topic_word_distribution matrix\n",
    "        for rank in range(num_top_words):\n",
    "            word_idx = sort_indices[rank]\n",
    "            print(vocab[word_idx], ':',\n",
    "                  topic_word_distributions[topic_idx, word_idx])\n",
    "        print()\n",
    "\n",
    "print_top_words(topic_word_distributions, num_top_words, tf_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `transform()` function to figure out for each document, what fraction of it is explained by each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10000x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 254416 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = lda.transform(tf)\n",
    "# Ill get a low dimensional version of the data, the document topic matrix (10, 10)\n",
    "# each of the 10,000 represented as a distribution over 10 different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00182, 0.00182, 0.02258, 0.00182, 0.00182, 0.00182, 0.00182,\n",
       "       0.96288, 0.00182, 0.00182])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The topic distribution of the 0th document is very small except for 0.96288 fior index 7\n",
    "# We saw from above, topic 7 is about religion, and we can check that document 0 is about religion\n",
    "doc_topic_matrix[0]\n",
    "\n",
    "\n",
    "# Note: for GMM when you do the .predict, itll give you k probabilites that sum to 1. \n",
    "# predict_proba of GMM \n",
    "# here: ... gives you a proba distr:\n",
    "# INTERPRETARATION DIFFERENT\n",
    "\n",
    "# FOR LDA: document can consist of a bunch of words, and different fraction of the words are truly in different topics: we get mixed membership, words are allowed to e in different topics\n",
    "# GMM: the above doesnt hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_topic_matrix[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this *could* be interpreted as a form of dimensionality reduction: document 0 is converted from its raw counts histogram representation to a 10-dimensional vector of probabilities, indicating estimated memberships to the 10 different topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "\n",
    "Here's a fancier way to visualize. This requires installation of the wordcloud package:\n",
    "\n",
    "```\n",
    "pip install wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yv/9jkc42cj01gb_08dbbq9fcnw0000gn/T/ipykernel_53404/2357757354.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnum_max_word_cloud_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "num_max_word_cloud_words = 100\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "num_topics = len(topic_word_distributions)\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    wc = WordCloud(max_words=num_max_word_cloud_words)\n",
    "    wc.generate_from_frequencies(dict(zip(vocab, topic_word_distributions[topic_idx])))\n",
    "    plt.figure()\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title('Topic %d' % topic_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing co-occurrences of words\n",
    "\n",
    "Here, we count the number of newsgroup posts in which two words both occur. This part of the demo should feel like a review of co-occurrence analysis from earlier in the course, except now we use scikit-learn's built-in CountVectorizer. Conceptually everything else in the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'year'\n",
    "word2 = 'team'\n",
    "\n",
    "word1_column_idx = tf_vectorizer.vocabulary_[word1]\n",
    "word2_column_idx = tf_vectorizer.vocabulary_[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[:, word1_column_idx].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_with_both_word1_and_word2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the log of the conditional probability of word 1 appearing given that word 2 appeared, where we add in a little bit of a fudge factor in the numerator (in this case, it's actually not needed but some times you do have two words that do not co-occur for which you run into a numerical issue due to taking the log of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5482462194376105"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eps = 0.1\n",
    "np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of prob of see one word given you see another, using count vectorizer!\n",
    "# Same content as in hw , but with a different tool\n",
    "# This uses np.arrays instead of counters. Needs to kep track of the indexes.\n",
    "# Understad how countvectorizer works\n",
    "\n",
    "def prob_see_word1_given_see_word2(word1, word2, vectorizer, eps=0.1):\n",
    "    word1_column_idx = vectorizer.vocabulary_[word1]\n",
    "    word2_column_idx = vectorizer.vocabulary_[word2]\n",
    "    documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2\n",
    "    return np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(str, str)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word1), type(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence\n",
    "\n",
    "The below code shows how one implements the topic coherence calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cell below, vectorizer.get_feature_names() is just tokenizing the text, \n",
    "# eliminating stopwords, eliminating some other words, and then giving you back\n",
    "# a list of the tokens in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic coherence\n",
    "\n",
    "def compute_average_coherence(topic_word_distributions, num_top_words, vectorizer, verbose=True):\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    average_coherence = 0\n",
    "    # foor loop thru different topics, for each topic double nested for loop, going thru each entry, ordering matters\n",
    "    # make sure words arent the same, then compute the log(prob)\n",
    "    # add a bunhc of them and divide by total number of topics\n",
    "    for topic_idx in range(num_topics):\n",
    "        if verbose:\n",
    "            print('[Topic ', topic_idx, ']', sep='')\n",
    "        \n",
    "        sort_indices = np.argsort(topic_word_distributions[topic_idx])[::-1]\n",
    "        coherence = 0.\n",
    "        for top_word_idx1 in sort_indices[:num_top_words]:\n",
    "            word1 = vocab[top_word_idx1]\n",
    "            for top_word_idx2 in sort_indices[:num_top_words]:\n",
    "                word2 = vocab[top_word_idx2]\n",
    "                if top_word_idx1 != top_word_idx2:\n",
    "                    coherence += prob_see_word1_given_see_word2(word1, word2, vectorizer, 0.1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Coherence:', coherence)\n",
    "            print()\n",
    "        average_coherence += coherence\n",
    "    average_coherence /= num_topics\n",
    "    if verbose:\n",
    "        print('Average coherence:', average_coherence)\n",
    "    return average_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topic 0]\n",
      "Coherence: -883.433278037819\n",
      "\n",
      "[Topic 1]\n",
      "Coherence: -1204.9568944144453\n",
      "\n",
      "[Topic 2]\n",
      "Coherence: -658.5193466692967\n",
      "\n",
      "[Topic 3]\n",
      "Coherence: -1262.9416512877067\n",
      "\n",
      "[Topic 4]\n",
      "Coherence: -1120.0387743529964\n",
      "\n",
      "[Topic 5]\n",
      "Coherence: -1068.283511244093\n",
      "\n",
      "[Topic 6]\n",
      "Coherence: -1018.020273866717\n",
      "\n",
      "[Topic 7]\n",
      "Coherence: -874.4985384459782\n",
      "\n",
      "[Topic 8]\n",
      "Coherence: -982.6298329129335\n",
      "\n",
      "[Topic 9]\n",
      "Coherence: -228.56240302255796\n",
      "\n",
      "Average coherence: -930.1884504254543\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-930.1884504254543"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_average_coherence(topic_word_distributions, num_top_words, tf_vectorizer, True)\n",
    "# These are negative, the highest possible is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique words\n",
    "\n",
    "The below code shows how one implements the number of unique words calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force implementation\n",
    "# nothing clever to speed upmcalculation\n",
    "\n",
    "#fro loop for each topic\n",
    "# For loop over each top word\n",
    "# another for loop for other topics\n",
    "# lok for all other top words in those other topics\n",
    "# check for uniqueness\n",
    "\n",
    "def compute_average_num_unique_words(topic_word_distributions, num_top_words, vectorizer, verbose=True):\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    average_number_of_unique_top_words = 0\n",
    "    for topic_idx1 in range(num_topics):\n",
    "        if verbose:\n",
    "            print('[Topic ', topic_idx1, ']', sep='')\n",
    "        \n",
    "        sort_indices1 = np.argsort(topic_word_distributions[topic_idx1])[::-1]\n",
    "        num_unique_top_words = 0\n",
    "        for top_word_idx1 in sort_indices1[:num_top_words]:\n",
    "            word1 = vocab[top_word_idx1]\n",
    "            break_ = False\n",
    "            for topic_idx2 in range(num_topics):\n",
    "                if topic_idx1 != topic_idx2:\n",
    "                    sort_indices2 = np.argsort(topic_word_distributions[topic_idx2])[::-1]\n",
    "                    for top_word_idx2 in sort_indices2[:num_top_words]:\n",
    "                        word2 = vocab[top_word_idx2]\n",
    "                        if word1 == word2:\n",
    "                            break_ = True\n",
    "                            break\n",
    "                    if break_:\n",
    "                        break\n",
    "            else:\n",
    "                num_unique_top_words += 1\n",
    "        if verbose:\n",
    "            print('Number of unique top words:', num_unique_top_words)\n",
    "            print()\n",
    "\n",
    "        average_number_of_unique_top_words += num_unique_top_words\n",
    "    average_number_of_unique_top_words /= num_topics\n",
    "    \n",
    "    if verbose:\n",
    "        print('Average number of unique top words:', average_number_of_unique_top_words)\n",
    "    \n",
    "    return average_number_of_unique_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Topic 0]\n",
      "Number of unique top words: 9\n",
      "\n",
      "[Topic 1]\n",
      "Number of unique top words: 15\n",
      "\n",
      "[Topic 2]\n",
      "Number of unique top words: 19\n",
      "\n",
      "[Topic 3]\n",
      "Number of unique top words: 18\n",
      "\n",
      "[Topic 4]\n",
      "Number of unique top words: 12\n",
      "\n",
      "[Topic 5]\n",
      "Number of unique top words: 13\n",
      "\n",
      "[Topic 6]\n",
      "Number of unique top words: 14\n",
      "\n",
      "[Topic 7]\n",
      "Number of unique top words: 10\n",
      "\n",
      "[Topic 8]\n",
      "Number of unique top words: 8\n",
      "\n",
      "[Topic 9]\n",
      "Number of unique top words: 20\n",
      "\n",
      "Average number of unique top words: 13.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13.8"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_average_num_unique_words(topic_word_distributions, num_top_words, tf_vectorizer, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting average coherence vs k (number of topics), and average number of unique words vs k\n",
    "\n",
    "Next, we plot the average coherence vs k and the average number of unique words vs k. Note that these are *not* the only topic model metrics available (much like how CH index is not the only metric available for clustering).\n",
    "\n",
    "For both average coherence and average number of unique words, we would like these to be high. In this particular example, it turns out k=2 yields very high values for both but if you look at the topics learned for k=2, they are qualitatively quite bad (basically one topic is gibberish and the other is everything else!). This observation reinforces the important idea that while there exist topic modeling metrics (such as coherence and number of unique words), you should definitely still look at what the learned topics are (e.g., by printing the top words per topic) to help decide on what value of k to use.\n",
    "\n",
    "Also, keep in mind that the results are in some sense \"noisy\" since the LDA fitting procedure is random. We're choosing a specific `random_state` seed value but if we try different random seeds, we can get different results. For simplicity, because LDA fitting is quite computationally expensive, we are *not* doing what we did with GMM's where we did many different random initializations. Thus, the conclusions we draw regarding how many topics to use might actually be different with different random initializations.\n",
    "\n",
    "At least according to average coherence and average number of unique words for the random seed we use, the results below suggests that using k=4 yields average coherence and average number of unique words that are still reasonably high (as good as or almost as good as the k=2 result), and inspecting the topics learned for k=4, they are definitely more interesting than the ones learned for k=2.\n",
    "\n",
    "From qualitatively looking at topics, the k=5, k=6, and k=7 topics also look decent. When k gets too large (e.g., k=10), there start to be topics that look like there might be too much overlap (such as multiple topics that seem to be about computers).\n",
    "\n",
    "Note that one of the things to look out for is whether there are \"stable\" topics, where even for slightly different values of k and different random initializations, LDA keeps finding specific topics (e.g., one on gibberish, one on numbers, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 2\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "ax : 0.6670609408224907\n",
      "max : 0.04897290356354059\n",
      "g9v : 0.013114422643457437\n",
      "b8f : 0.01275949912939879\n",
      "a86 : 0.010547142707890972\n",
      "pl : 0.008819612623778256\n",
      "145 : 0.00881675090221303\n",
      "1d9 : 0.007305507436215006\n",
      "db : 0.007159573058301496\n",
      "1t : 0.00557821214241208\n",
      "0t : 0.005377089263425004\n",
      "25 : 0.005211780342996569\n",
      "bhj : 0.005104981847457374\n",
      "3t : 0.004773718255446174\n",
      "34u : 0.004726396579798662\n",
      "giz : 0.004584428325550535\n",
      "2di : 0.0045371051926778325\n",
      "55 : 0.004286549836996119\n",
      "14 : 0.004247079858618981\n",
      "wm : 0.004101448384177003\n",
      "\n",
      "[Topic 1]\n",
      "people : 0.00898805018321434\n",
      "like : 0.008724015906915197\n",
      "don : 0.008445318198979547\n",
      "just : 0.008117711165098762\n",
      "know : 0.007726560502119272\n",
      "use : 0.006807342121575315\n",
      "time : 0.006601978036055969\n",
      "think : 0.0065897670065797375\n",
      "does : 0.00599569573547193\n",
      "new : 0.005707662001047904\n",
      "good : 0.005477410696991825\n",
      "edu : 0.005188926692129768\n",
      "make : 0.004467734634022856\n",
      "way : 0.004391946942939348\n",
      "god : 0.0042037116677492575\n",
      "used : 0.00410836241940848\n",
      "say : 0.004042354681202132\n",
      "ve : 0.004042230793685092\n",
      "right : 0.004015407615464605\n",
      "file : 0.003959205422499352\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 3\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "people : 0.013987379438521899\n",
      "don : 0.011524660963375346\n",
      "just : 0.010967976112691066\n",
      "think : 0.010217784634479091\n",
      "like : 0.009964277649120086\n",
      "know : 0.009234223571289897\n",
      "time : 0.00818251395696018\n",
      "good : 0.007331228467660352\n",
      "god : 0.006936159078250954\n",
      "said : 0.0063851562833832035\n",
      "say : 0.00634022436925281\n",
      "did : 0.005991990842787771\n",
      "right : 0.005942281837723303\n",
      "new : 0.005741586103397407\n",
      "way : 0.005672662128082932\n",
      "does : 0.005643696594475116\n",
      "make : 0.005429269467006293\n",
      "ve : 0.004878420810829421\n",
      "years : 0.004864967623517497\n",
      "going : 0.004673802945477092\n",
      "\n",
      "[Topic 1]\n",
      "use : 0.01244773450065218\n",
      "edu : 0.01126346019083091\n",
      "file : 0.009899941300464012\n",
      "program : 0.007018677072679079\n",
      "information : 0.006912590192704493\n",
      "available : 0.006853780538031965\n",
      "data : 0.006844735735509173\n",
      "like : 0.006720850461439024\n",
      "com : 0.006638007030520864\n",
      "does : 0.006444410483775505\n",
      "mail : 0.006332373100478364\n",
      "windows : 0.006315044731545731\n",
      "using : 0.006245824476178379\n",
      "key : 0.006193923454661427\n",
      "thanks : 0.005966709211683629\n",
      "drive : 0.005867773459190474\n",
      "used : 0.005786160568218421\n",
      "new : 0.005598871363047167\n",
      "know : 0.005332454197272463\n",
      "software : 0.0052909641829535685\n",
      "\n",
      "[Topic 2]\n",
      "ax : 0.68604063105398\n",
      "max : 0.05009072233000451\n",
      "g9v : 0.013485575446910127\n",
      "b8f : 0.01312055234170118\n",
      "a86 : 0.010845241613434808\n",
      "pl : 0.009068576651875217\n",
      "145 : 0.009035086598061274\n",
      "1d9 : 0.00751136296576256\n",
      "db : 0.007365489778231076\n",
      "1t : 0.005734915903615353\n",
      "0t : 0.0055280700414600615\n",
      "bhj : 0.005248219820615878\n",
      "3t : 0.0049075292549175895\n",
      "34u : 0.004858861948257405\n",
      "giz : 0.004712853760341552\n",
      "2di : 0.0046641840133856026\n",
      "25 : 0.00455579193842999\n",
      "55 : 0.004226656256448206\n",
      "wm : 0.004196597365437893\n",
      "14 : 0.004000263167269867\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 4\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "10 : 0.020775606453347972\n",
      "00 : 0.015890839621149584\n",
      "20 : 0.01370862334313512\n",
      "25 : 0.013601644987850624\n",
      "year : 0.013594635673486039\n",
      "game : 0.012965301259011644\n",
      "15 : 0.01281446793442682\n",
      "team : 0.012416605185666847\n",
      "12 : 0.01200158843407778\n",
      "11 : 0.011583394324247782\n",
      "new : 0.011183206859657917\n",
      "14 : 0.010589397196093081\n",
      "50 : 0.010024991529015564\n",
      "16 : 0.009833008522346818\n",
      "30 : 0.009705445241436112\n",
      "17 : 0.009353751988371137\n",
      "13 : 0.00920312973948339\n",
      "games : 0.00882507455924421\n",
      "18 : 0.008546215566491976\n",
      "40 : 0.007990140347664858\n",
      "\n",
      "[Topic 1]\n",
      "use : 0.013276949661338217\n",
      "edu : 0.012267336422302898\n",
      "file : 0.011566434456562217\n",
      "program : 0.008065671587659575\n",
      "available : 0.007681160188932213\n",
      "information : 0.007480435404680009\n",
      "data : 0.007434941850183757\n",
      "windows : 0.007399896886108286\n",
      "com : 0.007340443977972629\n",
      "like : 0.007152698596414284\n",
      "does : 0.007096002799591012\n",
      "mail : 0.007033384374437087\n",
      "using : 0.0069138854531540644\n",
      "thanks : 0.006892248571444717\n",
      "drive : 0.006788681866371032\n",
      "software : 0.006203964578431319\n",
      "used : 0.005965791036499411\n",
      "know : 0.005872577507701732\n",
      "space : 0.005619311028569055\n",
      "files : 0.005423185775158846\n",
      "\n",
      "[Topic 2]\n",
      "ax : 0.7694336151809155\n",
      "max : 0.05590620315698597\n",
      "g9v : 0.015123723771983425\n",
      "b8f : 0.01471432929726028\n",
      "a86 : 0.012162431729599878\n",
      "pl : 0.010169665152511954\n",
      "145 : 0.009876385397348647\n",
      "1d9 : 0.00842326069634432\n",
      "1t : 0.006430858200869931\n",
      "0t : 0.006198902004432631\n",
      "bhj : 0.005885015641233298\n",
      "3t : 0.005502916442890248\n",
      "34u : 0.005448338232758139\n",
      "giz : 0.0052846043454822305\n",
      "2di : 0.005230018373874223\n",
      "wm : 0.004676695241138819\n",
      "2tm : 0.004438521790306373\n",
      "75u : 0.004438519274554816\n",
      "7ey : 0.003565146249628605\n",
      "0d : 0.0031420297101864476\n",
      "\n",
      "[Topic 3]\n",
      "people : 0.015795911140035916\n",
      "don : 0.012761597900782592\n",
      "just : 0.011936575887743144\n",
      "think : 0.010999709691428675\n",
      "like : 0.010753928501201888\n",
      "know : 0.010324843095132303\n",
      "time : 0.008613581455996083\n",
      "god : 0.007712410358519168\n",
      "say : 0.007084985395544734\n",
      "said : 0.0068370461766585336\n",
      "good : 0.006593197861237151\n",
      "does : 0.006503028570432555\n",
      "did : 0.0063234864798292\n",
      "right : 0.006271311549565499\n",
      "way : 0.006110322587408365\n",
      "make : 0.005871057599517065\n",
      "ve : 0.005157698494190836\n",
      "believe : 0.005155273774726601\n",
      "going : 0.0050781770001162\n",
      "government : 0.004975447044627339\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 5\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "just : 0.01788986875726951\n",
      "don : 0.01755711550967227\n",
      "like : 0.016386048145262958\n",
      "know : 0.01524576861274901\n",
      "think : 0.014491329975876854\n",
      "good : 0.01276034503962281\n",
      "time : 0.011471087014762548\n",
      "people : 0.011219407785017697\n",
      "ve : 0.00858172660641144\n",
      "did : 0.008116535351038309\n",
      "say : 0.007875888166202504\n",
      "said : 0.007678048035553154\n",
      "way : 0.007542137242486355\n",
      "going : 0.007408145097860342\n",
      "god : 0.0074071880880660305\n",
      "really : 0.007234901025242828\n",
      "didn : 0.006692126916253543\n",
      "right : 0.006667378655305287\n",
      "ll : 0.006630570931032463\n",
      "make : 0.006455959390314091\n",
      "\n",
      "[Topic 1]\n",
      "use : 0.014335647877248416\n",
      "file : 0.01328286066822729\n",
      "edu : 0.011739138588429163\n",
      "windows : 0.008689762826347727\n",
      "program : 0.008652474941699548\n",
      "available : 0.008243612862010944\n",
      "using : 0.007731496553926784\n",
      "data : 0.00771268913740636\n",
      "drive : 0.007664969613300175\n",
      "mail : 0.0076012461807667756\n",
      "does : 0.007511142367459826\n",
      "com : 0.007497719500632217\n",
      "thanks : 0.007330801561702594\n",
      "software : 0.007274022927197559\n",
      "information : 0.007128754550897665\n",
      "like : 0.006685961466907405\n",
      "files : 0.006285441223146031\n",
      "version : 0.006170161674869499\n",
      "used : 0.006165552309976762\n",
      "ftp : 0.006036171238656391\n",
      "\n",
      "[Topic 2]\n",
      "10 : 0.024353843289057202\n",
      "00 : 0.019294286330864854\n",
      "25 : 0.015878390555695204\n",
      "20 : 0.015827143003794033\n",
      "15 : 0.015639730168933063\n",
      "12 : 0.015220670126098361\n",
      "11 : 0.014532714538286085\n",
      "space : 0.01422197905910557\n",
      "14 : 0.013202131067711839\n",
      "16 : 0.012547268137490181\n",
      "17 : 0.011526340617938358\n",
      "new : 0.011458708594472995\n",
      "13 : 0.011406768858740368\n",
      "18 : 0.011265357386054142\n",
      "30 : 0.0111825719875938\n",
      "50 : 0.011024513142984154\n",
      "1993 : 0.010454359422598706\n",
      "edu : 0.009536001262584526\n",
      "40 : 0.009457996901332411\n",
      "24 : 0.009286948231365763\n",
      "\n",
      "[Topic 3]\n",
      "people : 0.0178399256980091\n",
      "government : 0.009986220466769547\n",
      "law : 0.008382618743940959\n",
      "key : 0.008077617684382749\n",
      "use : 0.006580405294050447\n",
      "does : 0.0061882231764507\n",
      "mr : 0.006127602753321975\n",
      "god : 0.006059964929970658\n",
      "gun : 0.0060319137039072475\n",
      "state : 0.005851973769972758\n",
      "public : 0.005584139747842319\n",
      "don : 0.005417000586981113\n",
      "db : 0.005401382408991808\n",
      "right : 0.005245771246385886\n",
      "think : 0.0052296939644315525\n",
      "time : 0.005053885686666727\n",
      "fact : 0.005047232271562822\n",
      "believe : 0.005040817942256452\n",
      "make : 0.005036157216503352\n",
      "president : 0.004997775132483828\n",
      "\n",
      "[Topic 4]\n",
      "ax : 0.7685586473392421\n",
      "max : 0.05579735161776002\n",
      "g9v : 0.015105858999712783\n",
      "b8f : 0.014696929029633137\n",
      "a86 : 0.012147934763857596\n",
      "pl : 0.010123673188735622\n",
      "145 : 0.009894126036774125\n",
      "1d9 : 0.008413042149865624\n",
      "1t : 0.006422912313133303\n",
      "0t : 0.00619118545087805\n",
      "bhj : 0.005877678236271994\n",
      "3t : 0.005495997712040084\n",
      "34u : 0.005441482595443563\n",
      "giz : 0.005277918016504323\n",
      "2di : 0.005223393985632816\n",
      "wm : 0.004666691661868891\n",
      "2tm : 0.004432796847972883\n",
      "75u : 0.0044327955653250825\n",
      "7ey : 0.0035604137127846795\n",
      "0d : 0.0031378082029156784\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 6\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "like : 0.016852479569330253\n",
      "just : 0.01624908857341225\n",
      "don : 0.01432168553200919\n",
      "good : 0.014036402102538266\n",
      "think : 0.012288120987062752\n",
      "time : 0.010978590575114808\n",
      "know : 0.010511006543471495\n",
      "year : 0.00907102511248688\n",
      "ve : 0.008935106759377025\n",
      "new : 0.007749648930000116\n",
      "make : 0.007203414753732119\n",
      "game : 0.006984723567373855\n",
      "really : 0.006972371761322122\n",
      "got : 0.006968159542501393\n",
      "car : 0.006801159432506462\n",
      "way : 0.006668961381068071\n",
      "ll : 0.00664002541785189\n",
      "going : 0.006382567779276912\n",
      "better : 0.006331053807802954\n",
      "years : 0.006279998362410046\n",
      "\n",
      "[Topic 1]\n",
      "file : 0.014684399342629453\n",
      "use : 0.014099134117677867\n",
      "windows : 0.010163634821532165\n",
      "program : 0.009171897841265509\n",
      "does : 0.008546948639464508\n",
      "drive : 0.008483613061925693\n",
      "edu : 0.008365693100682532\n",
      "software : 0.008197953667931447\n",
      "thanks : 0.007978565222922599\n",
      "using : 0.00795437668874942\n",
      "available : 0.0077450955684646525\n",
      "data : 0.007258479078655452\n",
      "like : 0.007236749360850795\n",
      "mail : 0.007160801942510292\n",
      "files : 0.006983935748058578\n",
      "card : 0.006944423921097715\n",
      "version : 0.006902729510591516\n",
      "know : 0.006690034282180732\n",
      "ftp : 0.0064153556413757435\n",
      "window : 0.006276827647030434\n",
      "\n",
      "[Topic 2]\n",
      "10 : 0.021815903293409548\n",
      "edu : 0.018380285479533454\n",
      "00 : 0.01803483082137648\n",
      "25 : 0.014346641377615056\n",
      "15 : 0.014006473300625374\n",
      "20 : 0.013852657886336503\n",
      "12 : 0.013794799397224312\n",
      "11 : 0.013765440740076308\n",
      "space : 0.012364813795544105\n",
      "14 : 0.012087206613976949\n",
      "16 : 0.011563089974008333\n",
      "13 : 0.010760464070105856\n",
      "17 : 0.010744102379030224\n",
      "18 : 0.010581320050873477\n",
      "1993 : 0.010443881296028176\n",
      "new : 0.009941470382695364\n",
      "30 : 0.009414776602491102\n",
      "50 : 0.009184951410556724\n",
      "24 : 0.00894451550016259\n",
      "university : 0.008727802570915301\n",
      "\n",
      "[Topic 3]\n",
      "people : 0.014054283476713081\n",
      "key : 0.013209886651381786\n",
      "government : 0.012939236222488508\n",
      "law : 0.011905292967421755\n",
      "use : 0.010790266780879182\n",
      "gun : 0.008957383167459915\n",
      "public : 0.00861957495511258\n",
      "db : 0.008173298248367065\n",
      "state : 0.007676815674760135\n",
      "encryption : 0.006892653708530188\n",
      "used : 0.0068311658156582\n",
      "right : 0.006082953920414708\n",
      "does : 0.005995112438565502\n",
      "make : 0.0056963925958670066\n",
      "security : 0.005643721068334067\n",
      "states : 0.0056279950550295315\n",
      "don : 0.005382255862901452\n",
      "information : 0.0053745945845739915\n",
      "number : 0.0053469989355769765\n",
      "chip : 0.005195152121240399\n",
      "\n",
      "[Topic 4]\n",
      "people : 0.022516118466123676\n",
      "god : 0.018959647828147807\n",
      "don : 0.013423499150045686\n",
      "know : 0.013379340217087539\n",
      "said : 0.013067219553566612\n",
      "think : 0.011459323017328415\n",
      "just : 0.01136021830522689\n",
      "say : 0.010329683194737671\n",
      "did : 0.009679356460128415\n",
      "like : 0.009010560243454594\n",
      "jesus : 0.008763317267492172\n",
      "time : 0.00852354754555519\n",
      "believe : 0.008112383349375724\n",
      "does : 0.007360582520571009\n",
      "mr : 0.006698183820263974\n",
      "way : 0.006385355120724336\n",
      "armenian : 0.006155863843635275\n",
      "life : 0.005743294932649253\n",
      "world : 0.005738305093824698\n",
      "things : 0.005730775986188089\n",
      "\n",
      "[Topic 5]\n",
      "ax : 0.7683372376306211\n",
      "max : 0.055781543045980056\n",
      "g9v : 0.015101062245049828\n",
      "b8f : 0.014692250116136429\n",
      "a86 : 0.01214398848156198\n",
      "pl : 0.010118777713597074\n",
      "145 : 0.009906887185733303\n",
      "1d9 : 0.008410171226900636\n",
      "1t : 0.006420617594518415\n",
      "0t : 0.006188957377092919\n",
      "bhj : 0.005875536516572994\n",
      "3t : 0.005493974745897464\n",
      "34u : 0.005439469299154595\n",
      "giz : 0.005275946436316086\n",
      "2di : 0.0052214381605485945\n",
      "wm : 0.004667039118601915\n",
      "2tm : 0.004431068229625176\n",
      "75u : 0.004431067899009933\n",
      "7ey : 0.0035589358875741774\n",
      "cx : 0.0033107144431898075\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 7\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "like : 0.017829035658076316\n",
      "just : 0.016967162883081574\n",
      "good : 0.01588049515401469\n",
      "don : 0.0141335431317534\n",
      "think : 0.0120981370364379\n",
      "time : 0.011168025495623677\n",
      "know : 0.009810679625777237\n",
      "year : 0.009551121929627114\n",
      "ve : 0.0088104994782204\n",
      "new : 0.007977488084654876\n",
      "game : 0.007897277764507368\n",
      "really : 0.007436913414854621\n",
      "make : 0.007243692646968024\n",
      "car : 0.007226097605720573\n",
      "better : 0.007118680679023743\n",
      "got : 0.007051795626240988\n",
      "way : 0.006919298167688\n",
      "years : 0.006291294524424631\n",
      "team : 0.006184877235266438\n",
      "right : 0.006170630218570102\n",
      "\n",
      "[Topic 1]\n",
      "file : 0.015615156171206193\n",
      "use : 0.014186847573469306\n",
      "windows : 0.010326170351063793\n",
      "program : 0.009297015395362197\n",
      "drive : 0.008658166342739186\n",
      "software : 0.008390653074867919\n",
      "does : 0.008388253612561859\n",
      "using : 0.008048226290499717\n",
      "thanks : 0.008034733681416596\n",
      "edu : 0.00779893785267434\n",
      "available : 0.007692915544929095\n",
      "files : 0.007198408425196097\n",
      "card : 0.007169341564361409\n",
      "like : 0.007153422106531535\n",
      "data : 0.007031346468962803\n",
      "version : 0.0069552668750789935\n",
      "mail : 0.0068786447104656565\n",
      "know : 0.006546925634983991\n",
      "window : 0.0063578963219649304\n",
      "problem : 0.0063573851446217956\n",
      "\n",
      "[Topic 2]\n",
      "10 : 0.02205483183114173\n",
      "edu : 0.01949122102787648\n",
      "00 : 0.018245947709002286\n",
      "25 : 0.014388159272812328\n",
      "15 : 0.014186533582213516\n",
      "12 : 0.014021331089184158\n",
      "20 : 0.013983503222042955\n",
      "11 : 0.013962456662952205\n",
      "space : 0.013311825146601914\n",
      "14 : 0.012054671704414542\n",
      "16 : 0.011420189084481638\n",
      "13 : 0.010826406189119531\n",
      "17 : 0.010765250659190486\n",
      "1993 : 0.010577851372959477\n",
      "18 : 0.01044367907986733\n",
      "new : 0.010046128754133521\n",
      "30 : 0.009530351965624458\n",
      "50 : 0.00923767191424363\n",
      "24 : 0.00907196765849678\n",
      "university : 0.00878790722884679\n",
      "\n",
      "[Topic 3]\n",
      "key : 0.01800660906978901\n",
      "government : 0.014289806229344726\n",
      "use : 0.013392912240594185\n",
      "gun : 0.011940602564933383\n",
      "law : 0.011315145256077451\n",
      "db : 0.01102164971314816\n",
      "public : 0.01075905653405362\n",
      "people : 0.010666502862830683\n",
      "encryption : 0.00928684949995958\n",
      "chip : 0.007710219777670045\n",
      "used : 0.007466027415875519\n",
      "security : 0.00736173387282621\n",
      "state : 0.006983957793806743\n",
      "control : 0.00669931550303187\n",
      "keys : 0.006540765470165359\n",
      "number : 0.006536663158213455\n",
      "clipper : 0.0064013744620944795\n",
      "privacy : 0.006210533192503303\n",
      "information : 0.005989612884353921\n",
      "make : 0.005894046622100199\n",
      "\n",
      "[Topic 4]\n",
      "people : 0.027940492096063682\n",
      "don : 0.017733919199035594\n",
      "said : 0.017161206880436805\n",
      "know : 0.016831191462503392\n",
      "just : 0.01341203639960515\n",
      "think : 0.012978914267683337\n",
      "did : 0.011581335241183781\n",
      "like : 0.010359404215777348\n",
      "going : 0.009918865021318748\n",
      "say : 0.009660452752960496\n",
      "mr : 0.009638214840079087\n",
      "time : 0.009511945756932847\n",
      "didn : 0.009118489827328715\n",
      "armenian : 0.008272743373564519\n",
      "right : 0.007113216304728201\n",
      "want : 0.007072928593435861\n",
      "president : 0.007006147876163486\n",
      "armenians : 0.006773043856663243\n",
      "ve : 0.006768157094540119\n",
      "turkish : 0.006283041556202234\n",
      "\n",
      "[Topic 5]\n",
      "ax : 0.7684671646505757\n",
      "max : 0.05578130264602271\n",
      "g9v : 0.015103297797248798\n",
      "b8f : 0.0146944164219419\n",
      "a86 : 0.012145722768302806\n",
      "pl : 0.010119611664950635\n",
      "145 : 0.009896058325559759\n",
      "1d9 : 0.008411272996077761\n",
      "1t : 0.006421383205613338\n",
      "0t : 0.0061896837289940184\n",
      "bhj : 0.005876208656622851\n",
      "3t : 0.00549458468961693\n",
      "34u : 0.0054400681477923\n",
      "giz : 0.005276516325699887\n",
      "2di : 0.005221998811577763\n",
      "wm : 0.004667560554911541\n",
      "2tm : 0.00443149489202925\n",
      "75u : 0.004431494768341797\n",
      "7ey : 0.0035592147026092327\n",
      "cx : 0.0035218387215253883\n",
      "\n",
      "[Topic 6]\n",
      "god : 0.030466227611854337\n",
      "jesus : 0.014268222820384709\n",
      "people : 0.014077554732074806\n",
      "does : 0.013116235482918112\n",
      "believe : 0.010887373038089988\n",
      "israel : 0.00893202733613483\n",
      "christian : 0.008832688982006079\n",
      "bible : 0.008561705295926397\n",
      "true : 0.008338171554544157\n",
      "say : 0.008201329034166297\n",
      "think : 0.007794427072508322\n",
      "life : 0.007753579926468761\n",
      "church : 0.00754561925678073\n",
      "question : 0.007398164489349969\n",
      "religion : 0.006487366095143022\n",
      "faith : 0.0064375153505617285\n",
      "christ : 0.006280904439057572\n",
      "christians : 0.006258098302843449\n",
      "way : 0.006243292939675113\n",
      "point : 0.0060150203404209495\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 8\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "like : 0.017661001366634312\n",
      "just : 0.016866606325517113\n",
      "good : 0.016185365929317633\n",
      "don : 0.014220124835484625\n",
      "think : 0.011534850091803652\n",
      "time : 0.011208080557975517\n",
      "year : 0.010238062877606598\n",
      "know : 0.010061470659389418\n",
      "ve : 0.009095127769626954\n",
      "new : 0.008359869162322606\n",
      "game : 0.008229422828407875\n",
      "car : 0.00774755648361833\n",
      "really : 0.007473629499084065\n",
      "make : 0.0073903849890153\n",
      "got : 0.007227359419145756\n",
      "better : 0.007184019628962949\n",
      "way : 0.0068432272093408575\n",
      "ll : 0.006464853766416209\n",
      "team : 0.006457862531771675\n",
      "years : 0.006445413687116738\n",
      "\n",
      "[Topic 1]\n",
      "use : 0.014423728653978465\n",
      "file : 0.012381234894502144\n",
      "windows : 0.01215212554781952\n",
      "drive : 0.010204610555986631\n",
      "program : 0.009567308170146097\n",
      "does : 0.009358234567371048\n",
      "using : 0.008830810635433147\n",
      "card : 0.008343135987445774\n",
      "thanks : 0.008329872605144277\n",
      "software : 0.008231538644113391\n",
      "version : 0.007777843273367926\n",
      "like : 0.007644264115855421\n",
      "window : 0.007497499823544402\n",
      "problem : 0.007486899473908847\n",
      "dos : 0.007293619631793128\n",
      "image : 0.0071517838806143905\n",
      "know : 0.007099478844245626\n",
      "scsi : 0.006981563986623095\n",
      "files : 0.006936792549573861\n",
      "graphics : 0.006821902909722337\n",
      "\n",
      "[Topic 2]\n",
      "10 : 0.0321393562384871\n",
      "00 : 0.027401034052146143\n",
      "25 : 0.022109258720895357\n",
      "11 : 0.021123805405801156\n",
      "15 : 0.020901114367972533\n",
      "12 : 0.020749773281693463\n",
      "20 : 0.020730858616827184\n",
      "14 : 0.018664930285562584\n",
      "16 : 0.018208125180919188\n",
      "17 : 0.016491699262545866\n",
      "13 : 0.016362321996789678\n",
      "18 : 0.015392566359377288\n",
      "24 : 0.01373613339743992\n",
      "50 : 0.013221113556174459\n",
      "30 : 0.0132166210485867\n",
      "19 : 0.01257184418146775\n",
      "55 : 0.012422147000617457\n",
      "21 : 0.01235082366937971\n",
      "40 : 0.012341784342925264\n",
      "23 : 0.011258793493614343\n",
      "\n",
      "[Topic 3]\n",
      "key : 0.02513317274301868\n",
      "government : 0.01911745999987274\n",
      "gun : 0.016781967437429032\n",
      "use : 0.015563194721814753\n",
      "db : 0.015449234604217916\n",
      "law : 0.013641702326974241\n",
      "encryption : 0.012329348351085252\n",
      "chip : 0.011672097424691798\n",
      "people : 0.01008871096541619\n",
      "public : 0.01008382216625293\n",
      "keys : 0.009156908890308364\n",
      "clipper : 0.008977680963315216\n",
      "used : 0.008576281443049442\n",
      "guns : 0.0076810595002643365\n",
      "number : 0.007662337749576472\n",
      "control : 0.007304496313123503\n",
      "security : 0.007262506820998999\n",
      "state : 0.006920459974583096\n",
      "bit : 0.006900738744266417\n",
      "make : 0.006327598108311477\n",
      "\n",
      "[Topic 4]\n",
      "people : 0.024887139697562826\n",
      "said : 0.020969477483719582\n",
      "know : 0.015062099253364209\n",
      "don : 0.013073988635596516\n",
      "mr : 0.012202594755682358\n",
      "did : 0.011799291498782583\n",
      "armenian : 0.011151352972789018\n",
      "going : 0.010884620721062213\n",
      "didn : 0.010526919971877418\n",
      "president : 0.010119118720817959\n",
      "just : 0.009918308988397673\n",
      "time : 0.009522441062580036\n",
      "think : 0.009461136850750475\n",
      "armenians : 0.009129748124515875\n",
      "turkish : 0.00846922321899605\n",
      "like : 0.008284154648535543\n",
      "say : 0.00798588999782298\n",
      "went : 0.007768814905213023\n",
      "war : 0.0071963719818615044\n",
      "told : 0.006949709018715218\n",
      "\n",
      "[Topic 5]\n",
      "ax : 0.7711893587818562\n",
      "max : 0.0559542273907728\n",
      "g9v : 0.015156559803946878\n",
      "b8f : 0.014746229918060953\n",
      "a86 : 0.012188506975756679\n",
      "pl : 0.010150072669210678\n",
      "145 : 0.009899146883979172\n",
      "1d9 : 0.008440826927797771\n",
      "1t : 0.006443887787942356\n",
      "0t : 0.006211367948434271\n",
      "bhj : 0.005896781707820299\n",
      "3t : 0.005513806686448754\n",
      "34u : 0.005459096523850719\n",
      "giz : 0.005294965122729864\n",
      "2di : 0.005240254464921044\n",
      "wm : 0.004686555168112836\n",
      "2tm : 0.004446950043732632\n",
      "75u : 0.004446949965272228\n",
      "7ey : 0.0035715796395607833\n",
      "0d : 0.003147569707121064\n",
      "\n",
      "[Topic 6]\n",
      "edu : 0.020258005577058102\n",
      "information : 0.018054330270947612\n",
      "space : 0.015401707346716424\n",
      "mail : 0.014022074421910844\n",
      "com : 0.012276052521184716\n",
      "file : 0.011722450391621772\n",
      "send : 0.010574219459879944\n",
      "list : 0.01044350563583072\n",
      "available : 0.009372790383732387\n",
      "university : 0.00924721184464295\n",
      "new : 0.00894135853884839\n",
      "internet : 0.008889356186057327\n",
      "research : 0.008629968472021568\n",
      "data : 0.008398654806017737\n",
      "email : 0.008376287346056013\n",
      "nasa : 0.00837495038392586\n",
      "anonymous : 0.007980649560909735\n",
      "address : 0.007507564553577955\n",
      "ftp : 0.007503942031897771\n",
      "computer : 0.007312447254529496\n",
      "\n",
      "[Topic 7]\n",
      "god : 0.021211670592902524\n",
      "people : 0.01932816808591011\n",
      "think : 0.01264690641131503\n",
      "don : 0.012231786264765837\n",
      "does : 0.01205294816757208\n",
      "just : 0.010700113360800064\n",
      "believe : 0.010543942149482385\n",
      "jesus : 0.009893312119046212\n",
      "say : 0.009839471792239337\n",
      "know : 0.009515796489662921\n",
      "like : 0.008773553704861584\n",
      "time : 0.007256185089332646\n",
      "way : 0.007250653178948299\n",
      "true : 0.0070260883968934346\n",
      "question : 0.006725422490598352\n",
      "life : 0.0065649652969491335\n",
      "make : 0.006464572558674824\n",
      "good : 0.006440231163175911\n",
      "things : 0.0063245077339640346\n",
      "point : 0.006211538865620111\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 9\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "like : 0.016290666444561043\n",
      "just : 0.016109572302259314\n",
      "good : 0.01591998408545131\n",
      "don : 0.01345682928131839\n",
      "think : 0.01267532379437271\n",
      "year : 0.012031043180593363\n",
      "time : 0.011506208971841377\n",
      "game : 0.0095936756698935\n",
      "car : 0.008861658870142054\n",
      "new : 0.008491789796663759\n",
      "ve : 0.00829786816819498\n",
      "know : 0.0078067833055090775\n",
      "team : 0.007707870425712116\n",
      "make : 0.007674871319121082\n",
      "got : 0.007590768892011672\n",
      "years : 0.007521317603459092\n",
      "better : 0.007518048145453325\n",
      "really : 0.007346372828973151\n",
      "way : 0.007034423360390621\n",
      "power : 0.006566448188612813\n",
      "\n",
      "[Topic 1]\n",
      "drive : 0.015355231395836447\n",
      "use : 0.012559908422174576\n",
      "card : 0.011884579329690953\n",
      "file : 0.010715825975593545\n",
      "dos : 0.010264631487329215\n",
      "scsi : 0.010213702586526673\n",
      "disk : 0.00980979067318461\n",
      "software : 0.009777678629631031\n",
      "windows : 0.009089278470122012\n",
      "output : 0.008908559762058713\n",
      "program : 0.008850370933872623\n",
      "pc : 0.00879408146612969\n",
      "mac : 0.008695904083631504\n",
      "version : 0.008565966751210267\n",
      "bit : 0.008356445695350767\n",
      "available : 0.0079203296723622\n",
      "data : 0.007862687522739291\n",
      "using : 0.007559820136801286\n",
      "memory : 0.007374272389157403\n",
      "graphics : 0.006996138114178558\n",
      "\n",
      "[Topic 2]\n",
      "10 : 0.03259595500403585\n",
      "00 : 0.027458258523894066\n",
      "25 : 0.022241790633267193\n",
      "11 : 0.021703064879917067\n",
      "15 : 0.021100612635025648\n",
      "12 : 0.020702019961833518\n",
      "20 : 0.02065322306164026\n",
      "14 : 0.018963443912077133\n",
      "16 : 0.01758453084887431\n",
      "17 : 0.017079653636598024\n",
      "18 : 0.016823140412694174\n",
      "13 : 0.01681217395503477\n",
      "24 : 0.013625759689938424\n",
      "30 : 0.013231584439439018\n",
      "19 : 0.013195740624319595\n",
      "21 : 0.013059166511249538\n",
      "55 : 0.012878149024772143\n",
      "50 : 0.012614666603984732\n",
      "1993 : 0.012426639548509194\n",
      "40 : 0.012030551064452609\n",
      "\n",
      "[Topic 3]\n",
      "key : 0.023818024974100932\n",
      "government : 0.019567261091960143\n",
      "gun : 0.01617482571941761\n",
      "use : 0.015009725824652424\n",
      "db : 0.014752485504738958\n",
      "law : 0.014301152014809144\n",
      "encryption : 0.011972229607872706\n",
      "chip : 0.01096827240182045\n",
      "public : 0.010886803841178723\n",
      "people : 0.009753634308839448\n",
      "keys : 0.008747776718467298\n",
      "clipper : 0.008527619065533443\n",
      "state : 0.008383659756993565\n",
      "used : 0.008121190524315217\n",
      "control : 0.007863005251055525\n",
      "states : 0.007660020522145991\n",
      "security : 0.007562347080395562\n",
      "number : 0.007519688492694022\n",
      "guns : 0.007322302085650049\n",
      "private : 0.006143477546789012\n",
      "\n",
      "[Topic 4]\n",
      "people : 0.024464346175928676\n",
      "said : 0.022093259490519532\n",
      "know : 0.014674912728296376\n",
      "mr : 0.013071067999804589\n",
      "don : 0.012797259408076567\n",
      "did : 0.012199270708349988\n",
      "armenian : 0.01180250811552727\n",
      "going : 0.010906337510684072\n",
      "didn : 0.010680180547700683\n",
      "president : 0.010184523418148803\n",
      "just : 0.009834948699968149\n",
      "armenians : 0.009662803350892191\n",
      "time : 0.00944111430951646\n",
      "turkish : 0.008963691654384754\n",
      "think : 0.008659311396368904\n",
      "went : 0.008311690620289648\n",
      "like : 0.008110318918147803\n",
      "say : 0.007899000109468776\n",
      "war : 0.007325974947886871\n",
      "stephanopoulos : 0.007311246529044677\n",
      "\n",
      "[Topic 5]\n",
      "ax : 0.7713893701243456\n",
      "max : 0.05594034774767906\n",
      "g9v : 0.015160304465654709\n",
      "b8f : 0.014749868064037639\n",
      "a86 : 0.01219148115730736\n",
      "pl : 0.01013813587478889\n",
      "145 : 0.009902252401484778\n",
      "1d9 : 0.00844282850501952\n",
      "1t : 0.006445371213453421\n",
      "0t : 0.006212790769767541\n",
      "bhj : 0.005898122839172306\n",
      "3t : 0.005515048719574262\n",
      "34u : 0.0054603240347023485\n",
      "giz : 0.005296149684396151\n",
      "2di : 0.005241424828285781\n",
      "wm : 0.004704103574389207\n",
      "2tm : 0.004447914459546839\n",
      "75u : 0.004447914430547118\n",
      "7ey : 0.0035723168080773996\n",
      "0d : 0.0031481983089928917\n",
      "\n",
      "[Topic 6]\n",
      "edu : 0.020347867879674915\n",
      "space : 0.01961543936181583\n",
      "information : 0.018395816486433427\n",
      "mail : 0.01371460159720653\n",
      "new : 0.010852022757948507\n",
      "available : 0.010736155983573012\n",
      "send : 0.010504234777284545\n",
      "data : 0.010488996432860756\n",
      "university : 0.010305761977048035\n",
      "internet : 0.010292863424167912\n",
      "com : 0.010133053777612283\n",
      "list : 0.010111473752876273\n",
      "research : 0.009687684361364924\n",
      "nasa : 0.009677069669675794\n",
      "anonymous : 0.00905979536800793\n",
      "computer : 0.00880849572226842\n",
      "ftp : 0.008645832649986562\n",
      "email : 0.008418359739294336\n",
      "pub : 0.00838294977956079\n",
      "address : 0.007352260271304426\n",
      "\n",
      "[Topic 7]\n",
      "god : 0.021638014099719032\n",
      "people : 0.020368373457676902\n",
      "think : 0.013015923346792323\n",
      "don : 0.011873997247862969\n",
      "does : 0.011620495821675864\n",
      "believe : 0.010982743506964843\n",
      "just : 0.010320891798604945\n",
      "jesus : 0.01009179209040601\n",
      "say : 0.009923609097587726\n",
      "know : 0.008802096584230647\n",
      "like : 0.008445597225743822\n",
      "time : 0.007227534559055178\n",
      "true : 0.007177391886006824\n",
      "way : 0.007116106157627439\n",
      "life : 0.006691178334573095\n",
      "question : 0.006635274765708483\n",
      "good : 0.006511182744946799\n",
      "make : 0.006505774231532127\n",
      "things : 0.00634022634922335\n",
      "israel : 0.006314447957110356\n",
      "\n",
      "[Topic 8]\n",
      "know : 0.02084026377842588\n",
      "thanks : 0.02004603785923407\n",
      "like : 0.017712648630046273\n",
      "file : 0.016843371886460543\n",
      "use : 0.01592734499197518\n",
      "does : 0.015686372368193854\n",
      "don : 0.013452922920348517\n",
      "just : 0.012255548254389185\n",
      "problem : 0.01149670947391793\n",
      "help : 0.011339457759682016\n",
      "windows : 0.010578218245142773\n",
      "want : 0.009746468826662831\n",
      "files : 0.00964050249341239\n",
      "need : 0.009517829638799323\n",
      "ve : 0.009153773098338543\n",
      "edu : 0.00914708737862593\n",
      "mail : 0.00888890466116911\n",
      "using : 0.008269066288287105\n",
      "hi : 0.007652085721994726\n",
      "window : 0.007480680480211786\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Number of topics: 10\n",
      "\n",
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "good : 0.01592254908129389\n",
      "like : 0.01584763067117222\n",
      "just : 0.015714974597809874\n",
      "think : 0.014658035148150044\n",
      "don : 0.01336602502776772\n",
      "time : 0.012159230893303024\n",
      "year : 0.011442050656933937\n",
      "new : 0.008768217977593912\n",
      "years : 0.00843922077825026\n",
      "game : 0.008416482579473757\n",
      "make : 0.008318270139852606\n",
      "ve : 0.00805613381872604\n",
      "know : 0.00786552901690738\n",
      "going : 0.007357414502894818\n",
      "better : 0.007305177940555176\n",
      "really : 0.007282768897233162\n",
      "got : 0.007100242166187475\n",
      "way : 0.007020258221618519\n",
      "team : 0.006901091494924322\n",
      "car : 0.006860678090522195\n",
      "\n",
      "[Topic 1]\n",
      "drive : 0.025114459755967225\n",
      "card : 0.01904504522714293\n",
      "scsi : 0.01574807346309645\n",
      "disk : 0.015086151949241311\n",
      "use : 0.01311205775591249\n",
      "output : 0.012487568705565076\n",
      "file : 0.011474974819227298\n",
      "bit : 0.011450491727323115\n",
      "hard : 0.010426435918865882\n",
      "entry : 0.009962381704950415\n",
      "memory : 0.009892936703385204\n",
      "mac : 0.009531449582937765\n",
      "video : 0.009451338641933656\n",
      "drives : 0.009074000962777757\n",
      "pc : 0.0090703286112168\n",
      "windows : 0.008135023862197355\n",
      "16 : 0.00798823814975238\n",
      "bus : 0.007927283819698584\n",
      "controller : 0.007902057876189581\n",
      "program : 0.00784268458596016\n",
      "\n",
      "[Topic 2]\n",
      "10 : 0.032029220388909305\n",
      "00 : 0.026964330541393168\n",
      "25 : 0.021829691245344042\n",
      "15 : 0.0206063577495655\n",
      "11 : 0.02060435039972444\n",
      "20 : 0.02049576092374077\n",
      "12 : 0.020376684447947477\n",
      "14 : 0.018055470834875288\n",
      "16 : 0.016460265629681656\n",
      "13 : 0.016023012407497653\n",
      "17 : 0.016018903187206005\n",
      "18 : 0.015931416080248326\n",
      "30 : 0.013487129884524487\n",
      "50 : 0.013323083130055324\n",
      "24 : 0.01312690458019883\n",
      "19 : 0.012520561557602226\n",
      "55 : 0.01250023314639117\n",
      "21 : 0.01226424792228292\n",
      "40 : 0.01192815257556637\n",
      "22 : 0.011207231765196151\n",
      "\n",
      "[Topic 3]\n",
      "key : 0.02286656504255774\n",
      "government : 0.020853124988437204\n",
      "gun : 0.016677031123281342\n",
      "db : 0.015171423239595656\n",
      "law : 0.014649075730175279\n",
      "use : 0.014111126676381442\n",
      "people : 0.012470526208203755\n",
      "encryption : 0.010285224622752638\n",
      "public : 0.010243636203369282\n",
      "state : 0.009947341601966503\n",
      "chip : 0.009825392329557068\n",
      "keys : 0.008779074363170275\n",
      "clipper : 0.00860324584622739\n",
      "states : 0.00831822632566845\n",
      "control : 0.008226551411556786\n",
      "number : 0.008019292972255121\n",
      "used : 0.007703329865061306\n",
      "guns : 0.007611516117671719\n",
      "health : 0.0074015183803604065\n",
      "right : 0.006935759695920386\n",
      "\n",
      "[Topic 4]\n",
      "people : 0.02431129507816113\n",
      "said : 0.02424559321880428\n",
      "mr : 0.014104981910179977\n",
      "know : 0.013468288248487321\n",
      "armenian : 0.013342199409186437\n",
      "did : 0.01241301316278436\n",
      "don : 0.011539776119380056\n",
      "didn : 0.010944034432906041\n",
      "armenians : 0.01092331207436881\n",
      "turkish : 0.010132982460837872\n",
      "going : 0.009685278853463312\n",
      "went : 0.009482314763594613\n",
      "time : 0.009318116756590049\n",
      "president : 0.009271538629901233\n",
      "just : 0.009064415387593214\n",
      "war : 0.008272822237163071\n",
      "stephanopoulos : 0.008264930815089574\n",
      "jews : 0.007943422443838038\n",
      "say : 0.00778579920403193\n",
      "like : 0.007637486984592752\n",
      "\n",
      "[Topic 5]\n",
      "edu : 0.020183125501381658\n",
      "available : 0.01462208677587774\n",
      "window : 0.0145175966975094\n",
      "windows : 0.014508094728017699\n",
      "image : 0.014202932981631174\n",
      "version : 0.013567233933254529\n",
      "use : 0.013559184672232326\n",
      "software : 0.012291845460115209\n",
      "program : 0.011764165563758247\n",
      "graphics : 0.011586390886004624\n",
      "ftp : 0.010473416938225266\n",
      "server : 0.010462468444821689\n",
      "file : 0.010300988884546303\n",
      "using : 0.010279537804707145\n",
      "dos : 0.01018612827417759\n",
      "com : 0.010041868375764224\n",
      "display : 0.00950825069896609\n",
      "sun : 0.00841184948299741\n",
      "set : 0.008165843948144175\n",
      "motif : 0.008043634521501537\n",
      "\n",
      "[Topic 6]\n",
      "space : 0.02122937597565116\n",
      "information : 0.020605813849808002\n",
      "edu : 0.01308609835593781\n",
      "mail : 0.012122425926729597\n",
      "data : 0.012063443176285979\n",
      "new : 0.011922325100362566\n",
      "internet : 0.011805664499152158\n",
      "university : 0.011648063401924154\n",
      "nasa : 0.011260324762512446\n",
      "research : 0.011214434673689003\n",
      "computer : 0.010579251202203384\n",
      "send : 0.010082352367484646\n",
      "file : 0.009827860114832524\n",
      "anonymous : 0.009393114153924213\n",
      "list : 0.00884288720549653\n",
      "email : 0.008556680088168823\n",
      "technology : 0.008502540090233126\n",
      "address : 0.008453084833023794\n",
      "available : 0.008312068272885744\n",
      "com : 0.00801043664202556\n",
      "\n",
      "[Topic 7]\n",
      "god : 0.024214358169839564\n",
      "people : 0.019831156312240143\n",
      "think : 0.012548053861881268\n",
      "does : 0.011687743323611263\n",
      "jesus : 0.011293758739612999\n",
      "believe : 0.011272045532292002\n",
      "don : 0.010834089093902355\n",
      "say : 0.010114162234632518\n",
      "just : 0.009461924824189793\n",
      "know : 0.008276216060086664\n",
      "true : 0.007825806533253571\n",
      "like : 0.007419673532456676\n",
      "way : 0.007242156428134814\n",
      "life : 0.007196868607195379\n",
      "christian : 0.007055010808531854\n",
      "time : 0.0070227629792582015\n",
      "israel : 0.006934392612716913\n",
      "bible : 0.006745791914802942\n",
      "question : 0.0064075417201172645\n",
      "things : 0.00637398445911145\n",
      "\n",
      "[Topic 8]\n",
      "know : 0.026134132260814825\n",
      "like : 0.021317018148473164\n",
      "don : 0.020053165678589904\n",
      "thanks : 0.019992616431884633\n",
      "just : 0.017657856830905833\n",
      "does : 0.0150396282979183\n",
      "ve : 0.012652637710485493\n",
      "help : 0.01128297754768276\n",
      "want : 0.010651057192301491\n",
      "use : 0.010537774368367085\n",
      "mail : 0.010021837288920494\n",
      "problem : 0.00966309187171309\n",
      "edu : 0.009128531452268497\n",
      "good : 0.009109138814944394\n",
      "post : 0.008996488682684118\n",
      "file : 0.008922092098023733\n",
      "need : 0.008706364795622623\n",
      "looking : 0.0073136427958200205\n",
      "time : 0.007174443660697419\n",
      "bike : 0.007034238505179918\n",
      "\n",
      "[Topic 9]\n",
      "ax : 0.7717971886082466\n",
      "max : 0.055921756933680296\n",
      "g9v : 0.015168170327094601\n",
      "b8f : 0.014757516856658555\n",
      "a86 : 0.012197776886748507\n",
      "pl : 0.010147908394792498\n",
      "145 : 0.009912072552667453\n",
      "1d9 : 0.008447141779981918\n",
      "1t : 0.006448628175420717\n",
      "0t : 0.006215924619143329\n",
      "bhj : 0.005901090271008161\n",
      "3t : 0.005517813662944112\n",
      "34u : 0.005463059915719654\n",
      "giz : 0.00529879860622197\n",
      "2di : 0.005244044809233216\n",
      "wm : 0.004677791543976195\n",
      "2tm : 0.004450114768534644\n",
      "75u : 0.00445011475803213\n",
      "7ey : 0.003574054032872233\n",
      "0d : 0.0031497118103811115\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_values = range(2, 11)\n",
    "avg_coherences = []\n",
    "avg_num_unique_words = []\n",
    "\n",
    "for k in k_values:\n",
    "    lda_candidate = LatentDirichletAllocation(n_components=k, random_state=0)\n",
    "    lda_candidate.fit(tf)\n",
    "    topic_word_distributions = np.array([row / row.sum() for row in lda_candidate.components_])\n",
    "    print('-' * 80)\n",
    "    print('Number of topics:', k)\n",
    "    print()\n",
    "    print_top_words(topic_word_distributions, num_top_words, tf_vectorizer)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    avg_coherences.append(compute_average_coherence(topic_word_distributions, num_top_words, tf_vectorizer, False))\n",
    "    avg_num_unique_words.append(compute_average_num_unique_words(topic_word_distributions, num_top_words, tf_vectorizer, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average coherence')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_values, avg_coherences)\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Average coherence')\n",
    "\n",
    "# For differnt number of topics, I fitted a different LDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Average number of unique words')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(k_values, avg_num_unique_words)\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Average number of unique words')\n",
    "\n",
    "# For different topic models, the average number of unique words\n",
    "# We want coherence to be high and unique words to be high\n",
    "\n",
    "# Number of topic = 2, gets me high coherence and hogh unique words. BUT, for model 2, there are 2 topics: garbage and not garbage!\n",
    "# that doesnt help a lot\n",
    "\n",
    "# So, you want to tolerate a lower value for one of the metrics, but look at the actual topics to interpret what is going on\n",
    "# If we see resutlts of 4, starts to make more ssense but feels like still low\n",
    "# go to 5, starts to make a bit more sense\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('UDA_RIG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1a526087eeeed7e3a2dd7dbe3d8582928a2e8e80d21671a2817819bcd9f426f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

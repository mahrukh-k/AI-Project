{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "Visualizations  \n",
    "Preprocessing  \n",
    "Complete dataset  \n",
    "Split by government, media, public  \n",
    "**REmoveAdditional stopwords: add_stopwords = ['rt','train','southern','east','derailment','norfolk','palestine','february','u','ohio','month','week','news','please']**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "import pandas as pd\n",
    "# data: 20 news groups\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing (From Elaine's)\n",
    "df = pd.read_csv('../data/data_tweet.csv')\n",
    "\n",
    "# RIG (made some changes to stopwords)\n",
    "add_stopwords = set(['rt','train','southern','east','derailment','norfolk','palestine','february','u','ohio'])\n",
    "# add_stopwords_2 = set(['zÅ‚oÅ¼onych', 'ÃŸtrong', 'Ã¡ramellÃ¡tÃ¡sa', 'Ã¡ramellÃ¡tÃ¡st', 'Ã§a', 'Ã§avuÅŸ', 'Ã§evre', 'Ã§Ä±karak',\n",
    "#        'Ã§Ä±kardÄ±', 'Ã§Ä±kma', 'Ã§Ä±kmÄ±ÅŸ', 'Ã©cologique', 'Ã©conomiques', 'Ã©dito', 'Ã©lections', 'Ã©limination', 'Ã©lites', 'Ã©loignÃ©s', 'Ã©niÃ¨me',\n",
    "#        'Ã©quipes', 'Ã©tait', 'Ã©tat', 'Ã©tats', 'Ã©tatsunis', 'Ã©tranges', 'Ã©trangÃ¨res', 'Ã©tÃ©', 'Ã©ventuellement', 'Ã©verything', 'Ã©viter',\n",
    "#        'Ã©voque', 'Ãªtre', 'Ã¶nce', 'Ã¶nÃ¼nde', 'Ãºltima', 'Ã¼b', 'Ã¼ber', 'Ä¥mmm', 'Å›rodka', 'Å›rodowiska', 'Å›rodowisko', 'Å¥Ã§h', 'Å¼e', 'ËˆapÉ™thÄ“', 'Î¬Î»Î»Î·', 'Î­Ï‡ÎµÎ¹', 'Î±Î»Î±Î¼Ï€Î¬Î¼Î±', 'Î±Î»Î»Î·', 'Î±Ï€Î±Î½Ï„Î·Î¸Î¿ÏÎ½', 'Î±Ï€ÎµÏÎ³Î¹Î±8Î¼Î±ÏÏ„Î·',\n",
    "#        'Î±Ï€ÏŒÎ´ÎµÎ¹Î¾Î·', 'Î±ÏÏ‡Î¯ÏƒÎ¿Ï…Î½', 'Î²Î¯Î½Ï„ÎµÎ¿', 'Î²ÎµÎ»Î¿Ï€Î¿Ï…Î»Î¿Ï‚', 'Î²Î¿Ï…Î»Î®', 'Î³ÎµÎ½Î½Î¹Î¿ÏÎ½Ï„Î±Î¹', 'Î´Î¹ÎºÎ±Î¹Î¿Î»Î¿Î³Î¯ÎµÏ‚', 'Î´Î¿Ï…Î»ÎµÎ¹Î¬', 'Î´Ï…ÏƒÏ„ÏÏ‡Î·Î¼Î±', 'ÎµÎ¯Ï‡Îµ', 'ÎµÎºÏ„ÏÎ¿Ï‡Î¹Î±ÏƒÏ„Î·ÎºÎ±Î½', 'ÎµÎ½Î±', 'ÎµÏ€Î¹Î¸Î­ÏƒÎµÎ¹Ï‚', 'ÎµÏÏ‰Ï„Î·Î¼Î±Ï„Î±', 'Î¶Î·Ï„Î®ÏƒÎ¿Ï…Î¼Îµ',\n",
    "#        'Î¸Î±', 'ÎºÎ±Î¹', 'ÎºÎ±Î¹ÏÎ¿', 'ÎºÎ±Î½Î±ÎºÎ·', 'ÎºÎ¹', 'Î¼Î­ÏƒÏ‰', 'Î¼ÎµÏƒÎ±', 'Î¼Î·Î½Î±', 'Î¼Î¹Î±', 'Î½Î±', 'Î½Î´_Ï„ÎµÎ»Î¿Ï‚', 'Î¿Î¹', 'Î¿Ï‡Î±Î¹Î¿', 'Ï€Î¿Î»Î»Î¬', 'Ï€Î¿ÏƒÎ±', 'Ï€Î¿ÏƒÎµÏ‚', 'ÏƒÎ®Î¼ÎµÏÎ±', 'ÏƒÎµ', 'ÏƒÏ„Î·', 'ÏƒÏ„Î·Î½', 'ÏƒÏ„Î¿', 'ÏƒÏ„ÏÎ¬Ï„Î¿Ï‚', 'ÏƒÏ…Î¼Ï€Ï„Ï‰ÏƒÎ·', 'ÏƒÏ‡Î­ÏƒÎ·', \n",
    "#        'Ï„Î±', 'Ï„ÎµÎ»ÎµÏ…Ï„Î±Î¹Î¿', 'Ï„ÎµÎ¼Ï€Î·', 'Ï„ÎµÎ¼Ï€Î·_Î´Î¿Î»Î¿Ï†Î¿Î½Î¹Î±', 'Ï„Î¹', 'Ï„Î¹Ï‚', 'Ï„Î¿', 'Ï„Î¿Î½', 'Ï„ÏÎµÎ½Î±', 'Ï„ÏÎµÎ½Î¿', 'Ï„ÏÎ¿Î»', 'Ï„ÏÏÎ±', 'Ï‡Ï‰ÏÎµÏ‚', 'Ğ°Ğ²Ğ°Ñ€Ğ¸ĞµĞ¹', 'Ğ°Ğ»Ğ°Ğ±Ğ°Ğ¼Ğµ', 'Ğ°Ğ¼ĞµÑ€Ğ¸', 'Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ¹', 'Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ¼', 'Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·', \n",
    "#        'Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑŒ', 'Ğ²ĞµĞ»Ğ¸Ñ‡ĞµĞ·Ğ½Ğ¸Ğ¹', 'Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹', 'Ğ³Ñ€ÑƒĞ·Ğ¾Ğ²Ğ¾Ğ¹', 'Ğ´Ğ°Ğ²Ğ½Ğ¾', 'Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€', 'Ğ´Ğ»Ñ', 'Ğ´Ğ¾', 'ĞµÑ‰Ñ‘', 'Ğ¶Ğ´', 'Ğ¶ĞµĞ»ĞµĞ·Ğ½Ğ¾Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ‹Ğ¹', 'Ğ·Ğ°', 'Ğ¸ald', 'Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ñ', 'ĞºĞ°Ğº', 'ĞºĞ°ĞºĞ¾Ğ¹', 'ĞºĞ°Ñ€Ğ¾Ğ»Ğ¸Ğ½Ğ°', \n",
    "#        'ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸', 'ĞºĞ¾Ğ½Ğ³Ñ€ĞµÑÑĞ¾Ğ¼', 'Ğ»ĞµĞºÑĞ¸Ğ½Ğ³Ñ‚Ğ¾Ğ½Ğµ', 'Ğ¼Ğ°Ñ€Ñ‚Ğ°', 'Ğ½Ğ°', 'Ğ½Ğ°ÑĞµĞ»ĞµĞ½Ğ¸Ñ', 'Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾', 'Ğ½ĞµÑ‚', 'Ğ½Ğ¾Ñ€Ñ€Ğ¸Ñ', 'Ğ¾Ğ³Ğ°Ğ¹Ğ¾', 'Ğ¾Ğ´Ğ¸Ğ½', 'Ğ¾Ğ½Ğ¸', 'Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ÑÑ‚ÑĞ¼Ğ¸', 'Ğ¿ĞµÑ€ĞµĞ´', 'Ğ¿Ğ¾', 'Ğ¿Ğ¾ĞµĞ·Ğ´', 'Ğ¿Ğ¾ĞºĞ°Ğ·Ñ‹Ğ²Ğ°ĞµÑ‚', 'Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»',\n",
    "#        'Ğ¿ÑƒÑ‚Ğ¸', 'Ñ€Ğ°ÑÑ…Ğ¾Ğ´Ñ‹', 'Ñ€ĞµĞ»ÑŒÑĞ¾Ğ²', 'Ñ€ĞµĞ¼Ğ¾Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸', 'ÑĞµĞ²ĞµÑ€Ğ½Ğ°Ñ', 'ÑĞ¸Ñ‚ÑƒĞ°Ñ†Ğ¸Ğ¸', 'ÑĞ¾ĞºÑ€Ğ°Ñ‚Ğ¸Ğ»Ğ°', 'ÑĞ¾Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğ¹', 'ÑĞ¾ÑˆĞµĞ»', 'ÑĞ¾ÑˆÑ‘Ğ»', 'ÑÑ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ', 'ÑÑƒĞ±Ğ±Ğ¾Ñ‚Ñƒ', 'ÑÑ‡Ñ‘Ñ‚Ñƒ', 'Ñ‚Ğ°Ğ¼', 'Ñ‚Ğ¾Ğ³Ğ¾', 'Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ğ¾Ğ¹',\n",
    "#        'Ñ‡Ğ°Ğº', 'Ñ‡Ğ°ÑĞ¾Ğ²', 'Ñ‡Ñ‚Ğ¾', 'ÑˆÑ‚Ğ°Ñ‚', 'ÑˆÑ‚Ğ°Ñ‚Ğµ', 'Ñepublican', '××œ×‘××”', '×”×¤×¢×', 'Ø£ÙˆÙ‡Ø§ÙŠÙˆ', 'Ø§Ø²', 'Ø§Ø³Ù¾Ø±ÛŒÙ†Ú¯ÙÛŒÙ„Ø¯', 'Ø§Ù„Ø£Ù…ÙŠØ±ÙƒÙŠØ©', 'Ø§Ù„Ø¨Ø¶Ø§Ø¦Ø¹',\n",
    "#        'Ø§Ù„Ù‚Ø¶Ø¨Ø§Ù†', 'Ø§Ù†Ø¨Ø¹Ø§Ø«', 'Ø§ÙˆÙ‡Ø§ÛŒÙˆ', 'Ø§ÛŒÙ†', 'Ø¨Ø£ÙˆÙ‡Ø§ÙŠÙˆ', 'Ø¨Ø§Ø±', 'Ø¨Ø­Ø§Ø¯Ø«', 'Ø¨Ø¹Ø¯', 'ØªÙ†Ø­Ø±Ù', 'Ø«Ø§Ù†', 'Ø¬Ù†ÙˆØ¨ÛŒ', 'Ø­Ø¯ÙŠØ¯ÙŠØ©', 'Ø®Ø§Ø±Ø¬', 'Ø®Ø±ÙˆØ¬', 'Ø®Ù„Ø§Ù„', 'Ø¯Ø±', 'Ø¯ÛŒÚ¯Ø±', 'Ø±ÛŒÙ„', 'Ø³Ø§Ù…Ø©', 'Ø³ÙƒØ©', 'Ø´Ø¯', 'Ø´Ù‡Ø±', 'Ø¹Ø±Ø¨Ø©', 'Ø¹Ù†',\n",
    "#        'ÙÙŠ', 'Ù‚Ø·Ø§Ø±', 'Ù‚Ø·Ø§Ø±Ù‡Ø§ÛŒ', 'Ù„Ø«Ø§Ù†ÙŠ', 'Ù…Ø±Ø©', 'Ù…Ø³Ø§Ø±Ù‡Ø§', 'Ù…Ù…Ø§Ø«Ù„', 'Ù…Ù†', 'Ù…ÙˆØ§Ø¯', 'Ù†ÙˆØ±ÙÙˆÙ„Ùƒ', 'Ù†ÙˆØ±ÙÙˆÙ„Ú©', 'ÙˆØ§Ø­Ø¯', 'ÙˆÙ„Ø§ÙŠØ©', 'ÛŒÚ©ÛŒ', 'à¸à¸‡à¸²à¸™à¹ƒà¸«à¸', 'à¸à¸ª', 'à¸à¸²', 'à¸à¸²à¸£à¸•à¸à¸£à¸²à¸‡à¸„à¸£', 'à¸à¸²à¸£à¸—', 'à¸à¸²à¸£à¸ªà¸­à¸šà¸ªà¸§à¸™à¹€à¸š', 'à¸à¹à¸¥', 'à¸‚à¸­à¸‡à¸šà¸£',\n",
    "#        'à¸„à¸§à¸£à¸ˆà¸°à¹€à¸', 'à¸‡à¸‚à¸­à¸‡', 'à¸‡à¸„à¸™', 'à¸‡à¸—', 'à¸‡à¸™', 'à¸‡à¸¡', 'à¸‡à¹€à¸¥', 'à¸‡à¹à¸•', 'à¸Šà¸²à¸§à¸š', 'à¸à¸­à¹€à¸¡à¸£', 'à¸”à¸‚', 'à¸”à¸³à¹€à¸™', 'à¸•à¸à¸£à¸²à¸‡à¹€à¸›', 'à¸•à¸à¸¥à¸‡à¸¡à¸²à¹€à¸›', 'à¸•à¸§', 'à¸—à¸¡', 'à¸—à¸³à¹ƒà¸«', 'à¸—à¹à¸—', 'à¸™à¸à¸²à¸£à¸—à¸³à¸¥à¸²à¸¢à¸ªà¸²à¸£à¹€à¸„à¸¡', 'à¸™à¸à¸²à¸£à¹ƒà¸™à¸ à¸²à¸„à¸•à¸°à¸§', 'à¸™à¸„', 'à¸™à¸„à¸£', 'à¸™à¸‡',\n",
    "#        'à¸™à¸šà¸£à¸£à¸¢à¸²à¸à¸²à¸¨', 'à¸™à¸à¸šà¸§', 'à¸™à¸£à¸–à¹„à¸Ÿà¸šà¸£à¸£à¸—', 'à¸™à¸­à¸à¸ˆà¸²à¸à¸™', 'à¸™à¸­à¸­à¸à¸‚à¸­à¸‡à¸ªà¸«à¸£', 'à¸™à¹€à¸”',\n",
    "#        'à¸™à¹€à¸à¸£à¸²à¸°', 'à¸™à¹„à¸›à¸­à¸¢', 'à¸šà¸•', 'à¸šà¸™à¸Š', 'à¸šà¸£', 'à¸œà¸¥à¹€à¸ª', 'à¸à¸™à¸à¸£à¸”', 'à¸à¸™à¸—',\n",
    "#        'à¸à¸šà¸§', 'à¸à¸­à¹€à¸„', 'à¸¢à¸—', 'à¸¥à¸­à¸¢à¸‚', 'à¸§à¸™', 'à¸ªà¸²à¸£à¹€à¸„à¸¡', 'à¸ªà¸³à¸™', 'à¸«à¸²à¸à¸à¸™à¸•à¸',\n",
    "#        'à¸­à¸', 'à¸­à¸‡à¸•', 'à¸­à¸™', 'à¸­à¸™à¸œ', 'à¸­à¸¢à¸«à¸¡à¸²à¸‚à¸­à¸‡à¹€à¸„', 'à¸­à¸²à¸ˆà¸ˆà¸°à¸—à¸³à¹ƒà¸«', 'à¸­à¹€à¸›', 'à¸­à¹„à¸”',\n",
    "#        'à¸²à¸•à¸²à¸¢à¹à¸¥', 'à¸²à¸™', 'à¸²à¸™à¸„à¸™à¸™', 'à¸²à¸™à¸•', 'à¸²à¸™à¹„à¸›', 'à¸²à¸›à¸£à¸°à¹€à¸ à¸—', 'à¸²à¸›à¸¥', 'à¸²à¸¢à¹à¸¥à¸°à¸–',\n",
    "#        'à¸²à¸§', 'à¸²à¸«à¸¡à¸²à¸‚à¸­à¸‡à¹€à¸„', 'à¸²à¸­à¸­à¸à¸¡à¸²à¸”', 'à¸²à¸­à¸­à¸à¸¡à¸²à¹€à¸”', 'à¹€à¸', 'à¹€à¸„', 'à¹€à¸”', 'à¹€à¸›',\n",
    "#        'à¹€à¸', 'à¹€à¸à¸£à¸²à¸°à¸¡', 'à¹€à¸¥', 'à¹€à¸«à¸¥', 'à¹à¸•', 'à¹à¸¥à¸°à¹„à¸¡', 'à¹ƒà¸™à¹à¸­à¸•à¹à¸¥à¸™à¸•à¸²',\n",
    "#        'â„‚â„ğ”¼â„â„•ğ•†ğ”¹ğ•ğ•ƒ', 'ã†ã²ãƒ¼ãƒ¡ãƒ¢', 'ã‹æœˆåˆ†ã®é£Ÿæ–™', 'ãŒè¬ç½ªã—', 'ãŒè­°ä¼šã§è¨¼è¨€ã™ã‚‹ä¸­',\n",
    "#        'ãŒè­°ä¼šã«æ±ãƒ‘ãƒ¬ã‚¹ãƒãƒŠã«ã¤ã„ã¦', 'ãŒé‡è¦ãªã‚»ã‚¯ã‚¿ãƒ¼ã«ã¨ã£ã¦ã¯ä»–äººäº‹ã§ã¯ç„¡ã„ã‚“ã ã‚ˆã­', 'ã“ã®1ã‚«æœˆã§2ä»¶ç›®',\n",
    "#        'ã“ã®ä¼šç¤¾ã¯æ°‘é–“æ¥­è€…ã¨ã—ã¦', 'ã“ã®éƒ¨åˆ†', 'ãã®ç±³å›½ã®ç‰©æµã§ã™ãŒ___________', 'ã¡ã‚‡ã£ã¨è„±ç·šã—éãã ã‚',\n",
    "#        'ã¨å‘Šã’ã‚‹ã‚ãšã‹æ•°æ™‚é–“å‰ã«ç™ºç”Ÿã—ã¾ã—ãŸ', 'ã¨è¨ºæ–­ã•ã‚ŒãŸ', 'ã©ã†ã—ãŸã‚¢ãƒ¡ãƒªã‚«', 'ã«å¯¾ã™ã‚‹åˆ‘äº‹å‘Šè¨´ãŒã‚ã‚Šãã†ã ã¨å ±ã˜ã¦ã„ã‚‹',\n",
    "#        'ã¯ã£ãã‚Šã•ã›ã‚‹ãŸã‚ã«', 'ã¾ãŸã—ã¦ã‚‚', 'ã¾ãŸã¾ãŸ', 'ã¾ãŸã‚¢ãƒ¡ãƒªã‚«ã‚ªãƒã‚¤ã‚ªã§åŒ–å­¦å“ã‚’è¼‰ã›ãŸåˆ—è»Šã®è„±ç·šäº‹æ•…',\n",
    "#        'ã¾ãŸã‚ªãƒã‚¤ã‚ªã§è²¨ç‰©åˆ—è»ŠãŒè„±ç·š', 'ã‚¢ãƒ¡ãƒªã‚«ä¸‰å¤§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ†ãƒ¬ãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ•°æ™‚é–“é…ã‚Œã§è¦–è´ã§ãã‚‹æ™‚ä»£ã¨ãªã£ã¦ã„ã‚‹',\n",
    "#        'ã‚¢ãƒ©ãƒãƒå·ã‚«ãƒ«ãƒ•ãƒ¼ãƒ³éƒ¡ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯ã‚µã‚¶ãƒ³ã®ãŒè„±ç·šã—ãŸ', 'ã‚¢ãƒ©ãƒãƒå·é€Ÿå ±', 'ã‚¤ãƒ¼ã‚¹ãƒˆ',\n",
    "#        'ã‚¤ãƒ¼ã‚¹ãƒˆãƒ‘ãƒ¬ã‚¹ãƒãƒŠã®åˆ—è»Šè„±ç·šäº‹æ•…ã®é‡‘éŠ­çš„è²¬ä»»ã‚’è¿½åŠã™ã‚‹ã“ã¨ã‚’æ±‚ã‚ã‚‹', 'ã‚¦ã‚§ã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ‹ã‚¢å·',\n",
    "#        'ã‚¦ã‚§ã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ‹ã‚¢å·ã§å²©çŸ³æ»‘ã‚Šã«è¡çªã—ãŸå¾Œ', 'ã‚ªãƒã‚¤ã‚ª', 'ã‚ªãƒã‚¤ã‚ªã®è„±ç·šäº‹æ•…', 'ã‚ªãƒã‚¤ã‚ªãƒã‚§ãƒ«ãƒãƒ–ã‚¤ãƒª',\n",
    "#        'ã‚ªãƒã‚¤ã‚ªå·ãŒãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ã‚ªãƒã‚¤ã‚ªå·ã§2å›ç›®ã®è„±ç·šäº‹æ•…', 'ã‚ªãƒã‚¤ã‚ªå·ã§ã®åŒç¤¾ã®åˆ—è»Šã®è„±ç·šäº‹æ•…ã¯',\n",
    "#        'ã‚ªãƒã‚¤ã‚ªå·ã§ã®æœ‰æ¯’åŒ–å­¦ç‰©è³ªæµå‡ºã®å…ƒãƒˆãƒ©ãƒ³ãƒ—å½“å±€è€…ã‚’éé›£', 'ã‚ªãƒã‚¤ã‚ªå·ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ã‚ªãƒã‚¤ã‚ªå·ã§åˆ¥ã®åˆ—è»ŠãŒè„±ç·š',\n",
    "#        'ã‚ªãƒã‚¤ã‚ªå·ã«æ•‘æ€¥éšŠå“¡é¤Šæˆã®ãŸã‚ã®æ–°æ–½è¨­ã‚’å»ºè¨­', 'ã‚ªãƒã‚¤ã‚ªå·ã®ãƒ™ãƒƒãƒ‰ãƒ•ã‚©ãƒ¼ãƒ‰çˆ†ç™ºã«å‘ã‹ã£ãŸãŒ', 'ã‚ªãƒã‚¤ã‚ªå·ã®åŒ–å­¦ç‰©è³ªæµå‡ºå¾Œ',\n",
    "#        'ã‚ªãƒã‚¤ã‚ªå·ã®å¸æ³•é•·å®˜ãŒãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ã‚ªãƒã‚¤ã‚ªå·ã¯', 'ã‚ªãƒã‚¤ã‚ªå·ã‚¤ãƒ¼ã‚¹ãƒˆ', 'ã‚ªãƒã‚¤ã‚ªå·ã‚¯ãƒ©ãƒ¼ã‚¯éƒ¡ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯',\n",
    "#        'ã‚ªãƒã‚¤ã‚ªå·ã‚¯ãƒªãƒ¼ãƒ–ãƒ©ãƒ³ãƒ‰ã§ç«æ›œæ—¥ã®æ—©æœ', 'ã‚ªãƒã‚¤ã‚ªå·ã‚¹ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§', 'ã‚ªãƒã‚¤ã‚ªå·ã‚¹ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã®',\n",
    "#        'ã‚ªãƒã‚¤ã‚ªå·ã‚¹ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ã‚ªãƒã‚¤ã‚ªå·ãƒ‘ãƒ¬ã‚¹ãƒãƒŠå‘¨è¾ºã®æœ‰æ¯’åœ°åŸŸã«ã¤ã„ã¦',\n",
    "#        'ã‚ªãƒã‚¤ã‚ªå·è¥¿ä¸­å¤®ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ã‚ªãƒã‚¤ã‚ªå·è¥¿ä¸­å¤®éƒ¨ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ã‚ªãƒãƒæ”¿æ¨©ã§å¼·åŒ–ã•ã‚ŒãŸé‰„é“ã®å®‰å…¨å¯¾ç­–ãŒ',\n",
    "#        'ã‚ªãƒ¼ãƒãƒ¼', 'ã‚¯ãƒ©ãƒ¼ã‚¯éƒ¡ã®ä½æ°‘1500äººãŒåœé›»ã®ç‚º', 'ã‚¯ãƒªãƒ³ãƒˆãƒ³ç©ºæ¸¯ã®å¤–ã§é£›è¡Œæ©Ÿäº‹æ•…ã§äº¡ããªã£ãŸä¹—çµ„å“¡ã¯',\n",
    "#        'ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ‹…å½“ä¼æ¥­ã¨ã¯ç„¡ç¸ã ãŒ', 'ã‚³ãƒ­ãƒŠç¦ã¨éœ²å®‡æˆ¦äº‰ã§ç”£æ¥­å¾“äº‹è€…æ¿€æ¸›ã—ã¦ã¾ã™ã—ã­ã‡', 'ã‚µã‚¶ãƒ³', 'ã‚µã‚¶ãƒ³ceo',\n",
    "#        'ã‚µã‚¶ãƒ³ã«å¯¾ã—', 'ã‚µã‚¶ãƒ³ã®', 'ã‚µã‚¶ãƒ³ã®ãŒã¾ãŸã‚‚ã‚„è„±ç·š', 'ã‚µã‚¶ãƒ³ã®ã‚ªãƒã‚¤ã‚ªå·ã®åˆ—è»Šã®è„±ç·šäº‹æ•…ã§é‰„é“ã‚»ãƒ³ã‚µãƒ¼ãŒè„šå…‰ã‚’æµ´ã³ã‚‹',\n",
    "#        'ã‚µã‚¶ãƒ³ã®ã‚¹ãƒãƒ¼ã‚¯ã‚¹ãƒ‘ãƒ¼ã‚½ãƒ³ã¯', 'ã‚µã‚¶ãƒ³ã®åˆ—è»Š20ä¸¡ãŒè„±ç·š', 'ã‚µã‚¶ãƒ³ã®åˆ—è»ŠãŒè„±ç·šã—ãŸ', 'ã‚µã‚¶ãƒ³ã®å¾“æ¥­å“¡ãŒæ­»äº¡ã—ãŸ',\n",
    "#        'ã‚µã‚¶ãƒ³ã®è²¨ç‰©åˆ—è»ŠãŒè„±ç·š', 'ã‚µã‚¶ãƒ³ã®è²¨ç‰©åˆ—è»ŠãŒè„±ç·šã—', 'ã‚µã‚¶ãƒ³ã®è²¨ç‰©åˆ—è»ŠãŒè„±ç·šã—ãŸã“ã¨ã‚’å—ã‘ã¦',\n",
    "#        'ã‚µã‚¶ãƒ³ã®è²¨ç‰©åˆ—è»ŠãŒè„±ç·šã—ãŸå¾Œ', 'ã‚µã‚¶ãƒ³ã‚’æè¨´', 'ã‚µã‚¶ãƒ³ãƒˆãƒ¬ã‚¤ãƒ³ãŒè„±ç·š', 'ã‚µã‚¶ãƒ³ç¤¾ã®è„±ç·šäº‹æ•…ã«ã‚ˆã‚Š',\n",
    "#        'ã‚µã‚¶ãƒ³ç¤¾ã®è»Šä¸¡ãŒè„±ç·šã—ãŸå¾Œ', 'ã‚µã‚¶ãƒ³ç¤¾ã‚’æè¨´ã—ã¾ã—ãŸ', 'ã‚µã‚¶ãƒ³é‰„é“ãŒè„±ç·š', 'ã‚µã‚¶ãƒ³é‰„é“ãŒè„±ç·šã—',\n",
    "#        'ã‚µã‚¶ãƒ³é‰„é“ã®è²¨ç‰©åˆ—è»ŠãŒå†ã³è„±ç·š', 'ã‚¹ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ãƒ€ãƒ³ãƒ—ã‚«ãƒ¼ãŒè¡çªã—', 'ãƒ†ãƒ­ã‹',\n",
    "#        'ãƒ‡ã‚£ãƒ¼ã‚¼ãƒ«ãŒå·ã«æµå‡º', 'ãƒˆãƒ©ãƒ³ãƒ—æ”¿æ¨©ä¸‹ã§ãƒ­ãƒ“ãƒ¼ã‚¤ãƒ³ã‚°ã§å¼±ä½“åŒ–ã•ã‚ŒãŸä»¶ã¯æ³¨ç›®ã§', 'ãƒˆãƒ¬ã‚¤ãƒ«ã®è„±ç·šã¯',\n",
    "#        'ãƒãƒ¼ã‚¹ã‚«ãƒ­ãƒ©ã‚¤ãƒŠå·ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ãƒã‚¨ã®ã‚ˆã†ã«è½ã¡ã‚‹', 'ãƒãƒ¼ã‚¸ãƒ‹ã‚¢å·',\n",
    "#        'ãƒ‘ãƒ¬ã‚¹ãƒãƒŠã®é‰„é“è„±ç·šäº‹æ•…ç¾å ´ã‹ã‚‰ã®æ±šæŸ“ã•ã‚ŒãŸå»ƒæ£„ç‰©ã®ã™ã¹ã¦ã®å‡ºè·ã‚’ä¸­æ­¢ã—', 'ãƒ‘ãƒ¬ã‚¹ãƒãƒŠåˆ—è»Šã®è„±ç·šã‚’è¨´ãˆã‚‹',\n",
    "#        'ãƒ–ãƒ©ãƒƒã‚¯ãƒ­ãƒƒã‚¯', 'ãƒ–ãƒ©ãƒƒã‚¯ãƒ­ãƒƒã‚¯ã¯ä½•ã‚’è€ƒãˆã¦ã„ã‚‹ã®ã ã‚ã†ã‹', 'ãƒ–ãƒ«ãƒ¼ãƒ ãƒãƒ¼ã‚°ã«ã‚ˆã‚‹ã¨',\n",
    "#        'ãƒšãƒ³ã‚·ãƒ«ãƒãƒ‹ã‚¢å·çŸ¥äº‹ãŒã‚ªãƒã‚¤ã‚ªå·å—éƒ¨ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯åˆ—è»Šã®å¢œè½äº‹æ•…ã‚’éé›£ã—', 'ãƒ›ãƒ¯ã‚¤ãƒˆãƒã‚¦ã‚¹ã¯å…±å’Œå…šå“¡',\n",
    "#        'ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ç”¨ç´ æ', 'ãƒ¬ãƒ¼ãƒ«ãŒ', 'ãƒ´ã‚¡ãƒ³ã‚¬ãƒ¼ãƒ‰', 'ä¸€å‘¨åä¼šæœ‰äººå¸®æˆ‘å¼€å¥–', 'ä¸ä¸ºåˆ«äººæƒ³æƒ³', 'ä¸–ç•Œç·Šæ€¥æ”¾é€',\n",
    "#        'ä¸œå·´å‹’æ–¯å¦', 'ä¸œå·´å‹’æ–¯å¦çš„ç§¯æ°´æ— éœ€æ‹…å¿ƒhttps', 'ä¸­åœ‹äºº', 'ä¹—å‹™å“¡ã¯æµå‡ºã—ãŸç—•è·¡ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸ',\n",
    "#        'ä¹Ÿæ²¡æœ‰åŒ–å­¦ç‰©è´¨æ³„æ¼', 'äº‹ä»¶æ²’äººå—å‚·', 'äº‹æ•…å¯¼è‡´1500åå±…æ°‘åœç”µ', 'äº¤é€šéƒ¨é•·', 'äººãŒè² å‚·', 'äººå®¶å¤©æ°£é‚£éº¼å¥½',\n",
    "#        'ä»Šå›ã¯ã‚¢ãƒ©ãƒãƒå·ã§åˆ¥ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ä»Šå¤©åœ¨ç¶²ä¸Šåˆ†äº«ä¸€å€‹', 'ä»Šå¹´ã ã‘ã§è„±ç·šäº‹æ•…ã¯10ä»¶ä»¥ä¸Š',\n",
    "#        'ä»Šåº¦ã¯ã‚¢ãƒ©ãƒãƒå·ã§ã‚‚è„±ç·šäº‹æ•…ã£ã¦', 'ä»Šå¾Œç±³å›½æ”¿åºœã‚‚é‰„é“ä¼šç¤¾ã«å¯¾å¿œè¿«ã‚‹ã“ã¨ã«ãªã‚Šãã†', 'ä»Šæ—¥è©²å·', 'ä»–ä¸€ç›´èˆ‡',\n",
    "#        'ä¼¼ä¹æ¯ä¸€å¤©éƒ½æ˜¯å¦‚æ­¤', 'ä½†æˆ‘ä»¬è¢«å‘ŠçŸ¥æ²¡æœ‰å±é™©ç‰©è´¨', 'ä½†æœ‰ç•¶åœ°æ°‘çœ¾è¡¨ç¤º', 'ä½æ°‘ã«é¿é›£ã™ã‚‹ã‚ˆã†æŒ‡ç¤º',\n",
    "#        'ä½æ°‘ã¯å±‹å†…é€€é¿ã‚’å‘½ã˜ã‚‰ã‚Œã¾ã—ãŸ', 'ä¾†è‡ª', 'ä¿„äº¥ä¿„', 'ä¿„äº¥ä¿„å·', 'ä¿„äº¥ä¿„å·åˆ—è»Šè„«è»Œäº‹æ•…åƒè­°é™¢è½è­‰æœƒç›´æ’­',\n",
    "#        'ä¿„äº¥ä¿„å·åŒ–å­¸å“æ´©æ¼å¾Œæœ‰å¦ä¸€åˆ—è«¾ç¦å…‹å—æ–¹éµè·¯å…¬å¸åœ¨åŒ—å¡ç¾…èŠç´å·å‰‹æ˜Ÿæ•¦è„«è»Œ', 'ä¿„äº¥ä¿„å·åˆæœ‰ä¸€è¾†norfolk', 'ä¿„äº¥ä¿„å·å·é•·',\n",
    "#        'ä¿„äº¥ä¿„å·ç«è½¦äº‹æ•…èƒŒåçš„è¯ºç¦å…‹å—æ–¹å…¬å¸å’Œå…¶ä»–é“è·¯å…¬å¸èŠ±è´¹æ•°ç™¾ä¸‡ç¾å…ƒè¿›è¡Œè¥é”€å’Œæ¸¸è¯´', 'ä¿„äº¥ä¿„å·è¯ºç¦å…‹å—éƒ¨è´§è¿åˆ—è½¦è„±è½¨å',\n",
    "#        'ä¿„åˆä¿„å·å†æœ‰ç«è»Šå‡ºè»Œ', 'å‡çš„', 'å…ˆæœˆã«ã¯ã‚¤ãƒ¼ã‚¹ãƒˆãƒ‘ãƒ¬ã‚¹ãƒãƒ³ã§ã‚ˆã‚Šå¤§è¦æ¨¡ãªè„±ç·šäº‹æ•…ãŒç™ºç”Ÿã—ãŸ',\n",
    "#        'å…ˆæœˆã‚¤ãƒ¼ã‚¹ãƒˆãƒ‘ãƒ¬ã‚¹ãƒãƒŠã®ç”ºã§æœ‰æ¯’åŒ–å­¦ç‰©è³ªãŒæµå‡ºã—ãŸè„±ç·šäº‹æ•…ã«ã¤ã„ã¦', 'å…ˆæœˆã‚ªãƒã‚¤ã‚ªå·ã§ç™ºç”Ÿã—ãŸãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯',\n",
    "#        'å…ˆæœˆé ­ã¨ä»Šæœˆé ­ã«ã‚ªãƒã‚¤ã‚ªå·ã§è„±ç·šäº‹æ•…èµ·ã“ã—ãŸã®ã«ç¶šã', 'å…ˆé€±åœŸæ›œæ—¥ã«ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯ã‚µã‚¶ãƒ³ã®åˆ—è»ŠãŒã‚ªãƒã‚¤ã‚ªå·ã§è„±ç·šã—',\n",
    "#        'å…‹æ‹‰å…‹ç¸£', 'å…¨ã¦ã®æ–°ã—ã„ä½å®…ãƒ­ãƒ¼ãƒ³ã‚„ãƒ­ãƒ¼ãƒ³ãªã©ã‹ã‚‰åˆ©ç›Šã‚’å¾—ã‚‹ã§ã—ã‚‡ã†', 'å…¬è¡†ã¸ã®å±é™ºã¯å ±å‘Šã•ã‚Œã¦ã„ãªã„',\n",
    "#        'å…¬è¡†ã¸ã®å±é™ºã¯å ±å‘Šã•ã‚Œã¦ã„ãªã„ã¨è¿°ã¹ãŸ', 'å…¶å¾Œå†ç™¼ç”Ÿé‡‘å±¬å» çˆ†ç‚¸äº‹ä»¶', 'å†ç™¼ç”Ÿç«è»Šå‡ºè»Œäº‹ä»¶', 'å‡ºæ–¼é«˜åº¦è¬¹æ…çš„åŸå‰‡',\n",
    "#        'å‡ºè»Œ', 'å‡ºè»Œåœ°é»', 'å‡ºè»Œç™¼ç”Ÿåœ¨41è™Ÿå·éš›å…¬è·¯é™„è¿‘', 'å‡ºè½¨æ˜¯å¸¸æ€', 'åˆ‘äº‹å‘Šç™ºã‚’è¡Œã†', 'åˆ—è»Šã¨',\n",
    "#        'åˆ—è»Šã®è„±ç·šã«é–¢ã™ã‚‹ä¸Šé™¢å…¬è´ä¼šã§å¤‰æ›´ã‚’ç´„æŸ', 'åˆ—è»Šè„±ç·šäº‹æ•…', 'åˆ—è»Šè„±ç·šäº‹æ•…ã«é–¢ã™ã‚‹ä¸Šé™¢å…¬è´ä¼šã§å¤‰æ›´ã‚’ç´„æŸ',\n",
    "#        'åˆ¥ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'åˆ¥ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯ã‚µã‚¶ãƒ³åˆ—è»ŠãŒè„±ç·š', 'åˆ¥ã®åˆ—è»ŠãŒè„±ç·š', 'å‰17åˆ†é’Ÿæ˜¯æ©æ€¨', 'åŒ–å­¦æ€§æ°—ç®¡æ”¯ç‚',\n",
    "#        'åarmy', 'ås', 'å—è¯ºç¦å…‹æœ‰ä»€ä¹ˆé—®é¢˜å—', 'å±é™ºç‰©ã¯ç©ã‚“ã§ã„ãªã„ã¨å½“å±€ãŒç™ºè¡¨', 'å±é™ºç‰©å‡¦ç†ç­ãŒç¾å ´ã«æ€¥è¡Œ',\n",
    "#        'åˆä¸€èµ·', 'å¦ä¸€åˆ—è¯ºç¦å…‹å—æ–¹è´§è¿åˆ—è½¦å‡ºè½¨', 'å¦ä¸€åˆ—è¯ºç¦å…‹å—éƒ¨è´§è¿åˆ—è½¦å‡ºè½¨äº†', 'å¯æ†çš„',\n",
    "#        'åŒã˜é‰„é“ä¼šç¤¾ã®è²¨ç‰©åˆ—è»ŠãŒã¾ãŸè„±ç·šäº‹æ•…', 'åŒæ™‚ã«è¤‡æ•°åœ°åŒºã§åœé›»ã‚‚èµ·ãã¦ã„ã‚‹ã‚‰ã—ã„', 'å‘¼ç±²å¸‚æ°‘å°±åœ°é¿é›£',\n",
    "#        'å› ä¸ºæˆ‘ä»¬å¬è¯´ä¿„äº¥ä¿„å·åˆæœ‰ä¸€åˆ—ç«è½¦å‡ºè½¨äº†', 'å› ä¸ºç¦ç¦„å¯¿åˆ©ç”¨æˆ‘èµšäº†åƒä¸‡è¿˜æ¶å¿ƒæˆ‘', 'åœŸæ›œæ—¥', 'åœ¨å…§çš„è¯é‚¦å®˜å“¡ä¿æŒè¯ç¹«',\n",
    "#        'åœ¨é€±å…­è«¾ç¦å…‹å—æ–¹springfieldç«è»Šå‡ºè»Œå¾Œ', 'å †ç–Šåœ¨è·¯è»Œæ—', 'å ±å‘Šèªª', 'å ±å¾©æ”»æ’ƒ', 'å¢®è»Œ',\n",
    "#        'å¤§æ‰‹è£½è–¬ä¼šç¤¾ã¨ã®ç¹‹ãŒã‚Šã¯', 'å¤§è¦æ¨¡ãª', 'å¤§é‡é€®æ•', 'å¤ªç¼ºå¾·äº†', 'å¦™ãªãƒ¢ãƒã¯æ¼ã‚ŒãŸã‚Šã—ã¦ãªã„ãã†ã ã‘ã©', 'å®‰å…¨',\n",
    "#        'å¯¼è‡´åŒ—å¡ç½—æ¥çº³å·å»¶è¯¯', 'å°‘ãªãã¨ã‚‚', 'å°±åœ¨å‡ å¤©å‰', 'å±…æ°‘è¢«å‘ŠçŸ¥å°±åœ°é¿éš¾', 'å±‹å†…é€€é¿å‘½ä»¤',\n",
    "#        'å·å†…ã§ã¯æœ‰å®³ç‰©è³ªãŒæµå‡ºã—ãŸå…ˆæœˆã®äº‹æ•…ã«ç¶šãï¼’ä¾‹ç›®', 'å¸ƒè’‚å‰æ ¼è¯´ç¾å›½ä¸€å¹´æœ‰ä¸€åƒ', 'å¹¼å…æ•™è‚²ãªã©', 'å½“åœ°è€ç™¾å§“æ€•å•¥å‘¢',\n",
    "#        'å½“å±€ã«ã‚ˆã‚‹ã¨', 'å½“è©²ä¼æ¥­ãŒæ¬ é™¥ãƒ†ã‚¹ãƒˆã‚’å®Ÿæ–½ã—ã¦ã„ã‚‹ã“ã¨ã¯æ‰¿çŸ¥', 'å¾ˆå¯èƒ½ä¼šè¢«æŠ¥å¤', 'å¿…ç„¶çš„ã«æ··ã–ã‚Šåˆã£ã¦ã„ã¾ã—ãŸ',\n",
    "#        'å¿…éœ€å“ã‚’æ‰‹å…ƒã«ç”¨æ„ã—ã¦ãŠãã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™', 'æ€éº¼æœƒæœ‰äººç—…å€’', 'æ€ æ…¢ã‹', 'æƒ…å ±å¼±è€…ã•ã‚“ã¸', 'æ„å¤–',\n",
    "#        'æˆ‘å¿æ— å¯å¿é€‰æ‹©æ›å…‰ç¦ç¦„å¯¿åˆ©ç”¨è­¦å¯Ÿå†»å¡', 'æˆ¦äº‰ã‹', 'æŠ•ç¨¿æ™‚é–“', 'æ‹‰æ–‡çº³', 'æ‹œç™»', 'æ‹¡æ•£å¸Œæœ›',\n",
    "#        'æ“šå ±é€™åˆ—è¼‰è²¨ç«è»Šå…±æœ‰212å¡è»Šå»‚', 'æ”¶é’±è§£å†»', 'æ”¿åºœç£ä¿ƒå±…æ°‘å°±åœ°å¯»æ±‚åº‡æŠ¤', 'æ”¿æ²»å®¶ã‚‚ã‚ã‹ã£ã¦ã„ã‚‹ã‹æ€ªã—ã„', 'æ–¯æ‰˜æœ¬ç»´å°”å¸‚', 'æ–¯æ™®æ—è²å°”å¾·', 'æ—¥ãƒŸã‚·ã‚·ãƒƒãƒ”å·ã«ãƒ•ãƒƒåŒ–æ°´ç´ é…¸ãŒæµå‡º', 'æ—¥æœ¬ã®ãƒ¡ãƒ‡ã‚£ã‚¢ã¯ç†è§£ã—ã¦ã„ãªã„',\n",
    "#        'æ˜¾ç„¶æ­£åœ¨è¿›è¡Œ', 'æ™‚ä»£ã§ç«‹ã¡å¾€ç”Ÿã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¦ã„ã¾ã™', 'æš‚æ—¶æ²¡æœ‰äººå‘˜ä¼¤äº¡', 'æœèµ·ãã¦ç›®ã«é£›ã³è¾¼ã‚“ã§ããŸãƒ‹ãƒ¥ãƒ¼ã‚¹', 'æœ¬ä¾†æ˜¯é€—å¤§å®¶ä¸€æ¨‚çš„', 'æ¥è‡ª', 'æ±ãƒ‘',\n",
    "#        'æ±ãƒ‘ãƒ¬ã‚¹ãƒãƒŠã«é‰„é“æ¥­ç•Œã®ãƒ­ãƒ“ã‚¤ã‚¹ãƒˆã«ãã‚Œã‚‰ã‚’å£²ã‚Šæ¸¡ã—ãŸã“ã¨ã«å¯¾ã™ã‚‹è¬ç½ªã‚’è² ã£ã¦ã„ã¾ã™', 'æ±ãƒ‘ãƒ¬ã‚¹ãƒãƒŠã®ä½æ°‘ã¨åŠ´åƒè€…ãŒ',\n",
    "#        'æ±ãƒ‘ãƒ¬ã‚¹ãƒãƒŠã®æµå‡ºå¾Œ', 'æ¡‘å¾·æ–¯åŸº', 'æ±šæŸ“ã®è¦æ¨¡ãŒå¤§ããã¦è©±é¡Œã«ãªã£ã¦ã„ã‚‹ã‘ã©', 'æ²’æƒ³åˆ°ç¬¬ä¸€æ¢ç•™è¨€ä¾¿æ˜¯ç°¡çŸ­ä¸­æ–‡å­—çš„',\n",
    "#        'æ³¢è’‚å¥‡å¿', 'æ´©æ¼åŒ–å­¸ç‰©æ±¡æŸ“ç•¶åœ°', 'æµ·å¤–tech', 'æµ·å¤–ç§‘å­¦', 'æ·±ããŠè©«ã³ç”³ã—ä¸Šã’ã¾ã™', 'ç«æ›œæ—¥',\n",
    "#        'ç‹—å­åœ¨æ—ä¸­ç‹‚å¥”çš„è¦–é »', 'ç¾å ´ã«èª¿æŸ»å“¡ã‚’æ´¾é£', 'ç¾é‡‘', 'ç’°å¢ƒä¿è­·åºã¯ã‚ªãƒã‚¤ã‚ªå·ã®æœ‰æ¯’åˆ—è»Šäº‹æ•…ã«ã‚ˆã‚‹æ±šæŸ“å»ƒæ£„ç‰©ã®å‡ºè·ã®ä¸€æ™‚åœæ­¢ã‚’å‘½ä»¤', 'ç’°å¢ƒä¿è­·åºã¯ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ç”¨å¿ƒæ·±ã•ã‹ã‚‰',\n",
    "#        'ç•¶ä¸­20ç¯€å‡ºè»Œ', 'ç•¶å±€ç¨±è»Šä¸Šä¸¦ç„¡è¼‰æœ‰æœ‰æ¯’åŒ–å­¸ç‰©', 'ç–¯ç‹‚ä¸‰æœˆ', 'çš®ç‰¹', 'çš®ç‰¹å¸ƒè’‚å‰æ ¼', 'çœ‹åˆ°æœ‰æ¯’åŒ–å­¸ç‰©å“è™•ç†çµ„çš„è»Šè¼›å‡ºç¾', 'ç­‰ã§ã‚ã‚Š', 'ç±³ã§ã¯å¹´å¹³å‡1700ä»¶ã®è„±ç·šäº‹æ•…ãŒç™ºç”Ÿ', 'ç±³å›½é‹è¼¸å®‰å…¨å§”å“¡ä¼šã¯',\n",
    "#        'ç±³ç‰ˆ', 'ç·Šæ€¥äº‹å‹™ç®¡ç†å±€', 'ç¹¼ä¸Šæœˆç™¼ç”Ÿç«è»Šå‡ºè»Œäº‹ä»¶', 'ç¾Šæ°‘ä»¬åˆ«çæ‹…å¿ƒäº†', 'ç¾å›½', 'ç¾å›½ä¿„äº¥ä¿„å·å†å‘ç”Ÿåˆ—è½¦è„±è½¨äº‹æ•…', 'ç¾å›½ä¿„äº¥ä¿„å·è½½æœ‰æ¯’åŒ–å­¦å“åˆ—è½¦è„±è½¨',\n",
    "#        'ç¾å›½ç¯ä¿å±€é˜»æ­¢è¯ºç¦å…‹å—æ–¹å…¬å¸æ¸…é™¤æœ‰æ¯’çš„ä¿„äº¥ä¿„å·ç«è½¦è„±è½¨æ®‹éª¸', 'è‚‡äº‹çš„ç«è»ŠåŒæ¨£å±¬æ–¼norfolk', 'è„±ç·šã•ã›ã‚‹ã²ã¨ã‚’ãƒ‡ã‚£ãƒ¬ãƒ¼ãƒ©ãƒ¼ã¨å‘¼ã¶', 'è„±ç·šã¯derail', 'è„±ç·šã°ã£ã‹ã‚Šã—ã¦ã‚‹ãª', 'è„±è»Œ', 'è„±è½¨',\n",
    "#        'è‡ªåˆ†ãŸã¡ãŒå¼•ãèµ·ã“ã—ãŸæ··ä¹±ã®å¾Œå§‹æœ«ã®ä»£å„Ÿã‚’æ‰•ã†ã“ã¨ã«ãªã‚‹', 'è¥¿è¡Œå°å®', 'èª¿æŸ»å“¡ã‚’æ´¾é£ã—ã¦ã„ã‚‹', 'è«¾ç¦å…‹å—æ–¹å…¬å¸', 'è­‰å¯¦', 'è­°ä¼šã®å…±å’Œå…šå“¡ã¨å…ƒãƒˆãƒ©ãƒ³ãƒ—æ”¿æ¨©ã®å½“å±€è€…ã¯', 'è¯ºç¦å…‹å—æ–¹å£°ç§°å±…æ°‘æ²¡ä»€ä¹ˆå¥½æ‹…å¿ƒçš„', 'è²¨ç‰©åˆ—è»ŠãŒè„±ç·š',\n",
    "#        'è»½æ²¹ãŒå·ã«æµå‡º', 'è½¬å‘è¯„è®ºæ­¤æ¡æ¨ç‰¹æŠ½ä¸€ä¸‡u', 'è¿˜æœ‰uå•†å®‰å¾½çœèŠœæ¹–é˜²ç©ºæ´å¸®äººè—åŒ¿çš„æ•°åäº¿èµ„äº§', 'è¿™å¯èƒ½æ˜¯æˆ‘çš„æœ€åä¸€æ¡è§†é¢‘', 'è¿™æ¬¡æ˜¯åœ¨åŒ—å¡ç½—æ¥çº³å·', 'è¿™æ¬¡æ˜¯åœ¨æ–¯æ™®æ—è²å°”å¾·', 'é€Ÿå ±', 'é€±æœ«ã«ã¯',\n",
    "#        'é©åˆ‡ãªå‡¦åˆ†ã‚’ç¢ºå®Ÿã«ã™ã‚‹ã‚ˆã†å‘½ã˜ãŸ', 'é¿é›£ã™ã‚‹ã‚ˆã†æ±‚ã‚ã‚‰ã‚Œã¦ã„ã¾ã—ãŸ', 'é¿é›£æŒ‡ç¤ºã‚‚', 'é‚å…‹å¾·æº«', 'éƒ½æ•£äº†å§', 'é„‰ä¸‹', 'é‰„é“ã®', 'é‰„é“äº‹æ¥­è€…ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯',\n",
    "#        'é‰„é“ä¼šç¤¾ãŒäº‹æ•…é˜²æ­¢ã‚’ç›®çš„ã¨ã—ã¦ä½¿ç”¨ã—ã¦ã„ã‚‹ã‚»ãƒ³ã‚µãƒ¼ã®å½¹å‰²ã«æ”¹ã‚ã¦æ³¨ç›®ãŒé›†ã¾ã£ã¦ã„ã¾ã™', 'éµè·¯', 'é“è·¯', 'é˜¿å°¼æ–¯é¡¿', 'é˜¿æ‹‰å·´é¦¬ç¾å ´è„«è»Œç‚ºè½è­‰æœƒåŠ©èˆˆ', 'é˜¿æ‹‰å·´é©¬å·', 'é™„è¿‘å±…æ°‘è¢«è¦æ±‚å°±åœ°é¿éš¾',\n",
    "#        'é™°è¬€é›†åœ˜çš„è²èŠå¾·å…¬å¸å°±æ˜¯ä¿„äº¥ä¿„å·ç«è»Šè»Šç¦çš„ä¸»è¦è‚¡æ±ä¹‹ä¸€', 'é«˜é€Ÿé‰„é“è·¯ç·šã®ç­†é ­æ ªä¸»ã¯ä¸­å¤®éŠ€è¡Œå®¶', 'é»‘å²©è²èŠå¾·æ“æœ‰ä¹‹ä¸€è‚¡æ±',\n",
    "#        'ê±°ì£¼í•˜ëŠ”', 'ê³µë¬´ì›ë“¤ì´', 'ê·¼ì²˜ì—', 'ë‚´ë ¸ë‹¤', 'ë…¸í½', 'ëŒ€í”¼ì†Œë¡œ', 'ë¡œì´í„°', 'ë§Œì—', 'ëª…ë ¹ì„', 'ë°”ì´ë“ ì€', 'ë°œìƒí•´', 'ë°©ë¬¸í•˜ì§€', 'ë²ˆë„', 'ë²ˆì§¸', 'ë²ˆì§¸ë¡œ', 'ì‚¬ê³ ', 'ì‚¬ê³ ê°€', 'ì„œë˜', 'ì•Šì€',\n",
    "#        'ì—°ë£¨ëœ', 'ì—´ì°¨', 'ì—´ì°¨ê°€', 'ì˜¤í•˜ì´ì˜¤', 'ì˜¤í•˜ì´ì˜¤ì—ì„œ', 'ì˜¤í•˜ì´ì˜¤ì£¼ì—ì„œ', 'ì£¼ë¯¼ë“¤ì—ê²Œ', 'ì§€ì—­', 'ì°¾ì€', 'ì² ë„ê°€', 'ì¶œì²˜', 'íƒˆì„ ', 'íƒˆì„ í•´', 'í† ìš”ì¼', 'íŠ¸ëŸ¼í”„', 'í˜„ì¥', 'í˜„ì§€'])\n",
    "\n",
    "\n",
    "remove_stopwords = set([\"no\", \"not\", \"nor\", \"against\", \"aren't\", \"couldn't\", \"didn't\", \n",
    "                                                   \"doesn't\", \"don't\", \"haven't\", \"hadn't\", \"hasn't\", \"isn't\", \"mightn't\",\n",
    "                                                   \"mustn't\", \"needn't\", \"shouldn't\", \"wasn't\", \"weren't\", \"wouldn't\"])\n",
    "# remove stopwords (not impacting sentiment analysis) and punctuations\n",
    "stop_words = set(stopwords.words('english')) - remove_stopwords\n",
    "stop_words = set(stopwords.words('english')).union(add_stopwords)\n",
    "# stop_words = set(stopwords.words('english')).union(add_stopwords_2)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "# normalize pos tags\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "      return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "  else:         \n",
    "      return None\n",
    "\n",
    "\n",
    "# lemmatize each token with pos tag\n",
    "def lemma_token(row):\n",
    "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), row))\n",
    "  lemmatized = []\n",
    "  for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, just append the original token\n",
    "        lemmatized.append(word)\n",
    "    else:       \n",
    "        # else use the pos tag to lemmatize the token\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
    "  return lemmatized\n",
    "\n",
    "\n",
    "# RIG (edit: added http://)\n",
    "# remove urls\n",
    "df['Message_no_url'] = df['MESSAGE'].apply(lambda x: \" \".join([word for word in x.split(\" \") if not(word.startswith((\"https://\", 'http://')))]))\n",
    "\n",
    "\n",
    "# tokenize MESSAGE and remove stopwords\n",
    "df['Text'] = df['Message_no_url'].map(tokenizer.tokenize)\n",
    "df['Text'] = df['Text'].apply(lambda x : nltk.pos_tag([item.lower() for item in x if item.lower() not in stop_words]))\n",
    "\n",
    "# apply lemmatization and reset index\n",
    "df['Text'] = df['Text'].apply(lambda row : lemma_token(row))\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# join words\n",
    "df['T_text'] = df['Text'].apply(lambda row : \" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'derailment',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'east',\n",
       " 'february',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'norfolk',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'ohio',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'palestine',\n",
       " 're',\n",
       " 'rt',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'southern',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'train',\n",
       " 'u',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85220"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not rndom sample\n",
    "sample_data = df.T_text.tolist()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## We first used a randome sample to test this\n",
    "# import random\n",
    "# sample_size = 10000\n",
    "# random.seed(1)\n",
    "# # We will use a random sample of 10k tweets to test\n",
    "# idx_sample = random.sample(range(len(df.T_text)), sample_size)\n",
    "# # sample_data is a list of strings, each string is a tweet. ssample_Data is length 10k\n",
    "# sample_data = [df.T_text[i] for i in idx_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CountVectorizer model to get term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# We could cap the size of the vocab to vocab_size\n",
    "# If not capped, we have 48k words\n",
    "max_vocabulary = 10000\n",
    "\n",
    "# We can also set options to only keep words that appear in at least this percentage of documents\n",
    "# - min_df = 2 # words in less than 2 documents we discard them\n",
    "# And we can choose to only keep words that appear in at most this percentage of documents:\n",
    "# - max_df = 0.95 # words that appear in more than 95% of documents, we discard them\n",
    "# Finally, we can set max_features, to keep only the most frequently occuring words\n",
    "# - max_features = max_vocabulary # after I do the max_df, min_df and stopwords filetrs, I look at how many words I have left: if its greater than\n",
    "# max_features, I only keep the most popular. BUT, if the words left after the 3 filters is less than max_features, then it will be ignored. \n",
    "\n",
    "\n",
    "# tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "#                                 min_df=2,\n",
    "#                                 stop_words='english',\n",
    "#                                 max_features=max_vocabulary)\n",
    "\n",
    "# tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "tf_vectorizer = CountVectorizer(min_df = 2)\n",
    "\n",
    "# tf: term frequency vectorizer\n",
    "\n",
    "# the fitting is learning the vocabulary: goes thru all the data and know the words it needs to keep track of\n",
    "# transform: Ive done the fitting, Ill go back to data points and convert each one into a feat vector representation using the vocab i learnt during fitting\n",
    "tf_fit = tf_vectorizer.fit(sample_data)\n",
    "tf = tf_vectorizer.fit_transform(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85220, 23665)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the matrix (documents, words)\n",
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems like the highest term-doc freq is 10\n",
    "max_freq = tf.toarray().max()\n",
    "max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump state go trump trump trump trump trump trump trump trump trump'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for curiosity, I want to know which doc / word combination has highest valeus\n",
    "max_freq_idx = list(np.argwhere(tf.toarray()==max_freq)[0])\n",
    "max_freq_doc, max_freq_word = max_freq_idx\n",
    "# to print out the vocabulary in the doc-word freq matrix\n",
    "tf_vectorizer.get_feature_names_out()[max_freq_word]\n",
    "# In which document is this word?\n",
    "sample_data[max_freq_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wearing', 'weartv', 'weary', 'weasel', 'weaselzippers', 'weather',\n",
       "       'weatherchannel', 'weatherfox64', 'weaver', 'web', 'web3',\n",
       "       'webcast', 'weber', 'webershandwick', 'webguytv', 'webpage',\n",
       "       'website', 'websterscat', 'wed', 'wedge', 'wednesday',\n",
       "       'wednesdaymorning', 'wednesdaymotivation', 'wednesdaythought',\n",
       "       'wednesdaywisdom', 'wee', 'weed', 'week', 'weekday', 'weekend',\n",
       "       'weeklong', 'weekly', 'weeklywrapup', 'weep', 'weer', 'wef',\n",
       "       'weflysoon', 'wehearpodcast', 'weigh', 'weighs', 'weight', 'weir',\n",
       "       'weird', 'weirdly', 'weirdo', 'weirdos', 'weirton', 'weisman',\n",
       "       'welcome', 'welfare', 'well', 'wellbeing', 'wellknownequine',\n",
       "       'wellness', 'wellsfargo', 'wellsh40', 'welovepresidenttrump',\n",
       "       'welp', 'welt', 'wemyss_b', 'wendoverpro', 'wendy',\n",
       "       'wendybugliari', 'wendyldahl', 'wendynas', 'wendyp4545',\n",
       "       'wendywi89642313', 'wepeoplefreedom', 'weptsmile', 'werent',\n",
       "       'werewolf_doctor', 'wes', 'wesa', 'wesh', 'wesleyhunttx', 'west',\n",
       "       'westbound', 'westcentralohio', 'westergrenjon', 'western',\n",
       "       'westinghouse', 'westjournalism', 'westley_boyd', 'westmoreland',\n",
       "       'westvirginia', 'westwatch', 'wet', 'wethepeople', 'wetm18news',\n",
       "       'wewintheylose', 'wews', 'wfaa', 'wfh', 'wfla', 'wfmj', 'wfpl',\n",
       "       'wfplnews', 'wfsbnews', 'wftx', 'wgal', 'wglxlsgv9s', 'wgmcewen',\n",
       "       'wgnmorningnews', 'wgnnews', 'wgrz', 'wh', 'wha', 'whack', 'whale',\n",
       "       'whalenmona', 'whalesorg', 'whammy', 'whartonknows', 'whas11',\n",
       "       'what46hasdone', 'whataboutism', 'whataday', 'whatcha',\n",
       "       'whatdidyedorong', 'whatever', 'whatifisaidit',\n",
       "       'whatilearnedtoday', 'whats', 'whatsapp0032499357495',\n",
       "       'whatsoever', 'whatsupdoc2x', 'whcos', 'wheat', 'wheel',\n",
       "       'wheelbearing', 'wheeler', 'wheels', 'wheeze', 'whelton',\n",
       "       'whenever', 'whenpigsflyyy', 'whereas', 'whereby', 'wherein',\n",
       "       'wheres', 'wheresbiden', 'wherespete', 'wherever', 'whet',\n",
       "       'whether', 'whew', 'whew_lord', 'whick', 'whilst', 'whine',\n",
       "       'whiner', 'whiny', 'whio', 'whioradio', 'whiotv', 'whip',\n",
       "       'whiplash', 'whirlwindwisdom', 'whiskey', 'whisper', 'whistle',\n",
       "       'whistleblower', 'whitaker', 'white', 'white_lenka', 'whitehouse',\n",
       "       'whiteness', 'whitenoise', 'whitenoisemovie', 'whitepatrick',\n",
       "       'whitewash', 'whitewonderly', 'whitfordted', 'whitining',\n",
       "       'whitlockjason', 'whitmer', 'who_shot_jgr', 'whoa', 'whodat35',\n",
       "       'whodisdontcare', 'whodunit', 'whoever', 'whole', 'wholeheartedly',\n",
       "       'wholemarsblog', 'wholly', 'whoop', 'whoopi', 'whoopigoldberg',\n",
       "       'whoown', 'whop', 'whose', 'whowatwherewolf', 'whstancil',\n",
       "       'whsvnews', 'whydidtheydothis', 'whyynews', 'wi', 'wicked', 'wide',\n",
       "       'wideawake885', 'widely', 'wider', 'widespread', 'wieder', 'wield',\n",
       "       'wife', 'wikileaks', 'wikipedia', 'wil', 'wil_johnson1', 'wild',\n",
       "       'wildangel1968', 'wildey', 'wildfire', 'wildlife', 'wildly',\n",
       "       'wildpalmsltd', 'wile', 'wilesgehrig', 'wilkamaxx',\n",
       "       'wilkowmajority', 'will_bunch', 'will_flannigan', 'will_tanner_1',\n",
       "       'willapercy', 'willblakely3', 'willfreedom21', 'willful',\n",
       "       'willfully', 'willhild', 'william', 'william60736669',\n",
       "       'williamdavidw12', 'williamjmccart7', 'williamlegate', 'williams',\n",
       "       'williamsburg', 'williamson', 'williamtaylor83', 'williegeist',\n",
       "       'willing', 'willingly', 'willingness', 'willis', 'willmenaker',\n",
       "       'willofsnow', 'willow', 'willsimons_94', 'willy', 'wilmington',\n",
       "       'wilson', 'wilton', 'wily_kaioty', 'wimp', 'win', 'wind',\n",
       "       'windelljason1', 'windfall', 'windhorse_1', 'windmill', 'window',\n",
       "       'windows', 'windowsill', 'windsorvillhelm', 'windy',\n",
       "       'windy_wyoming_', 'wine', 'winery', 'wing', 'winger', 'wink',\n",
       "       'wink_nod', 'winkle', 'winner', 'winnow', 'winslow_nj', 'winston',\n",
       "       'winter', 'winy', 'wipe', 'wipolitics', 'wire', 'wiright',\n",
       "       'wisconsin', 'wisctv_news3', 'wiscwoman', 'wisdems', 'wisdom',\n",
       "       'wise', 'wisely', 'wiser', 'wisgop', 'wish', 'wish_tv', 'wished',\n",
       "       'wishfulldreamz', 'wishyouwerehere', 'wistfulwitness', 'wit',\n",
       "       'witch', 'witchestruth', 'witfnews', 'withdraw', 'withdrawal',\n",
       "       'withdrawn', 'withdraws', 'withdrew', 'wither', 'withheld',\n",
       "       'withhold', 'within', 'without', 'witless', 'witn', 'witness',\n",
       "       'witnessing', 'witsken', 'witzshared', 'wiunion', 'wizard',\n",
       "       'wizchadwick1', 'wizwackatroll', 'wjactv', 'wjcharliee',\n",
       "       'wjmcgurn', 'wjxt4', 'wjz', 'wk', 'wkbn', 'wkiloski', 'wkok1070',\n",
       "       'wkortepeter', 'wkrc', 'wks', 'wksu', 'wkyc', 'wkyt', 'wlfi',\n",
       "       'wlky', 'wlnimorningline', 'wlos_13', 'wltx', 'wlwt', 'wm',\n",
       "       'wmcactionnews5', 'wmd', 'wmkornblum66', 'wmm_podcast', 'wncn',\n",
       "       'wnct9', 'wndu', 'wnep', 'wnn7', 'wnn7com', 'wnt', 'wny', 'wo',\n",
       "       'wobble', 'wobbly', 'woe', 'woefully', 'woes', 'woio', 'wojahn',\n",
       "       'wojcik', 'woke', 'wokeism', 'wokeness', 'wokes', 'wokeslayerr',\n",
       "       'wokesocieties', 'wolf', 'wolfblitzer', 'wolfe', 'wolsned',\n",
       "       'woman', 'wombatcat1', 'wonder', 'wonderful', 'wonderfully',\n",
       "       'wondering', 'wont', 'woobietoosday', 'woobietuesday', 'wood',\n",
       "       'woodcutterbrian', 'wooded', 'woodhouseb', 'woodland', 'woodrow',\n",
       "       'woodshed', 'woodtv', 'woody', 'woofkoof', 'woopsie', 'woopsy',\n",
       "       'wor', 'worcteaparty', 'word', 'worded', 'wordle', 'wore', 'work',\n",
       "       'workday', 'worked', 'workemail17', 'worker', 'workersstrikeback',\n",
       "       'workersutopia', 'workforce', 'working', 'workingfamilies',\n",
       "       'workingpod', 'workplace', 'workshop', 'world', 'worldbreakingn9',\n",
       "       'worldnews', 'worldnewsintweets', 'worldnewsinvids',\n",
       "       'worldnewsnuggest', 'worldnewstonight', 'worldoutlook3',\n",
       "       'worldsource24', 'worldwide', 'worldwidenews', 'worm', 'worn',\n",
       "       'worried', 'worries', 'worrisome', 'worry', 'worrying', 'worse',\n",
       "       'worsen', 'worship', 'worst', 'worstpresidentever', 'worth',\n",
       "       'worthless', 'worthwhile', 'worthy', 'wosunews', 'wou', 'would',\n",
       "       'woulda', 'wouldnt', 'wound', 'wow', 'wowk13news', 'wowt6news',\n",
       "       'wowterrifying', 'wp', 'wpbf25news', 'wpmredtree', 'wpsdlocal6',\n",
       "       'wptf', 'wpxi', 'wpxi_lori', 'wqlfojiorx', 'wr', 'wraithfodder',\n",
       "       'wrap', 'wray', 'wreak', 'wrec', 'wreck', 'wreckage', 'wrecked',\n",
       "       'wrecking', 'wrench', 'wrestle', 'wrestlerkw7', 'wrgt', 'wri',\n",
       "       'wright', 'wring', 'wrist', 'wristband', 'write', 'writer',\n",
       "       'writes', 'writeup', 'writing', 'wrkrsstrikeback', 'wrong',\n",
       "       'wrongdoing', 'wrongfully', 'wrongly', 'wrschgn', 'wrt', 'wrtv',\n",
       "       'ws', 'wsbt', 'wsbtv', 'wschodniej', 'wshaneschmidt', 'wshawnm',\n",
       "       'wshstand', 'wsj', 'wsjgraphics', 'wsjopinion', 'wsjpolitics',\n",
       "       'wsjwhatsnow', 'wsmartin218', 'wsoctv', 'wspa7', 'wsteaks', 'wsw',\n",
       "       'wsws', 'wsws_updates', 'wsyx', 'wsyx6', 'wt', 'wtae', 'wtaf',\n",
       "       'wtafih', 'wtajnews', 'wtaptelevision', 'wtc', 'wtccmidsouth',\n",
       "       'wtf', 'wtffff', 'wtfork', 'wth', 'wthrcom', 'wti', 'wtoc11',\n",
       "       'wtol11toledo', 'wtopelius', 'wtov', 'wtov9', 'wtp',\n",
       "       'wtparethenews', 'wtpatriotsusa', 'wtpblue', 'wtpearth',\n",
       "       'wtrf7news', 'wttw', 'wtwonews', 'wtyppod', 'wuhan', 'wurst',\n",
       "       'wusa9', 'wut', 'wutangkids', 'wv', 'wva', 'wvamwater', 'wvemd',\n",
       "       'wvnews247', 'wvns59news', 'wvtm', 'wvtm13', 'wvupress', 'ww1',\n",
       "       'ww2', 'ww2_gal', 'ww3', 'wwfrbeffzq', 'wwg1wga', 'wwi', 'wwii',\n",
       "       'wwiii', 'wwj950', 'wwjtraffic', 'wwltv', 'wwmtnews', 'wwomenl',\n",
       "       'www', 'wxbrenn', 'wxbywilliams', 'wxii', 'wxix', 'wxopwyu',\n",
       "       'wxyzdetroit', 'wyffnews4', 'wykoleiÅ‚', 'wylieguide', 'wymondham',\n",
       "       'wynnws', 'wyntonmarsalis', 'wyntre999', 'wyoming', 'wytv', 'x2',\n",
       "       'x22report', 'x3', 'x3r0gxx', 'x4eileen', 'x5', 'xashe10x',\n",
       "       'xaviaerd', 'xhnews', 'xi', 'xin97558681', 'xinhua', 'xnewsalerts',\n",
       "       'xrp_dex', 'xrpking09531420', 'xtrabiggg', 'xx', 'xx17965797n',\n",
       "       'xxx', 'xxxalbertopelle', 'xyuknw', 'y2ymlfnnyc', 'ya',\n",
       "       'yaboytroy27', 'yabutaleb7', 'yaelbraunpivet', 'yah', 'yahoo',\n",
       "       'yahoofinance', 'yahoolife', 'yahoolifeuk', 'yahoonews',\n",
       "       'yahoonewsau', 'yall', 'yamiche', 'yank', 'yap', 'yappydagger',\n",
       "       'yard', 'yardmaster', 'yardtour', 'yarosisnancy', 'yawn', 'yay',\n",
       "       'yaystack', 'ychgnmcayy', 'ydanasmithdutra', 'ydsa', 'ydsagt',\n",
       "       'ye', 'yea', 'yeah', 'yeap', 'year', 'yearly', 'yeh', 'yell',\n",
       "       'yellen', 'yellow', 'yellowpaiges', 'yellowserf',\n",
       "       'yellowstone1776', 'yep', 'yer', 'yes', 'yesitsm97576245',\n",
       "       'yesseniaamarys', 'yessfun', 'yest', 'yesterday', 'yesteryear',\n",
       "       'yet', 'yetiglass25', 'yfblanchet', 'yield', 'yikes', 'yinzp365',\n",
       "       'yitgordon', 'ylsjrlezst', 'yo', 'yogurt', 'yohiobaseball',\n",
       "       'yoittsconnie', 'yok', 'yolalindayola', 'yolo304741', 'yoo',\n",
       "       'yoooo', 'york', 'yorkers', 'yosoychÃ¡vez', 'yost', 'young',\n",
       "       'youngkin', 'youngstown', 'youranonnews', 'yourboypat47',\n",
       "       'yourcallradio', 'yourchoicenews', 'youre', 'youredailynews',\n",
       "       'yourlibertynews', 'yourmomswatchin', 'yournews15', 'yournewsnet',\n",
       "       'youtbe', 'youth', 'youtu', 'youtube', 'youtubers', 'youtubeã‚ˆã‚Š',\n",
       "       'yp6vcceqtb', 'ypgkm0fzji', 'ypsigal', 'ypu', 'yr', 'yrs',\n",
       "       'yruwhining', 'yserashenai', 'yt', 'yt9pcrulxo', 'yuge', 'yuhline',\n",
       "       'yuji', 'yukon', 'yup', 'yves', 'yvespdb', 'z_tarot',\n",
       "       'zac_unchained', 'zacakamadu', 'zacchino', 'zachary38090591',\n",
       "       'zachary_cohen', 'zachgorchow', 'zachsteinbrook', 'zack',\n",
       "       'zackbornstein', 'zacksjerryrig', 'zagozana', 'zainsville',\n",
       "       'zaizul_abidin', 'zaleskiluke', 'zany', 'zardoz420wpn',\n",
       "       'zazzybritches', 'ze', 'zealand', 'zealous', 'zeedlethorp',\n",
       "       'zeekarkham', 'zeelandcap', 'zegdie', 'zehirli', 'zein',\n",
       "       'zekegary2', 'zelenskaua', 'zelensky', 'zelenskyy', 'zelenskyyua',\n",
       "       'zelinsky', 'zemmoureric', 'zenkus', 'zenkuswatch', 'zenoc_oshits',\n",
       "       'zephyr21848384', 'zero', 'zerohedge', 'zerosum24',\n",
       "       'zerowarningshot', 'zerowaste', 'zeteamd', 'zg4ever',\n",
       "       'zhang_yueting', 'zhangxinyue', 'zhirji28', 'zhrvtenghl',\n",
       "       'zibalady1', 'zilch', 'zioevil', 'ziondaughter', 'zionist', 'zip',\n",
       "       'zipillinois', 'zito', 'zlqhj05tmc', 'zntf3ro9bn', 'zombaplateau',\n",
       "       'zombie', 'zombywoof2022', 'zone', 'zooeyd1', 'zoom', 'zoomarang',\n",
       "       'zoomer', 'zoomerwaffen', 'zoomin', 'zooming', 'zoophilewins',\n",
       "       'zsnakeplissken', 'zsuzsa', 'zt', 'zt_followers', 'zug',\n",
       "       'zugly747', 'zugs', 'zvr09', 'zwv134', 'zzt', 'Ã§a', 'Ã©niÃ¨me',\n",
       "       'Ã©quipes', 'Ã©tait', 'Ã©tat', 'Ã©tats', 'Ã©tatsunis', 'Ã©tÃ©',\n",
       "       'Ã©ventuellement', 'Ã©viter', 'Ã¶nÃ¼nde', 'Ã¼ber', 'ÎºÎ±Î¹', 'ÏƒÏ„Î¿',\n",
       "       'Ğ°Ğ»Ğ°Ğ±Ğ°Ğ¼Ğµ', 'Ğ°Ğ¼ĞµÑ€Ğ¸ĞºĞ°Ğ½ÑĞºĞ¾Ğ¹', 'Ğ³ĞµĞ½ĞµÑ€Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹', 'Ğ³Ñ€ÑƒĞ·Ğ¾Ğ²Ğ¾Ğ¹', 'Ğ´Ğ°Ğ²Ğ½Ğ¾',\n",
       "       'Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€', 'Ğ´Ğ¾', 'ĞµÑ‰Ñ‘', 'Ğ¶Ğ´', 'Ğ¶ĞµĞ»ĞµĞ·Ğ½Ğ¾Ğ´Ğ¾Ñ€Ğ¾Ğ¶Ğ½Ñ‹Ğ¹', 'Ğ·Ğ°', 'Ğ¸ald',\n",
       "       'Ğ¸ÑĞ¿Ñ‹Ñ‚Ğ°Ğ½Ğ¸Ñ', 'ĞºĞ°Ğº', 'ĞºĞ°ĞºĞ¾Ğ¹', 'ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸Ğ¸', 'Ğ»ĞµĞºÑĞ¸Ğ½Ğ³Ñ‚Ğ¾Ğ½Ğµ', 'Ğ¼Ğ°Ñ€Ñ‚Ğ°',\n",
       "       'Ğ½ĞµÑĞºĞ¾Ğ»ÑŒĞºĞ¾', 'Ğ¾Ğ³Ğ°Ğ¹Ğ¾', 'Ğ¾Ğ´Ğ¸Ğ½', 'Ğ¾Ğ½Ğ¸', 'Ğ¿ĞµÑ€ĞµĞ´', 'Ğ¿Ğ¾', 'Ğ¿Ğ¾ĞµĞ·Ğ´',\n",
       "       'Ğ¿Ñ€Ğ¾Ğ²ĞµĞ»', 'Ğ¿ÑƒÑ‚Ğ¸', 'Ñ€ĞµĞ»ÑŒÑĞ¾Ğ²', 'Ñ€ĞµĞ¼Ğ¾Ğ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸', 'ÑĞ¾ÑˆĞµĞ»', 'ÑĞ¾ÑˆÑ‘Ğ»',\n",
       "       'ÑÑƒĞ±Ğ±Ğ¾Ñ‚Ñƒ', 'ÑÑ‡Ñ‘Ñ‚Ñƒ', 'Ñ‚Ğ°Ğ¼', 'Ñ‚Ğ¾Ğ³Ğ¾', 'Ñ‚Ñ€Ğ°Ğ½ÑĞ¿Ğ¾Ñ€Ñ‚Ğ½Ğ¾Ğ¹', 'Ñ‡Ğ°ÑĞ¾Ğ²', 'ÑˆÑ‚Ğ°Ñ‚',\n",
       "       'Ñepublican', '××œ×‘××”', '×”×¤×¢×', 'Ø£ÙˆÙ‡Ø§ÙŠÙˆ', 'Ø§Ø²', 'Ø§Ø³Ù¾Ø±ÛŒÙ†Ú¯ÙÛŒÙ„Ø¯',\n",
       "       'Ø§Ù„Ù‚Ø¶Ø¨Ø§Ù†', 'Ø§ÙˆÙ‡Ø§ÛŒÙˆ', 'Ø§ÛŒÙ†', 'Ø¨Ø§Ø±', 'Ø¬Ù†ÙˆØ¨ÛŒ', 'Ø®Ø§Ø±Ø¬', 'Ø®Ø±ÙˆØ¬', 'Ø¯Ø±',\n",
       "       'Ø¯ÛŒÚ¯Ø±', 'Ø±ÛŒÙ„', 'Ø´Ø¯', 'Ø¹Ù†', 'ÙÙŠ', 'Ù‚Ø·Ø§Ø±', 'Ù‚Ø·Ø§Ø±Ù‡Ø§ÛŒ', 'Ù†ÙˆØ±ÙÙˆÙ„Ú©',\n",
       "       'ÙˆÙ„Ø§ÙŠØ©', 'ÛŒÚ©ÛŒ', 'à¸™à¸­à¸à¸ˆà¸²à¸à¸™', 'à¹à¸•', 'â„‚â„ğ”¼â„â„•ğ•†ğ”¹ğ•ğ•ƒ', 'ã†ã²ãƒ¼ãƒ¡ãƒ¢', 'ãŒè¬ç½ªã—',\n",
       "       'ãŒè­°ä¼šã§è¨¼è¨€ã™ã‚‹ä¸­', 'ãŒé‡è¦ãªã‚»ã‚¯ã‚¿ãƒ¼ã«ã¨ã£ã¦ã¯ä»–äººäº‹ã§ã¯ç„¡ã„ã‚“ã ã‚ˆã­', 'ã“ã®éƒ¨åˆ†',\n",
       "       'ãã®ç±³å›½ã®ç‰©æµã§ã™ãŒ___________', 'ã¾ãŸã—ã¦ã‚‚', 'ã¾ãŸã¾ãŸ',\n",
       "       'ã‚¢ãƒ¡ãƒªã‚«ä¸‰å¤§ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ†ãƒ¬ãƒ“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ•°æ™‚é–“é…ã‚Œã§è¦–è´ã§ãã‚‹æ™‚ä»£ã¨ãªã£ã¦ã„ã‚‹', 'ã‚¦ã‚§ã‚¹ãƒˆãƒãƒ¼ã‚¸ãƒ‹ã‚¢å·', 'ã‚ªãƒã‚¤ã‚ª',\n",
       "       'ã‚ªãƒã‚¤ã‚ªã®è„±ç·šäº‹æ•…', 'ã‚ªãƒã‚¤ã‚ªå·ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ã‚ªãƒã‚¤ã‚ªå·ã®åŒ–å­¦ç‰©è³ªæµå‡ºå¾Œ',\n",
       "       'ã‚ªãƒã‚¤ã‚ªå·ã‚¯ãƒªãƒ¼ãƒ–ãƒ©ãƒ³ãƒ‰ã§ç«æ›œæ—¥ã®æ—©æœ', 'ã‚ªãƒã‚¤ã‚ªå·ã‚¹ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ã®', 'ã‚ªãƒãƒæ”¿æ¨©ã§å¼·åŒ–ã•ã‚ŒãŸé‰„é“ã®å®‰å…¨å¯¾ç­–ãŒ',\n",
       "       'ã‚µã‚¶ãƒ³', 'ã‚µã‚¶ãƒ³ceo', 'ã‚µã‚¶ãƒ³ã®', 'ã‚µã‚¶ãƒ³ã®ã‚ªãƒã‚¤ã‚ªå·ã®åˆ—è»Šã®è„±ç·šäº‹æ•…ã§é‰„é“ã‚»ãƒ³ã‚µãƒ¼ãŒè„šå…‰ã‚’æµ´ã³ã‚‹',\n",
       "       'ã‚µã‚¶ãƒ³ã®åˆ—è»ŠãŒè„±ç·šã—ãŸ', 'ã‚µã‚¶ãƒ³ã®å¾“æ¥­å“¡ãŒæ­»äº¡ã—ãŸ', 'ã‚µã‚¶ãƒ³ã®è²¨ç‰©åˆ—è»ŠãŒè„±ç·šã—', 'ã‚µã‚¶ãƒ³ãƒˆãƒ¬ã‚¤ãƒ³ãŒè„±ç·š',\n",
       "       'ã‚¹ãƒ—ãƒªãƒ³ã‚°ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã§ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ãƒ€ãƒ³ãƒ—ã‚«ãƒ¼ãŒè¡çªã—', 'ãƒˆãƒ©ãƒ³ãƒ—æ”¿æ¨©ä¸‹ã§ãƒ­ãƒ“ãƒ¼ã‚¤ãƒ³ã‚°ã§å¼±ä½“åŒ–ã•ã‚ŒãŸä»¶ã¯æ³¨ç›®ã§',\n",
       "       'ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'ãƒã‚¨ã®ã‚ˆã†ã«è½ã¡ã‚‹', 'ãƒãƒ¼ã‚¸ãƒ‹ã‚¢å·',\n",
       "       'ãƒšãƒ³ã‚·ãƒ«ãƒãƒ‹ã‚¢å·çŸ¥äº‹ãŒã‚ªãƒã‚¤ã‚ªå·å—éƒ¨ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯åˆ—è»Šã®å¢œè½äº‹æ•…ã‚’éé›£ã—', 'ä¸œå·´å‹’æ–¯å¦', 'äº‹ä»¶æ²’äººå—å‚·',\n",
       "       'ä»Šå¹´ã ã‘ã§è„±ç·šäº‹æ•…ã¯10ä»¶ä»¥ä¸Š', 'ä»Šåº¦ã¯ã‚¢ãƒ©ãƒãƒå·ã§ã‚‚è„±ç·šäº‹æ•…ã£ã¦', 'ä½†æœ‰ç•¶åœ°æ°‘çœ¾è¡¨ç¤º', 'ä½æ°‘ã«é¿é›£ã™ã‚‹ã‚ˆã†æŒ‡ç¤º',\n",
       "       'ä½æ°‘ã¯å±‹å†…é€€é¿ã‚’å‘½ã˜ã‚‰ã‚Œã¾ã—ãŸ', 'ä¿„äº¥ä¿„å·', 'ä¿„äº¥ä¿„å·è¯ºç¦å…‹å—éƒ¨è´§è¿åˆ—è½¦è„±è½¨å',\n",
       "       'å…ˆæœˆã«ã¯ã‚¤ãƒ¼ã‚¹ãƒˆãƒ‘ãƒ¬ã‚¹ãƒãƒ³ã§ã‚ˆã‚Šå¤§è¦æ¨¡ãªè„±ç·šäº‹æ•…ãŒç™ºç”Ÿã—ãŸ', 'å…ˆæœˆé ­ã¨ä»Šæœˆé ­ã«ã‚ªãƒã‚¤ã‚ªå·ã§è„±ç·šäº‹æ•…èµ·ã“ã—ãŸã®ã«ç¶šã',\n",
       "       'å‡ºè»Œ', 'å‡ºè½¨æ˜¯å¸¸æ€', 'åˆ‘äº‹å‘Šç™ºã‚’è¡Œã†', 'åˆ—è»Šã¨', 'åˆ—è»Šã®è„±ç·šã«é–¢ã™ã‚‹ä¸Šé™¢å…¬è´ä¼šã§å¤‰æ›´ã‚’ç´„æŸ', 'åˆ—è»Šè„±ç·šäº‹æ•…',\n",
       "       'åˆ—è»Šè„±ç·šäº‹æ•…ã«é–¢ã™ã‚‹ä¸Šé™¢å…¬è´ä¼šã§å¤‰æ›´ã‚’ç´„æŸ', 'åˆ¥ã®ãƒãƒ¼ãƒ•ã‚©ãƒ¼ã‚¯', 'åˆ¥ã®åˆ—è»ŠãŒè„±ç·š', 'åarmy', 'ås',\n",
       "       'å±é™ºç‰©å‡¦ç†ç­ãŒç¾å ´ã«æ€¥è¡Œ', 'åˆä¸€èµ·', 'å¦ä¸€åˆ—è¯ºç¦å…‹å—æ–¹è´§è¿åˆ—è½¦å‡ºè½¨', 'å¦ä¸€åˆ—è¯ºç¦å…‹å—éƒ¨è´§è¿åˆ—è½¦å‡ºè½¨äº†',\n",
       "       'åŒã˜é‰„é“ä¼šç¤¾ã®è²¨ç‰©åˆ—è»ŠãŒã¾ãŸè„±ç·šäº‹æ•…', 'å‘¼ç±²å¸‚æ°‘å°±åœ°é¿é›£', 'å †ç–Šåœ¨è·¯è»Œæ—', 'å¢®è»Œ', 'å¤§è¦æ¨¡ãª', 'å®‰å…¨',\n",
       "       'å¯¼è‡´åŒ—å¡ç½—æ¥çº³å·å»¶è¯¯', 'å°±åœ¨å‡ å¤©å‰', 'å±…æ°‘è¢«å‘ŠçŸ¥å°±åœ°é¿éš¾', 'å¸ƒè’‚å‰æ ¼è¯´ç¾å›½ä¸€å¹´æœ‰ä¸€åƒ', 'å¹¼å…æ•™è‚²ãªã©', 'æ„å¤–',\n",
       "       'æŠ•ç¨¿æ™‚é–“', 'æ“šå ±é€™åˆ—è¼‰è²¨ç«è»Šå…±æœ‰212å¡è»Šå»‚', 'æ–¯æ™®æ—è²å°”å¾·', 'æ—¥ãƒŸã‚·ã‚·ãƒƒãƒ”å·ã«ãƒ•ãƒƒåŒ–æ°´ç´ é…¸ãŒæµå‡º',\n",
       "       'æ±šæŸ“ã®è¦æ¨¡ãŒå¤§ããã¦è©±é¡Œã«ãªã£ã¦ã„ã‚‹ã‘ã©', 'æµ·å¤–ç§‘å­¦', 'ç«æ›œæ—¥',\n",
       "       'ç’°å¢ƒä¿è­·åºã¯ã‚ªãƒã‚¤ã‚ªå·ã®æœ‰æ¯’åˆ—è»Šäº‹æ•…ã«ã‚ˆã‚‹æ±šæŸ“å»ƒæ£„ç‰©ã®å‡ºè·ã®ä¸€æ™‚åœæ­¢ã‚’å‘½ä»¤', 'ç•¶ä¸­20ç¯€å‡ºè»Œ',\n",
       "       'ç•¶å±€ç¨±è»Šä¸Šä¸¦ç„¡è¼‰æœ‰æœ‰æ¯’åŒ–å­¸ç‰©', 'çš®ç‰¹', 'çœ‹åˆ°æœ‰æ¯’åŒ–å­¸ç‰©å“è™•ç†çµ„çš„è»Šè¼›å‡ºç¾', 'ç±³ã§ã¯å¹´å¹³å‡1700ä»¶ã®è„±ç·šäº‹æ•…ãŒç™ºç”Ÿ',\n",
       "       'ç¾Šæ°‘ä»¬åˆ«çæ‹…å¿ƒäº†', 'ç¾å›½', 'ç¾å›½ä¿„äº¥ä¿„å·å†å‘ç”Ÿåˆ—è½¦è„±è½¨äº‹æ•…', 'è‚‡äº‹çš„ç«è»ŠåŒæ¨£å±¬æ–¼norfolk', 'è„±è»Œ',\n",
       "       'è„±è½¨', 'è‡ªåˆ†ãŸã¡ãŒå¼•ãèµ·ã“ã—ãŸæ··ä¹±ã®å¾Œå§‹æœ«ã®ä»£å„Ÿã‚’æ‰•ã†ã“ã¨ã«ãªã‚‹', 'è¯ºç¦å…‹å—æ–¹å£°ç§°å±…æ°‘æ²¡ä»€ä¹ˆå¥½æ‹…å¿ƒçš„',\n",
       "       'è¿™æ¬¡æ˜¯åœ¨åŒ—å¡ç½—æ¥çº³å·', 'é€Ÿå ±', 'é€±æœ«ã«ã¯', 'éƒ½æ•£äº†å§', 'éµè·¯', 'é“è·¯', 'é˜¿æ‹‰å·´é©¬å·',\n",
       "       'é™„è¿‘å±…æ°‘è¢«è¦æ±‚å°±åœ°é¿éš¾', 'ì—´ì°¨', 'íƒˆì„ ', 'ğƒğ¢ğ¬ğšğ¬ğ­ğğ«', 'ğ„ğšğ¬ğ­', 'ğ„ğ±ğ©ğ¨ğ¬ğ¢ğ§ğ ',\n",
       "       'ğ‡ğ„ğ€ğƒğ‹ğˆğğ„ğ’', 'ğ‡ğğšğ«ğ­', 'ğ‰ğšğœğ¤', 'ğŠğƒğŠğ€', 'ğğ„ğ–ğ’ğ‘ğ€ğƒğˆğ', 'ğğ¨ğ­',\n",
       "       'ğğšğ¥ğğ¬ğ­ğ¢ğ§ğ', 'ğ“ğ«ğšğ¢ğ§', 'ğ•ğ“', 'ğ›ğ®ğ²ğ¢ğ§ğ ', 'ğ¢ğ­', 'ğ°ğ¢ğ­ğ¡', 'ğ”¸ğ•„ğ”¼â„ğ•€â„‚ğ”¸ğ•Š',\n",
       "       'ğ—–ğ—®ğ—»', 'ğ——ğ—œğ—¦ğ—£ğ—¢ğ—¦ğ—”ğ—Ÿ', 'ğ——ğ—¢', 'ğ—¦ğ—˜ğ—”ğ—¥ğ—–ğ—›', 'ğ—§ğ—²ğ˜…ğ—®ğ˜€', 'ğ—ªğ—˜ğ—Ÿğ—Ÿğ—¦', 'ğ˜€ğ—®ğ˜†', 'ğ˜„ğ—µğ˜†',\n",
       "       'ğ˜†ğ—¼ğ˜‚'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get the whole vocabulary, sorted alphabetically\n",
    "tf_vectorizer.get_feature_names_out()[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go in reverse: given a word, we can figure out which column index it corresponds to. To do this, we use the `vocabulary_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3596"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.vocabulary_['car']\n",
    "# .vocabulary_ get the word index in the vocabulary\n",
    "# IN OTHER WORDS: what column do I have to look at to find the word car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what the raw counts are for the 0-th post as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85220, 23665)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0].toarray()\n",
    "\n",
    "# I can index in to the zeroeth row of the matrix and convert into an array, I am anazlizing just the zeroeth row:\n",
    "# feature vector representation for the zeroeth document: \n",
    "# We should be able to write code to know what word corresponds each non zero number there\n",
    "\n",
    "# tf_vectorizer.get_feature_names_out()[(tf[0].toarray() != 0)[0]] # LIKE THIS!\n",
    "\n",
    "# run a for loop that goes thru the entries, and check to which indeces this corresponds in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting procedure determines the every topic's distribution over words; this information is stored in the `components_` attribute. There's a catch: we actually have to normalize to get the probability distributions (without this normalization, instead what the model has are pseudocounts for how often different words appear per topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit an LDA model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now LDA. This will take a while ong\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 23665)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape\n",
    "# The shaope of this is 16 by 1000, 10 topics and 1000 words\n",
    "# These are not probability distributions yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([212806.09756, 183229.36112, 226403.02355, 249360.46048,\n",
       "       196956.05727])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we access: sum ACROSS COLUMNS (get rid of columns), I get 6 numbers: a sum for each row.\n",
    "# They dont sum uop to 1, because not probability distributions\n",
    "\n",
    "# .components_ is a raw count histogram, but why are there fractions?\n",
    "# Because of how it works: when it estimates how much a word belongs to a topic it does a prob assignment, not a deterministic assignment.\n",
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])\n",
    "# We can get the proba distributions: take each row and divide by sum, then we get what was in the purple box in the lecture slide:\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 23665)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that each topic's word distribution sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.sum(axis=1)\n",
    "# then we have distributions: every row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out what the probabilities for the different words are for a specific topic. This isn't very easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.      0.      0.      ... 0.00001 0.00001 0.00001]\n"
     ]
    }
   ],
   "source": [
    "print(topic_word_distributions[4])\n",
    "# lets look at 0th topic, it is saying that the 0th word appears 0.00011, word 1 appears with prob 0.00191\n",
    "# Lets view the data differently to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, usually people do something like looking at the most probable words per topic, and try to use these words to interpret what the different topics correspond to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "toxic : 0.022457791846392967\n",
      "ceo : 0.01909519770061295\n",
      "resident : 0.017347292501322095\n",
      "chemical : 0.016160113157076755\n",
      "epa : 0.013899063024322483\n",
      "via : 0.009893960751737617\n",
      "pay : 0.008269276069613581\n",
      "say : 0.008163893554262587\n",
      "senate : 0.008134594278846838\n",
      "tell : 0.007605117286419225\n",
      "congress : 0.006781515182579619\n",
      "order : 0.006306676179334696\n",
      "clean : 0.006226100852638858\n",
      "rail : 0.0062106600464542985\n",
      "crash : 0.006076923915123679\n",
      "site : 0.005999986680991488\n",
      "worker : 0.0059216764863732185\n",
      "animal : 0.005689835953484064\n",
      "company : 0.00549751936463235\n",
      "demand : 0.005352323604188377\n",
      "\n",
      "[Topic 1]\n",
      "derail : 0.041301288720680895\n",
      "another : 0.03180513917666337\n",
      "car : 0.026418350367137678\n",
      "derails : 0.022203092403190045\n",
      "hazardous : 0.019883504315243262\n",
      "material : 0.015784198361090688\n",
      "say : 0.013526961781535762\n",
      "springfield : 0.012509990870489616\n",
      "20 : 0.011057471751499764\n",
      "company : 0.010800731026674894\n",
      "alabama : 0.010715894642125327\n",
      "carry : 0.009308797231190406\n",
      "freight : 0.008695898991201418\n",
      "cargo : 0.008482252175005624\n",
      "place : 0.008091923299740364\n",
      "second : 0.007548877594248619\n",
      "county : 0.007522726704506382\n",
      "report : 0.007300660550027382\n",
      "month : 0.007231423910512517\n",
      "one : 0.007019434564381862\n",
      "\n",
      "[Topic 2]\n",
      "toxic : 0.024399433631634115\n",
      "chemical : 0.018556797501523944\n",
      "visit : 0.013117524876437109\n",
      "water : 0.012062389845854405\n",
      "say : 0.010973712439973757\n",
      "site : 0.00844431795530629\n",
      "biden : 0.007837146592019666\n",
      "town : 0.007748579055315083\n",
      "resident : 0.007402385870002908\n",
      "state : 0.007034684176878805\n",
      "federal : 0.007014036491315758\n",
      "release : 0.006943703134839403\n",
      "week : 0.006781308268996687\n",
      "epa : 0.006548100561755021\n",
      "air : 0.006254763588951667\n",
      "disaster : 0.0057565792934920115\n",
      "follow : 0.00573390107272309\n",
      "environmental : 0.005488453374713875\n",
      "response : 0.005398110140580032\n",
      "trump : 0.005321220579266696\n",
      "\n",
      "[Topic 3]\n",
      "trump : 0.018502068531496673\n",
      "biden : 0.013314674872412476\n",
      "people : 0.012179552557099477\n",
      "go : 0.009201033734639479\n",
      "get : 0.00865621546372934\n",
      "need : 0.007154970123360606\n",
      "say : 0.007040251673666007\n",
      "water : 0.006922296359066375\n",
      "disaster : 0.006480895117766147\n",
      "oh : 0.0057712524173923006\n",
      "buttigieg : 0.00563832997784419\n",
      "pete : 0.005540222921467095\n",
      "gop : 0.005460751698488064\n",
      "help : 0.005425819781321395\n",
      "regulation : 0.005069411637819183\n",
      "president : 0.005018373647870214\n",
      "blame : 0.004838856230112978\n",
      "republican : 0.004819385292446188\n",
      "like : 0.004649180333924542\n",
      "would : 0.004602272585002344\n",
      "\n",
      "[Topic 4]\n",
      "safety : 0.020854286101955024\n",
      "trump : 0.012096334684715907\n",
      "rail : 0.009905084947262426\n",
      "disaster : 0.009302870886439376\n",
      "regulation : 0.008105892834487346\n",
      "brake : 0.007738428284822817\n",
      "rule : 0.006814332639663208\n",
      "make : 0.0066215506193103195\n",
      "would : 0.0065813934167291074\n",
      "like : 0.006260418724992104\n",
      "railroad : 0.006026254068120426\n",
      "year : 0.005842938838686263\n",
      "derailment : 0.005839382265900528\n",
      "accident : 0.00567860726893937\n",
      "new : 0.005491509162680534\n",
      "cause : 0.004876663188700848\n",
      "ntsb : 0.004615220286816077\n",
      "happen : 0.004435757515635707\n",
      "buttigieg : 0.004175798070820346\n",
      "right : 0.004164262724009133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can sort the probability by big to small\n",
    "# for each topic, listing top 20 most popular words (by probability)\n",
    "\n",
    "num_top_words = 20\n",
    "\n",
    "def print_top_words(topic_word_distributions, num_top_words, vectorizer):\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    print('Displaying the top %d words per topic and their probabilities within the topic...' % num_top_words)\n",
    "    print()\n",
    "\n",
    "    for topic_idx in range(num_topics):\n",
    "        print('[Topic ', topic_idx, ']', sep='')\n",
    "        # This is the interesting part: take the topic_word_distributions only for the topic_idxth topic, and get the indexes sorted from highest to lowest\n",
    "        sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "        # then for the range 0,num_words, print the vocab of each of the words and its entry in the topic_word_distribution matrix\n",
    "        for rank in range(num_top_words):\n",
    "            word_idx = sort_indices[rank]\n",
    "            print(vocab[word_idx], ':',\n",
    "                  topic_word_distributions[topic_idx, word_idx])\n",
    "        print()\n",
    "\n",
    "print_top_words(topic_word_distributions, num_top_words, tf_vectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "Present these results and then tweak:\n",
    "- Present this\n",
    "- Divid by govt people and public and present results\n",
    "- set a minimum frequency for the terms included and a max for vocabulary\n",
    "- Present overall topics\n",
    "- Divide the set by govt, media an public and redo analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `transform()` function to figure out for each document, what fraction of it is explained by each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider removing:\n",
    "terms_remove = ['train', 'https', 'rt', 'palestine', 'east', 'ohio', 'norfolk', 'southern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = lda.transform(tf)\n",
    "# Ill get a low dimensional version of the data, the document topic matrix (10, 10)\n",
    "# each of the 10,000 represented as a distribution over 10 different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic distribution of the 0th document is very small except for 0.96288 fior index 7\n",
    "# We saw from above, topic 7 is about religion, and we can check that document 0 is about religion\n",
    "doc_topic_matrix[0]\n",
    "\n",
    "\n",
    "# Note: for GMM when you do the .predict, itll give you k probabilites that sum to 1. \n",
    "# predict_proba of GMM \n",
    "# here: ... gives you a proba distr:\n",
    "# INTERPRETARATION DIFFERENT\n",
    "\n",
    "# FOR LDA: document can consist of a bunch of words, and different fraction of the words are truly in different topics: we get mixed membership, words are allowed to e in different topics\n",
    "# GMM: the above doesnt hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this *could* be interpreted as a form of dimensionality reduction: document 0 is converted from its raw counts histogram representation to a 10-dimensional vector of probabilities, indicating estimated memberships to the 10 different topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **--------------------- Stop here by now ---------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "\n",
    "Here's a fancier way to visualize. This requires installation of the wordcloud package:\n",
    "\n",
    "```\n",
    "pip install wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "num_max_word_cloud_words = 100\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "num_topics = len(topic_word_distributions)\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    wc = WordCloud(max_words=num_max_word_cloud_words)\n",
    "    wc.generate_from_frequencies(dict(zip(vocab, topic_word_distributions[topic_idx])))\n",
    "    plt.figure()\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title('Topic %d' % topic_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing co-occurrences of words\n",
    "\n",
    "Here, we count the number of newsgroup posts in which two words both occur. This part of the demo should feel like a review of co-occurrence analysis from earlier in the course, except now we use scikit-learn's built-in CountVectorizer. Conceptually everything else in the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'year'\n",
    "word2 = 'team'\n",
    "\n",
    "word1_column_idx = tf_vectorizer.vocabulary_[word1]\n",
    "word2_column_idx = tf_vectorizer.vocabulary_[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[:, word1_column_idx].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_both_word1_and_word2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the log of the conditional probability of word 1 appearing given that word 2 appeared, where we add in a little bit of a fudge factor in the numerator (in this case, it's actually not needed but some times you do have two words that do not co-occur for which you run into a numerical issue due to taking the log of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of prob of see one word given you see another, using count vectorizer!\n",
    "# Same content as in hw , but with a different tool\n",
    "# This uses np.arrays instead of counters. Needs to kep track of the indexes.\n",
    "# Understad how countvectorizer works\n",
    "\n",
    "def prob_see_word1_given_see_word2(word1, word2, vectorizer, eps=0.1):\n",
    "    word1_column_idx = vectorizer.vocabulary_[word1]\n",
    "    word2_column_idx = vectorizer.vocabulary_[word2]\n",
    "    documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2\n",
    "    return np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word1), type(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence\n",
    "\n",
    "The below code shows how one implements the topic coherence calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cell below, vectorizer.get_feature_names() is just tokenizing the text, \n",
    "# eliminating stopwords, eliminating some other words, and then giving you back\n",
    "# a list of the tokens in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic coherence\n",
    "\n",
    "def compute_average_coherence(topic_word_distributions, num_top_words, vectorizer, verbose=True):\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    average_coherence = 0\n",
    "    # foor loop thru different topics, for each topic double nested for loop, going thru each entry, ordering matters\n",
    "    # make sure words arent the same, then compute the log(prob)\n",
    "    # add a bunhc of them and divide by total number of topics\n",
    "    for topic_idx in range(num_topics):\n",
    "        if verbose:\n",
    "            print('[Topic ', topic_idx, ']', sep='')\n",
    "        \n",
    "        sort_indices = np.argsort(topic_word_distributions[topic_idx])[::-1]\n",
    "        coherence = 0.\n",
    "        for top_word_idx1 in sort_indices[:num_top_words]:\n",
    "            word1 = vocab[top_word_idx1]\n",
    "            for top_word_idx2 in sort_indices[:num_top_words]:\n",
    "                word2 = vocab[top_word_idx2]\n",
    "                if top_word_idx1 != top_word_idx2:\n",
    "                    coherence += prob_see_word1_given_see_word2(word1, word2, vectorizer, 0.1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Coherence:', coherence)\n",
    "            print()\n",
    "        average_coherence += coherence\n",
    "    average_coherence /= num_topics\n",
    "    if verbose:\n",
    "        print('Average coherence:', average_coherence)\n",
    "    return average_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_coherence(topic_word_distributions, num_top_words, tf_vectorizer, True)\n",
    "# These are negative, the highest possible is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique words\n",
    "\n",
    "The below code shows how one implements the number of unique words calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force implementation\n",
    "# nothing clever to speed upmcalculation\n",
    "\n",
    "#fro loop for each topic\n",
    "# For loop over each top word\n",
    "# another for loop for other topics\n",
    "# lok for all other top words in those other topics\n",
    "# check for uniqueness\n",
    "\n",
    "def compute_average_num_unique_words(topic_word_distributions, num_top_words, vectorizer, verbose=True):\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    average_number_of_unique_top_words = 0\n",
    "    for topic_idx1 in range(num_topics):\n",
    "        if verbose:\n",
    "            print('[Topic ', topic_idx1, ']', sep='')\n",
    "        \n",
    "        sort_indices1 = np.argsort(topic_word_distributions[topic_idx1])[::-1]\n",
    "        num_unique_top_words = 0\n",
    "        for top_word_idx1 in sort_indices1[:num_top_words]:\n",
    "            word1 = vocab[top_word_idx1]\n",
    "            break_ = False\n",
    "            for topic_idx2 in range(num_topics):\n",
    "                if topic_idx1 != topic_idx2:\n",
    "                    sort_indices2 = np.argsort(topic_word_distributions[topic_idx2])[::-1]\n",
    "                    for top_word_idx2 in sort_indices2[:num_top_words]:\n",
    "                        word2 = vocab[top_word_idx2]\n",
    "                        if word1 == word2:\n",
    "                            break_ = True\n",
    "                            break\n",
    "                    if break_:\n",
    "                        break\n",
    "            else:\n",
    "                num_unique_top_words += 1\n",
    "        if verbose:\n",
    "            print('Number of unique top words:', num_unique_top_words)\n",
    "            print()\n",
    "\n",
    "        average_number_of_unique_top_words += num_unique_top_words\n",
    "    average_number_of_unique_top_words /= num_topics\n",
    "    \n",
    "    if verbose:\n",
    "        print('Average number of unique top words:', average_number_of_unique_top_words)\n",
    "    \n",
    "    return average_number_of_unique_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_num_unique_words(topic_word_distributions, num_top_words, tf_vectorizer, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting average coherence vs k (number of topics), and average number of unique words vs k\n",
    "\n",
    "Next, we plot the average coherence vs k and the average number of unique words vs k. Note that these are *not* the only topic model metrics available (much like how CH index is not the only metric available for clustering).\n",
    "\n",
    "For both average coherence and average number of unique words, we would like these to be high. In this particular example, it turns out k=2 yields very high values for both but if you look at the topics learned for k=2, they are qualitatively quite bad (basically one topic is gibberish and the other is everything else!). This observation reinforces the important idea that while there exist topic modeling metrics (such as coherence and number of unique words), you should definitely still look at what the learned topics are (e.g., by printing the top words per topic) to help decide on what value of k to use.\n",
    "\n",
    "Also, keep in mind that the results are in some sense \"noisy\" since the LDA fitting procedure is random. We're choosing a specific `random_state` seed value but if we try different random seeds, we can get different results. For simplicity, because LDA fitting is quite computationally expensive, we are *not* doing what we did with GMM's where we did many different random initializations. Thus, the conclusions we draw regarding how many topics to use might actually be different with different random initializations.\n",
    "\n",
    "At least according to average coherence and average number of unique words for the random seed we use, the results below suggests that using k=4 yields average coherence and average number of unique words that are still reasonably high (as good as or almost as good as the k=2 result), and inspecting the topics learned for k=4, they are definitely more interesting than the ones learned for k=2.\n",
    "\n",
    "From qualitatively looking at topics, the k=5, k=6, and k=7 topics also look decent. When k gets too large (e.g., k=10), there start to be topics that look like there might be too much overlap (such as multiple topics that seem to be about computers).\n",
    "\n",
    "Note that one of the things to look out for is whether there are \"stable\" topics, where even for slightly different values of k and different random initializations, LDA keeps finding specific topics (e.g., one on gibberish, one on numbers, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 11)\n",
    "avg_coherences = []\n",
    "avg_num_unique_words = []\n",
    "\n",
    "for k in k_values:\n",
    "    lda_candidate = LatentDirichletAllocation(n_components=k, random_state=0)\n",
    "    lda_candidate.fit(tf)\n",
    "    topic_word_distributions = np.array([row / row.sum() for row in lda_candidate.components_])\n",
    "    print('-' * 80)\n",
    "    print('Number of topics:', k)\n",
    "    print()\n",
    "    print_top_words(topic_word_distributions, num_top_words, tf_vectorizer)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    avg_coherences.append(compute_average_coherence(topic_word_distributions, num_top_words, tf_vectorizer, False))\n",
    "    avg_num_unique_words.append(compute_average_num_unique_words(topic_word_distributions, num_top_words, tf_vectorizer, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, avg_coherences)\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Average coherence')\n",
    "\n",
    "# For differnt number of topics, I fitted a different LDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, avg_num_unique_words)\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Average number of unique words')\n",
    "\n",
    "# For different topic models, the average number of unique words\n",
    "# We want coherence to be high and unique words to be high\n",
    "\n",
    "# Number of topic = 2, gets me high coherence and hogh unique words. BUT, for model 2, there are 2 topics: garbage and not garbage!\n",
    "# that doesnt help a lot\n",
    "\n",
    "# So, you want to tolerate a lower value for one of the metrics, but look at the actual topics to interpret what is going on\n",
    "# If we see resutlts of 4, starts to make more ssense but feels like still low\n",
    "# go to 5, starts to make a bit more sense\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('UDA_RIG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1a526087eeeed7e3a2dd7dbe3d8582928a2e8e80d21671a2817819bcd9f426f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "Visualizations  \n",
    "Preprocessing  \n",
    "Complete dataset  \n",
    "Split by government, media, public  \n",
    "**REmoveAdditional stopwords: add_stopwords = ['rt','train','southern','east','derailment','norfolk','palestine','february','u','ohio','month','week','news','please']**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ribarragi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=5, suppress=True)\n",
    "import pandas as pd\n",
    "# data: 20 news groups\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing (From Elaine's)\n",
    "df = pd.read_csv('../data/data_tweet.csv')\n",
    "\n",
    "# RIG (made some changes to stopwords)\n",
    "add_stopwords = set(['rt','train','southern','east','derailment','norfolk','palestine','february','u','ohio'])\n",
    "# add_stopwords_2 = set(['złożonych', 'ßtrong', 'áramellátása', 'áramellátást', 'ça', 'çavuş', 'çevre', 'çıkarak',\n",
    "#        'çıkardı', 'çıkma', 'çıkmış', 'écologique', 'économiques', 'édito', 'élections', 'élimination', 'élites', 'éloignés', 'énième',\n",
    "#        'équipes', 'était', 'état', 'états', 'étatsunis', 'étranges', 'étrangères', 'été', 'éventuellement', 'éverything', 'éviter',\n",
    "#        'évoque', 'être', 'önce', 'önünde', 'última', 'üb', 'über', 'ĥmmm', 'środka', 'środowiska', 'środowisko', 'ťçh', 'że', 'ˈapəthē', 'άλλη', 'έχει', 'αλαμπάμα', 'αλλη', 'απαντηθούν', 'απεργια8μαρτη',\n",
    "#        'απόδειξη', 'αρχίσουν', 'βίντεο', 'βελοπουλος', 'βουλή', 'γεννιούνται', 'δικαιολογίες', 'δουλειά', 'δυστύχημα', 'είχε', 'εκτροχιαστηκαν', 'ενα', 'επιθέσεις', 'ερωτηματα', 'ζητήσουμε',\n",
    "#        'θα', 'και', 'καιρο', 'κανακη', 'κι', 'μέσω', 'μεσα', 'μηνα', 'μια', 'να', 'νδ_τελος', 'οι', 'οχαιο', 'πολλά', 'ποσα', 'ποσες', 'σήμερα', 'σε', 'στη', 'στην', 'στο', 'στράτος', 'συμπτωση', 'σχέση', \n",
    "#        'τα', 'τελευταιο', 'τεμπη', 'τεμπη_δολοφονια', 'τι', 'τις', 'το', 'τον', 'τρενα', 'τρενο', 'τρολ', 'τώρα', 'χωρες', 'аварией', 'алабаме', 'амери', 'американской', 'американском', 'анализ', \n",
    "#        'безопасность', 'величезний', 'генеральный', 'грузовой', 'давно', 'директор', 'для', 'до', 'ещё', 'жд', 'железнодорожный', 'за', 'иald', 'испытания', 'как', 'какой', 'каролина', \n",
    "#        'компании', 'конгрессом', 'лексингтоне', 'марта', 'на', 'населения', 'несколько', 'нет', 'норрис', 'огайо', 'один', 'они', 'опасностями', 'перед', 'по', 'поезд', 'показывает', 'провел',\n",
    "#        'пути', 'расходы', 'рельсов', 'ремонтировали', 'северная', 'ситуации', 'сократила', 'сообщений', 'сошел', 'сошёл', 'стабильность', 'субботу', 'счёту', 'там', 'того', 'транспортной',\n",
    "#        'чак', 'часов', 'что', 'штат', 'штате', 'яepublican', 'אלבמה', 'הפעם', 'أوهايو', 'از', 'اسپرینگفیلد', 'الأميركية', 'البضائع',\n",
    "#        'القضبان', 'انبعاث', 'اوهایو', 'این', 'بأوهايو', 'بار', 'بحادث', 'بعد', 'تنحرف', 'ثان', 'جنوبی', 'حديدية', 'خارج', 'خروج', 'خلال', 'در', 'دیگر', 'ریل', 'سامة', 'سكة', 'شد', 'شهر', 'عربة', 'عن',\n",
    "#        'في', 'قطار', 'قطارهای', 'لثاني', 'مرة', 'مسارها', 'مماثل', 'من', 'مواد', 'نورفولك', 'نورفولک', 'واحد', 'ولاية', 'یکی', 'กงานใหญ', 'กส', 'กา', 'การตกรางคร', 'การท', 'การสอบสวนเบ', 'กแล', 'ของบร',\n",
    "#        'ควรจะเก', 'งของ', 'งคน', 'งท', 'งน', 'งม', 'งเล', 'งแต', 'ชาวบ', 'ฐอเมร', 'ดข', 'ดำเน', 'ตกรางเป', 'ตกลงมาเป', 'ตว', 'ทม', 'ทำให', 'ทแท', 'นการทำลายสารเคม', 'นการในภาคตะว', 'นค', 'นคร', 'นง',\n",
    "#        'นบรรยากาศ', 'นพบว', 'นรถไฟบรรท', 'นอกจากน', 'นออกของสหร', 'นเด',\n",
    "#        'นเพราะ', 'นไปอย', 'บต', 'บนช', 'บร', 'ผลเส', 'ฝนกรด', 'ฝนท',\n",
    "#        'พบว', 'พอเค', 'ยท', 'ลอยข', 'วน', 'สารเคม', 'สำน', 'หากฝนตก',\n",
    "#        'อก', 'องต', 'อน', 'อนผ', 'อยหมาของเค', 'อาจจะทำให', 'อเป', 'อได',\n",
    "#        'าตายแล', 'าน', 'านคนน', 'านต', 'านไป', 'าประเภท', 'าปล', 'ายและถ',\n",
    "#        'าว', 'าหมาของเค', 'าออกมาด', 'าออกมาเด', 'เก', 'เค', 'เด', 'เป',\n",
    "#        'เพ', 'เพราะม', 'เล', 'เหล', 'แต', 'และไม', 'ในแอตแลนตา',\n",
    "#        'ℂℍ𝔼ℝℕ𝕆𝔹𝕐𝕃', 'うひーメモ', 'か月分の食料', 'が謝罪し', 'が議会で証言する中',\n",
    "#        'が議会に東パレスチナについて', 'が重要なセクターにとっては他人事では無いんだよね', 'この1カ月で2件目',\n",
    "#        'この会社は民間業者として', 'この部分', 'その米国の物流ですが___________', 'ちょっと脱線し過ぎだろ',\n",
    "#        'と告げるわずか数時間前に発生しました', 'と診断された', 'どうしたアメリカ', 'に対する刑事告訴がありそうだと報じている',\n",
    "#        'はっきりさせるために', 'またしても', 'またまた', 'またアメリカオハイオで化学品を載せた列車の脱線事故',\n",
    "#        'またオハイオで貨物列車が脱線', 'アメリカ三大ネットワークのテレビニュースを数時間遅れで視聴できる時代となっている',\n",
    "#        'アラバマ州カルフーン郡でノーフォークサザンのが脱線した', 'アラバマ州速報', 'イースト',\n",
    "#        'イーストパレスチナの列車脱線事故の金銭的責任を追及することを求める', 'ウェストバージニア州',\n",
    "#        'ウェストバージニア州で岩石滑りに衝突した後', 'オハイオ', 'オハイオの脱線事故', 'オハイオチェルノブイリ',\n",
    "#        'オハイオ州がノーフォーク', 'オハイオ州で2回目の脱線事故', 'オハイオ州での同社の列車の脱線事故は',\n",
    "#        'オハイオ州での有毒化学物質流出の元トランプ当局者を非難', 'オハイオ州でノーフォーク', 'オハイオ州で別の列車が脱線',\n",
    "#        'オハイオ州に救急隊員養成のための新施設を建設', 'オハイオ州のベッドフォード爆発に向かったが', 'オハイオ州の化学物質流出後',\n",
    "#        'オハイオ州の司法長官がノーフォーク', 'オハイオ州は', 'オハイオ州イースト', 'オハイオ州クラーク郡でノーフォーク',\n",
    "#        'オハイオ州クリーブランドで火曜日の早朝', 'オハイオ州スプリングフィールドで', 'オハイオ州スプリングフィールドでの',\n",
    "#        'オハイオ州スプリングフィールドでノーフォーク', 'オハイオ州パレスチナ周辺の有毒地域について',\n",
    "#        'オハイオ州西中央でノーフォーク', 'オハイオ州西中央部でノーフォーク', 'オバマ政権で強化された鉄道の安全対策が',\n",
    "#        'オーバー', 'クラーク郡の住民1500人が停電の為', 'クリントン空港の外で飛行機事故で亡くなった乗組員は',\n",
    "#        'クリーンアップ担当企業とは無縁だが', 'コロナ禍と露宇戦争で産業従事者激減してますしねぇ', 'サザン', 'サザンceo',\n",
    "#        'サザンに対し', 'サザンの', 'サザンのがまたもや脱線', 'サザンのオハイオ州の列車の脱線事故で鉄道センサーが脚光を浴びる',\n",
    "#        'サザンのスポークスパーソンは', 'サザンの列車20両が脱線', 'サザンの列車が脱線した', 'サザンの従業員が死亡した',\n",
    "#        'サザンの貨物列車が脱線', 'サザンの貨物列車が脱線し', 'サザンの貨物列車が脱線したことを受けて',\n",
    "#        'サザンの貨物列車が脱線した後', 'サザンを提訴', 'サザントレインが脱線', 'サザン社の脱線事故により',\n",
    "#        'サザン社の車両が脱線した後', 'サザン社を提訴しました', 'サザン鉄道が脱線', 'サザン鉄道が脱線し',\n",
    "#        'サザン鉄道の貨物列車が再び脱線', 'スプリングフィールドでノーフォーク', 'ダンプカーが衝突し', 'テロか',\n",
    "#        'ディーゼルが川に流出', 'トランプ政権下でロビーイングで弱体化された件は注目で', 'トレイルの脱線は',\n",
    "#        'ノースカロライナ州のノーフォーク', 'ノーフォーク', 'ハエのように落ちる', 'バージニア州',\n",
    "#        'パレスチナの鉄道脱線事故現場からの汚染された廃棄物のすべての出荷を中止し', 'パレスチナ列車の脱線を訴える',\n",
    "#        'ブラックロック', 'ブラックロックは何を考えているのだろうか', 'ブルームバーグによると',\n",
    "#        'ペンシルバニア州知事がオハイオ州南部のノーフォーク列車の墜落事故を非難し', 'ホワイトハウスは共和党員',\n",
    "#        'リーディング用素材', 'レールが', 'ヴァンガード', '一周后会有人帮我开奖', '不为别人想想', '世界緊急放送',\n",
    "#        '东巴勒斯坦', '东巴勒斯坦的积水无需担心https', '中國人', '乗務員は流出した痕跡を見つけられませんでした',\n",
    "#        '也没有化学物质泄漏', '事件沒人受傷', '事故导致1500名居民停电', '交通部長', '人が負傷', '人家天氣那麼好',\n",
    "#        '今回はアラバマ州で別のノーフォーク', '今天在網上分享一個', '今年だけで脱線事故は10件以上',\n",
    "#        '今度はアラバマ州でも脱線事故って', '今後米国政府も鉄道会社に対応迫ることになりそう', '今日該州', '他一直與',\n",
    "#        '似乎每一天都是如此', '但我们被告知没有危险物质', '但有當地民眾表示', '住民に避難するよう指示',\n",
    "#        '住民は屋内退避を命じられました', '來自', '俄亥俄', '俄亥俄州', '俄亥俄州列車脫軌事故參議院聽證會直播',\n",
    "#        '俄亥俄州化學品洩漏後有另一列諾福克南方鐵路公司在北卡羅萊納州剋星敦脫軌', '俄亥俄州又有一辆norfolk', '俄亥俄州州長',\n",
    "#        '俄亥俄州火车事故背后的诺福克南方公司和其他铁路公司花费数百万美元进行营销和游说', '俄亥俄州诺福克南部货运列车脱轨后',\n",
    "#        '俄合俄州再有火車出軌', '假的', '先月にはイーストパレスチンでより大規模な脱線事故が発生した',\n",
    "#        '先月イーストパレスチナの町で有毒化学物質が流出した脱線事故について', '先月オハイオ州で発生したノーフォーク',\n",
    "#        '先月頭と今月頭にオハイオ州で脱線事故起こしたのに続き', '先週土曜日にノーフォークサザンの列車がオハイオ州で脱線し',\n",
    "#        '克拉克縣', '全ての新しい住宅ローンやローンなどから利益を得るでしょう', '公衆への危険は報告されていない',\n",
    "#        '公衆への危険は報告されていないと述べた', '其後再發生金屬廠爆炸事件', '再發生火車出軌事件', '出於高度謹慎的原則',\n",
    "#        '出軌', '出軌地點', '出軌發生在41號州際公路附近', '出轨是常态', '刑事告発を行う', '列車と',\n",
    "#        '列車の脱線に関する上院公聴会で変更を約束', '列車脱線事故', '列車脱線事故に関する上院公聴会で変更を約束',\n",
    "#        '別のノーフォーク', '別のノーフォークサザン列車が脱線', '別の列車が脱線', '前17分钟是恩怨', '化学性気管支炎',\n",
    "#        '卐army', '卐s', '南诺福克有什么问题吗', '危険物は積んでいないと当局が発表', '危険物処理班が現場に急行',\n",
    "#        '又一起', '另一列诺福克南方货运列车出轨', '另一列诺福克南部货运列车出轨了', '可憐的',\n",
    "#        '同じ鉄道会社の貨物列車がまた脱線事故', '同時に複数地区で停電も起きているらしい', '呼籲市民就地避難',\n",
    "#        '因为我们听说俄亥俄州又有一列火车出轨了', '因为福禄寿利用我赚了千万还恶心我', '土曜日', '在內的聯邦官員保持聯繫',\n",
    "#        '在週六諾福克南方springfield火車出軌後', '堆疊在路軌旁', '報告說', '報復攻撃', '墮軌',\n",
    "#        '大手製薬会社との繋がりは', '大規模な', '大量逮捕', '太缺德了', '妙なモノは漏れたりしてないそうだけど', '安全',\n",
    "#        '导致北卡罗来纳州延误', '少なくとも', '就在几天前', '居民被告知就地避难', '屋内退避命令',\n",
    "#        '州内では有害物質が流出した先月の事故に続き２例目', '布蒂吉格说美国一年有一千', '幼児教育など', '当地老百姓怕啥呢',\n",
    "#        '当局によると', '当該企業が欠陥テストを実施していることは承知', '很可能会被报复', '必然的に混ざり合っていました',\n",
    "#        '必需品を手元に用意しておくことをお勧めします', '怎麼會有人病倒', '怠慢か', '情報弱者さんへ', '意外',\n",
    "#        '我忍无可忍选择曝光福禄寿利用警察冻卡', '戦争か', '投稿時間', '拉文纳', '拜登', '拡散希望',\n",
    "#        '據報這列載貨火車共有212卡車廂', '收钱解冻', '政府督促居民就地寻求庇护', '政治家もわかっているか怪しい', '斯托本维尔市', '斯普林菲尔德', '日ミシシッピ川にフッ化水素酸が流出', '日本のメディアは理解していない',\n",
    "#        '显然正在进行', '時代で立ち往生していることを示しています', '暂时没有人员伤亡', '朝起きて目に飛び込んできたニュース', '本來是逗大家一樂的', '来自', '東パ',\n",
    "#        '東パレスチナに鉄道業界のロビイストにそれらを売り渡したことに対する謝罪を負っています', '東パレスチナの住民と労働者が',\n",
    "#        '東パレスチナの流出後', '桑德斯基', '汚染の規模が大きくて話題になっているけど', '沒想到第一條留言便是簡短中文字的',\n",
    "#        '波蒂奇县', '洩漏化學物污染當地', '海外tech', '海外科学', '深くお詫び申し上げます', '火曜日',\n",
    "#        '狗子在林中狂奔的視頻', '現場に調査員を派遣', '現金', '環境保護庁はオハイオ州の有毒列車事故による汚染廃棄物の出荷の一時停止を命令', '環境保護庁はノーフォーク', '用心深さから',\n",
    "#        '當中20節出軌', '當局稱車上並無載有有毒化學物', '疯狂三月', '皮特', '皮特布蒂吉格', '看到有毒化學物品處理組的車輛出現', '等であり', '米では年平均1700件の脱線事故が発生', '米国運輸安全委員会は',\n",
    "#        '米版', '緊急事務管理局', '繼上月發生火車出軌事件', '羊民们别瞎担心了', '美国', '美国俄亥俄州再发生列车脱轨事故', '美国俄亥俄州载有毒化学品列车脱轨',\n",
    "#        '美国环保局阻止诺福克南方公司清除有毒的俄亥俄州火车脱轨残骸', '肇事的火車同樣屬於norfolk', '脱線させるひとをディレーラーと呼ぶ', '脱線はderail', '脱線ばっかりしてるな', '脱軌', '脱轨',\n",
    "#        '自分たちが引き起こした混乱の後始末の代償を払うことになる', '西行小宝', '調査員を派遣している', '諾福克南方公司', '證實', '議会の共和党員と元トランプ政権の当局者は', '诺福克南方声称居民没什么好担心的', '貨物列車が脱線',\n",
    "#        '軽油が川に流出', '转发评论此条推特抽一万u', '还有u商安徽省芜湖防空洞帮人藏匿的数十亿资产', '这可能是我的最后一条视频', '这次是在北卡罗来纳州', '这次是在斯普林菲尔德', '速報', '週末には',\n",
    "#        '適切な処分を確実にするよう命じた', '避難するよう求められていました', '避難指示も', '邁克德溫', '都散了吧', '鄉下', '鉄道の', '鉄道事業者のノーフォーク',\n",
    "#        '鉄道会社が事故防止を目的として使用しているセンサーの役割に改めて注目が集まっています', '鐵路', '铁路', '阿尼斯顿', '阿拉巴馬現場脫軌為聽證會助興', '阿拉巴马州', '附近居民被要求就地避难',\n",
    "#        '陰謀集團的貝萊德公司就是俄亥俄州火車車禍的主要股東之一', '高速鉄道路線の筆頭株主は中央銀行家', '黑岩貝萊德擁有之一股東',\n",
    "#        '거주하는', '공무원들이', '근처에', '내렸다', '노퍽', '대피소로', '로이터', '만에', '명령을', '바이든은', '발생해', '방문하지', '번도', '번째', '번째로', '사고', '사고가', '서던', '않은',\n",
    "#        '연루된', '열차', '열차가', '오하이오', '오하이오에서', '오하이오주에서', '주민들에게', '지역', '찾은', '철도가', '출처', '탈선', '탈선해', '토요일', '트럼프', '현장', '현지'])\n",
    "\n",
    "\n",
    "remove_stopwords = set([\"no\", \"not\", \"nor\", \"against\", \"aren't\", \"couldn't\", \"didn't\", \n",
    "                                                   \"doesn't\", \"don't\", \"haven't\", \"hadn't\", \"hasn't\", \"isn't\", \"mightn't\",\n",
    "                                                   \"mustn't\", \"needn't\", \"shouldn't\", \"wasn't\", \"weren't\", \"wouldn't\"])\n",
    "# remove stopwords (not impacting sentiment analysis) and punctuations\n",
    "stop_words = set(stopwords.words('english')) - remove_stopwords\n",
    "stop_words = set(stopwords.words('english')).union(add_stopwords)\n",
    "# stop_words = set(stopwords.words('english')).union(add_stopwords_2)\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "# normalize pos tags\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tagger(nltk_tag):\n",
    "  if nltk_tag.startswith('J'):\n",
    "      return wordnet.ADJ\n",
    "  elif nltk_tag.startswith('V'):\n",
    "      return wordnet.VERB\n",
    "  elif nltk_tag.startswith('N'):\n",
    "      return wordnet.NOUN\n",
    "  elif nltk_tag.startswith('R'):\n",
    "      return wordnet.ADV\n",
    "  else:         \n",
    "      return None\n",
    "\n",
    "\n",
    "# lemmatize each token with pos tag\n",
    "def lemma_token(row):\n",
    "  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), row))\n",
    "  lemmatized = []\n",
    "  for word, tag in wordnet_tagged:\n",
    "    if tag is None:\n",
    "        # if there is no available tag, just append the original token\n",
    "        lemmatized.append(word)\n",
    "    else:       \n",
    "        # else use the pos tag to lemmatize the token\n",
    "        lemmatized.append(lemmatizer.lemmatize(word, tag))\n",
    "  return lemmatized\n",
    "\n",
    "\n",
    "# RIG (edit: added http://)\n",
    "# remove urls\n",
    "df['Message_no_url'] = df['MESSAGE'].apply(lambda x: \" \".join([word for word in x.split(\" \") if not(word.startswith((\"https://\", 'http://')))]))\n",
    "\n",
    "\n",
    "# tokenize MESSAGE and remove stopwords\n",
    "df['Text'] = df['Message_no_url'].map(tokenizer.tokenize)\n",
    "df['Text'] = df['Text'].apply(lambda x : nltk.pos_tag([item.lower() for item in x if item.lower() not in stop_words]))\n",
    "\n",
    "# apply lemmatization and reset index\n",
    "df['Text'] = df['Text'].apply(lambda row : lemma_token(row))\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# join words\n",
    "df['T_text'] = df['Text'].apply(lambda row : \" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'derailment',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'east',\n",
       " 'february',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'norfolk',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'ohio',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'palestine',\n",
       " 're',\n",
       " 'rt',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'southern',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'train',\n",
       " 'u',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85220"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not rndom sample\n",
    "sample_data = df.T_text.tolist()\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## We first used a randome sample to test this\n",
    "# import random\n",
    "# sample_size = 10000\n",
    "# random.seed(1)\n",
    "# # We will use a random sample of 10k tweets to test\n",
    "# idx_sample = random.sample(range(len(df.T_text)), sample_size)\n",
    "# # sample_data is a list of strings, each string is a tweet. ssample_Data is length 10k\n",
    "# sample_data = [df.T_text[i] for i in idx_sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use CountVectorizer model to get term frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# We could cap the size of the vocab to vocab_size\n",
    "# If not capped, we have 48k words\n",
    "max_vocabulary = 10000\n",
    "\n",
    "# We can also set options to only keep words that appear in at least this percentage of documents\n",
    "# - min_df = 2 # words in less than 2 documents we discard them\n",
    "# And we can choose to only keep words that appear in at most this percentage of documents:\n",
    "# - max_df = 0.95 # words that appear in more than 95% of documents, we discard them\n",
    "# Finally, we can set max_features, to keep only the most frequently occuring words\n",
    "# - max_features = max_vocabulary # after I do the max_df, min_df and stopwords filetrs, I look at how many words I have left: if its greater than\n",
    "# max_features, I only keep the most popular. BUT, if the words left after the 3 filters is less than max_features, then it will be ignored. \n",
    "\n",
    "\n",
    "# tf_vectorizer = CountVectorizer(max_df=0.95,\n",
    "#                                 min_df=2,\n",
    "#                                 stop_words='english',\n",
    "#                                 max_features=max_vocabulary)\n",
    "\n",
    "# tf_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "tf_vectorizer = CountVectorizer(min_df = 2)\n",
    "\n",
    "# tf: term frequency vectorizer\n",
    "\n",
    "# the fitting is learning the vocabulary: goes thru all the data and know the words it needs to keep track of\n",
    "# transform: Ive done the fitting, Ill go back to data points and convert each one into a feat vector representation using the vocab i learnt during fitting\n",
    "tf_fit = tf_vectorizer.fit(sample_data)\n",
    "tf = tf_vectorizer.fit_transform(sample_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85220, 23665)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the matrix (documents, words)\n",
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seems like the highest term-doc freq is 10\n",
    "max_freq = tf.toarray().max()\n",
    "max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'trump state go trump trump trump trump trump trump trump trump trump'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just for curiosity, I want to know which doc / word combination has highest valeus\n",
    "max_freq_idx = list(np.argwhere(tf.toarray()==max_freq)[0])\n",
    "max_freq_doc, max_freq_word = max_freq_idx\n",
    "# to print out the vocabulary in the doc-word freq matrix\n",
    "tf_vectorizer.get_feature_names_out()[max_freq_word]\n",
    "# In which document is this word?\n",
    "sample_data[max_freq_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['wearing', 'weartv', 'weary', 'weasel', 'weaselzippers', 'weather',\n",
       "       'weatherchannel', 'weatherfox64', 'weaver', 'web', 'web3',\n",
       "       'webcast', 'weber', 'webershandwick', 'webguytv', 'webpage',\n",
       "       'website', 'websterscat', 'wed', 'wedge', 'wednesday',\n",
       "       'wednesdaymorning', 'wednesdaymotivation', 'wednesdaythought',\n",
       "       'wednesdaywisdom', 'wee', 'weed', 'week', 'weekday', 'weekend',\n",
       "       'weeklong', 'weekly', 'weeklywrapup', 'weep', 'weer', 'wef',\n",
       "       'weflysoon', 'wehearpodcast', 'weigh', 'weighs', 'weight', 'weir',\n",
       "       'weird', 'weirdly', 'weirdo', 'weirdos', 'weirton', 'weisman',\n",
       "       'welcome', 'welfare', 'well', 'wellbeing', 'wellknownequine',\n",
       "       'wellness', 'wellsfargo', 'wellsh40', 'welovepresidenttrump',\n",
       "       'welp', 'welt', 'wemyss_b', 'wendoverpro', 'wendy',\n",
       "       'wendybugliari', 'wendyldahl', 'wendynas', 'wendyp4545',\n",
       "       'wendywi89642313', 'wepeoplefreedom', 'weptsmile', 'werent',\n",
       "       'werewolf_doctor', 'wes', 'wesa', 'wesh', 'wesleyhunttx', 'west',\n",
       "       'westbound', 'westcentralohio', 'westergrenjon', 'western',\n",
       "       'westinghouse', 'westjournalism', 'westley_boyd', 'westmoreland',\n",
       "       'westvirginia', 'westwatch', 'wet', 'wethepeople', 'wetm18news',\n",
       "       'wewintheylose', 'wews', 'wfaa', 'wfh', 'wfla', 'wfmj', 'wfpl',\n",
       "       'wfplnews', 'wfsbnews', 'wftx', 'wgal', 'wglxlsgv9s', 'wgmcewen',\n",
       "       'wgnmorningnews', 'wgnnews', 'wgrz', 'wh', 'wha', 'whack', 'whale',\n",
       "       'whalenmona', 'whalesorg', 'whammy', 'whartonknows', 'whas11',\n",
       "       'what46hasdone', 'whataboutism', 'whataday', 'whatcha',\n",
       "       'whatdidyedorong', 'whatever', 'whatifisaidit',\n",
       "       'whatilearnedtoday', 'whats', 'whatsapp0032499357495',\n",
       "       'whatsoever', 'whatsupdoc2x', 'whcos', 'wheat', 'wheel',\n",
       "       'wheelbearing', 'wheeler', 'wheels', 'wheeze', 'whelton',\n",
       "       'whenever', 'whenpigsflyyy', 'whereas', 'whereby', 'wherein',\n",
       "       'wheres', 'wheresbiden', 'wherespete', 'wherever', 'whet',\n",
       "       'whether', 'whew', 'whew_lord', 'whick', 'whilst', 'whine',\n",
       "       'whiner', 'whiny', 'whio', 'whioradio', 'whiotv', 'whip',\n",
       "       'whiplash', 'whirlwindwisdom', 'whiskey', 'whisper', 'whistle',\n",
       "       'whistleblower', 'whitaker', 'white', 'white_lenka', 'whitehouse',\n",
       "       'whiteness', 'whitenoise', 'whitenoisemovie', 'whitepatrick',\n",
       "       'whitewash', 'whitewonderly', 'whitfordted', 'whitining',\n",
       "       'whitlockjason', 'whitmer', 'who_shot_jgr', 'whoa', 'whodat35',\n",
       "       'whodisdontcare', 'whodunit', 'whoever', 'whole', 'wholeheartedly',\n",
       "       'wholemarsblog', 'wholly', 'whoop', 'whoopi', 'whoopigoldberg',\n",
       "       'whoown', 'whop', 'whose', 'whowatwherewolf', 'whstancil',\n",
       "       'whsvnews', 'whydidtheydothis', 'whyynews', 'wi', 'wicked', 'wide',\n",
       "       'wideawake885', 'widely', 'wider', 'widespread', 'wieder', 'wield',\n",
       "       'wife', 'wikileaks', 'wikipedia', 'wil', 'wil_johnson1', 'wild',\n",
       "       'wildangel1968', 'wildey', 'wildfire', 'wildlife', 'wildly',\n",
       "       'wildpalmsltd', 'wile', 'wilesgehrig', 'wilkamaxx',\n",
       "       'wilkowmajority', 'will_bunch', 'will_flannigan', 'will_tanner_1',\n",
       "       'willapercy', 'willblakely3', 'willfreedom21', 'willful',\n",
       "       'willfully', 'willhild', 'william', 'william60736669',\n",
       "       'williamdavidw12', 'williamjmccart7', 'williamlegate', 'williams',\n",
       "       'williamsburg', 'williamson', 'williamtaylor83', 'williegeist',\n",
       "       'willing', 'willingly', 'willingness', 'willis', 'willmenaker',\n",
       "       'willofsnow', 'willow', 'willsimons_94', 'willy', 'wilmington',\n",
       "       'wilson', 'wilton', 'wily_kaioty', 'wimp', 'win', 'wind',\n",
       "       'windelljason1', 'windfall', 'windhorse_1', 'windmill', 'window',\n",
       "       'windows', 'windowsill', 'windsorvillhelm', 'windy',\n",
       "       'windy_wyoming_', 'wine', 'winery', 'wing', 'winger', 'wink',\n",
       "       'wink_nod', 'winkle', 'winner', 'winnow', 'winslow_nj', 'winston',\n",
       "       'winter', 'winy', 'wipe', 'wipolitics', 'wire', 'wiright',\n",
       "       'wisconsin', 'wisctv_news3', 'wiscwoman', 'wisdems', 'wisdom',\n",
       "       'wise', 'wisely', 'wiser', 'wisgop', 'wish', 'wish_tv', 'wished',\n",
       "       'wishfulldreamz', 'wishyouwerehere', 'wistfulwitness', 'wit',\n",
       "       'witch', 'witchestruth', 'witfnews', 'withdraw', 'withdrawal',\n",
       "       'withdrawn', 'withdraws', 'withdrew', 'wither', 'withheld',\n",
       "       'withhold', 'within', 'without', 'witless', 'witn', 'witness',\n",
       "       'witnessing', 'witsken', 'witzshared', 'wiunion', 'wizard',\n",
       "       'wizchadwick1', 'wizwackatroll', 'wjactv', 'wjcharliee',\n",
       "       'wjmcgurn', 'wjxt4', 'wjz', 'wk', 'wkbn', 'wkiloski', 'wkok1070',\n",
       "       'wkortepeter', 'wkrc', 'wks', 'wksu', 'wkyc', 'wkyt', 'wlfi',\n",
       "       'wlky', 'wlnimorningline', 'wlos_13', 'wltx', 'wlwt', 'wm',\n",
       "       'wmcactionnews5', 'wmd', 'wmkornblum66', 'wmm_podcast', 'wncn',\n",
       "       'wnct9', 'wndu', 'wnep', 'wnn7', 'wnn7com', 'wnt', 'wny', 'wo',\n",
       "       'wobble', 'wobbly', 'woe', 'woefully', 'woes', 'woio', 'wojahn',\n",
       "       'wojcik', 'woke', 'wokeism', 'wokeness', 'wokes', 'wokeslayerr',\n",
       "       'wokesocieties', 'wolf', 'wolfblitzer', 'wolfe', 'wolsned',\n",
       "       'woman', 'wombatcat1', 'wonder', 'wonderful', 'wonderfully',\n",
       "       'wondering', 'wont', 'woobietoosday', 'woobietuesday', 'wood',\n",
       "       'woodcutterbrian', 'wooded', 'woodhouseb', 'woodland', 'woodrow',\n",
       "       'woodshed', 'woodtv', 'woody', 'woofkoof', 'woopsie', 'woopsy',\n",
       "       'wor', 'worcteaparty', 'word', 'worded', 'wordle', 'wore', 'work',\n",
       "       'workday', 'worked', 'workemail17', 'worker', 'workersstrikeback',\n",
       "       'workersutopia', 'workforce', 'working', 'workingfamilies',\n",
       "       'workingpod', 'workplace', 'workshop', 'world', 'worldbreakingn9',\n",
       "       'worldnews', 'worldnewsintweets', 'worldnewsinvids',\n",
       "       'worldnewsnuggest', 'worldnewstonight', 'worldoutlook3',\n",
       "       'worldsource24', 'worldwide', 'worldwidenews', 'worm', 'worn',\n",
       "       'worried', 'worries', 'worrisome', 'worry', 'worrying', 'worse',\n",
       "       'worsen', 'worship', 'worst', 'worstpresidentever', 'worth',\n",
       "       'worthless', 'worthwhile', 'worthy', 'wosunews', 'wou', 'would',\n",
       "       'woulda', 'wouldnt', 'wound', 'wow', 'wowk13news', 'wowt6news',\n",
       "       'wowterrifying', 'wp', 'wpbf25news', 'wpmredtree', 'wpsdlocal6',\n",
       "       'wptf', 'wpxi', 'wpxi_lori', 'wqlfojiorx', 'wr', 'wraithfodder',\n",
       "       'wrap', 'wray', 'wreak', 'wrec', 'wreck', 'wreckage', 'wrecked',\n",
       "       'wrecking', 'wrench', 'wrestle', 'wrestlerkw7', 'wrgt', 'wri',\n",
       "       'wright', 'wring', 'wrist', 'wristband', 'write', 'writer',\n",
       "       'writes', 'writeup', 'writing', 'wrkrsstrikeback', 'wrong',\n",
       "       'wrongdoing', 'wrongfully', 'wrongly', 'wrschgn', 'wrt', 'wrtv',\n",
       "       'ws', 'wsbt', 'wsbtv', 'wschodniej', 'wshaneschmidt', 'wshawnm',\n",
       "       'wshstand', 'wsj', 'wsjgraphics', 'wsjopinion', 'wsjpolitics',\n",
       "       'wsjwhatsnow', 'wsmartin218', 'wsoctv', 'wspa7', 'wsteaks', 'wsw',\n",
       "       'wsws', 'wsws_updates', 'wsyx', 'wsyx6', 'wt', 'wtae', 'wtaf',\n",
       "       'wtafih', 'wtajnews', 'wtaptelevision', 'wtc', 'wtccmidsouth',\n",
       "       'wtf', 'wtffff', 'wtfork', 'wth', 'wthrcom', 'wti', 'wtoc11',\n",
       "       'wtol11toledo', 'wtopelius', 'wtov', 'wtov9', 'wtp',\n",
       "       'wtparethenews', 'wtpatriotsusa', 'wtpblue', 'wtpearth',\n",
       "       'wtrf7news', 'wttw', 'wtwonews', 'wtyppod', 'wuhan', 'wurst',\n",
       "       'wusa9', 'wut', 'wutangkids', 'wv', 'wva', 'wvamwater', 'wvemd',\n",
       "       'wvnews247', 'wvns59news', 'wvtm', 'wvtm13', 'wvupress', 'ww1',\n",
       "       'ww2', 'ww2_gal', 'ww3', 'wwfrbeffzq', 'wwg1wga', 'wwi', 'wwii',\n",
       "       'wwiii', 'wwj950', 'wwjtraffic', 'wwltv', 'wwmtnews', 'wwomenl',\n",
       "       'www', 'wxbrenn', 'wxbywilliams', 'wxii', 'wxix', 'wxopwyu',\n",
       "       'wxyzdetroit', 'wyffnews4', 'wykoleił', 'wylieguide', 'wymondham',\n",
       "       'wynnws', 'wyntonmarsalis', 'wyntre999', 'wyoming', 'wytv', 'x2',\n",
       "       'x22report', 'x3', 'x3r0gxx', 'x4eileen', 'x5', 'xashe10x',\n",
       "       'xaviaerd', 'xhnews', 'xi', 'xin97558681', 'xinhua', 'xnewsalerts',\n",
       "       'xrp_dex', 'xrpking09531420', 'xtrabiggg', 'xx', 'xx17965797n',\n",
       "       'xxx', 'xxxalbertopelle', 'xyuknw', 'y2ymlfnnyc', 'ya',\n",
       "       'yaboytroy27', 'yabutaleb7', 'yaelbraunpivet', 'yah', 'yahoo',\n",
       "       'yahoofinance', 'yahoolife', 'yahoolifeuk', 'yahoonews',\n",
       "       'yahoonewsau', 'yall', 'yamiche', 'yank', 'yap', 'yappydagger',\n",
       "       'yard', 'yardmaster', 'yardtour', 'yarosisnancy', 'yawn', 'yay',\n",
       "       'yaystack', 'ychgnmcayy', 'ydanasmithdutra', 'ydsa', 'ydsagt',\n",
       "       'ye', 'yea', 'yeah', 'yeap', 'year', 'yearly', 'yeh', 'yell',\n",
       "       'yellen', 'yellow', 'yellowpaiges', 'yellowserf',\n",
       "       'yellowstone1776', 'yep', 'yer', 'yes', 'yesitsm97576245',\n",
       "       'yesseniaamarys', 'yessfun', 'yest', 'yesterday', 'yesteryear',\n",
       "       'yet', 'yetiglass25', 'yfblanchet', 'yield', 'yikes', 'yinzp365',\n",
       "       'yitgordon', 'ylsjrlezst', 'yo', 'yogurt', 'yohiobaseball',\n",
       "       'yoittsconnie', 'yok', 'yolalindayola', 'yolo304741', 'yoo',\n",
       "       'yoooo', 'york', 'yorkers', 'yosoychávez', 'yost', 'young',\n",
       "       'youngkin', 'youngstown', 'youranonnews', 'yourboypat47',\n",
       "       'yourcallradio', 'yourchoicenews', 'youre', 'youredailynews',\n",
       "       'yourlibertynews', 'yourmomswatchin', 'yournews15', 'yournewsnet',\n",
       "       'youtbe', 'youth', 'youtu', 'youtube', 'youtubers', 'youtubeより',\n",
       "       'yp6vcceqtb', 'ypgkm0fzji', 'ypsigal', 'ypu', 'yr', 'yrs',\n",
       "       'yruwhining', 'yserashenai', 'yt', 'yt9pcrulxo', 'yuge', 'yuhline',\n",
       "       'yuji', 'yukon', 'yup', 'yves', 'yvespdb', 'z_tarot',\n",
       "       'zac_unchained', 'zacakamadu', 'zacchino', 'zachary38090591',\n",
       "       'zachary_cohen', 'zachgorchow', 'zachsteinbrook', 'zack',\n",
       "       'zackbornstein', 'zacksjerryrig', 'zagozana', 'zainsville',\n",
       "       'zaizul_abidin', 'zaleskiluke', 'zany', 'zardoz420wpn',\n",
       "       'zazzybritches', 'ze', 'zealand', 'zealous', 'zeedlethorp',\n",
       "       'zeekarkham', 'zeelandcap', 'zegdie', 'zehirli', 'zein',\n",
       "       'zekegary2', 'zelenskaua', 'zelensky', 'zelenskyy', 'zelenskyyua',\n",
       "       'zelinsky', 'zemmoureric', 'zenkus', 'zenkuswatch', 'zenoc_oshits',\n",
       "       'zephyr21848384', 'zero', 'zerohedge', 'zerosum24',\n",
       "       'zerowarningshot', 'zerowaste', 'zeteamd', 'zg4ever',\n",
       "       'zhang_yueting', 'zhangxinyue', 'zhirji28', 'zhrvtenghl',\n",
       "       'zibalady1', 'zilch', 'zioevil', 'ziondaughter', 'zionist', 'zip',\n",
       "       'zipillinois', 'zito', 'zlqhj05tmc', 'zntf3ro9bn', 'zombaplateau',\n",
       "       'zombie', 'zombywoof2022', 'zone', 'zooeyd1', 'zoom', 'zoomarang',\n",
       "       'zoomer', 'zoomerwaffen', 'zoomin', 'zooming', 'zoophilewins',\n",
       "       'zsnakeplissken', 'zsuzsa', 'zt', 'zt_followers', 'zug',\n",
       "       'zugly747', 'zugs', 'zvr09', 'zwv134', 'zzt', 'ça', 'énième',\n",
       "       'équipes', 'était', 'état', 'états', 'étatsunis', 'été',\n",
       "       'éventuellement', 'éviter', 'önünde', 'über', 'και', 'στο',\n",
       "       'алабаме', 'американской', 'генеральный', 'грузовой', 'давно',\n",
       "       'директор', 'до', 'ещё', 'жд', 'железнодорожный', 'за', 'иald',\n",
       "       'испытания', 'как', 'какой', 'компании', 'лексингтоне', 'марта',\n",
       "       'несколько', 'огайо', 'один', 'они', 'перед', 'по', 'поезд',\n",
       "       'провел', 'пути', 'рельсов', 'ремонтировали', 'сошел', 'сошёл',\n",
       "       'субботу', 'счёту', 'там', 'того', 'транспортной', 'часов', 'штат',\n",
       "       'яepublican', 'אלבמה', 'הפעם', 'أوهايو', 'از', 'اسپرینگفیلد',\n",
       "       'القضبان', 'اوهایو', 'این', 'بار', 'جنوبی', 'خارج', 'خروج', 'در',\n",
       "       'دیگر', 'ریل', 'شد', 'عن', 'في', 'قطار', 'قطارهای', 'نورفولک',\n",
       "       'ولاية', 'یکی', 'นอกจากน', 'แต', 'ℂℍ𝔼ℝℕ𝕆𝔹𝕐𝕃', 'うひーメモ', 'が謝罪し',\n",
       "       'が議会で証言する中', 'が重要なセクターにとっては他人事では無いんだよね', 'この部分',\n",
       "       'その米国の物流ですが___________', 'またしても', 'またまた',\n",
       "       'アメリカ三大ネットワークのテレビニュースを数時間遅れで視聴できる時代となっている', 'ウェストバージニア州', 'オハイオ',\n",
       "       'オハイオの脱線事故', 'オハイオ州でノーフォーク', 'オハイオ州の化学物質流出後',\n",
       "       'オハイオ州クリーブランドで火曜日の早朝', 'オハイオ州スプリングフィールドでの', 'オバマ政権で強化された鉄道の安全対策が',\n",
       "       'サザン', 'サザンceo', 'サザンの', 'サザンのオハイオ州の列車の脱線事故で鉄道センサーが脚光を浴びる',\n",
       "       'サザンの列車が脱線した', 'サザンの従業員が死亡した', 'サザンの貨物列車が脱線し', 'サザントレインが脱線',\n",
       "       'スプリングフィールドでノーフォーク', 'ダンプカーが衝突し', 'トランプ政権下でロビーイングで弱体化された件は注目で',\n",
       "       'ノーフォーク', 'ハエのように落ちる', 'バージニア州',\n",
       "       'ペンシルバニア州知事がオハイオ州南部のノーフォーク列車の墜落事故を非難し', '东巴勒斯坦', '事件沒人受傷',\n",
       "       '今年だけで脱線事故は10件以上', '今度はアラバマ州でも脱線事故って', '但有當地民眾表示', '住民に避難するよう指示',\n",
       "       '住民は屋内退避を命じられました', '俄亥俄州', '俄亥俄州诺福克南部货运列车脱轨后',\n",
       "       '先月にはイーストパレスチンでより大規模な脱線事故が発生した', '先月頭と今月頭にオハイオ州で脱線事故起こしたのに続き',\n",
       "       '出軌', '出轨是常态', '刑事告発を行う', '列車と', '列車の脱線に関する上院公聴会で変更を約束', '列車脱線事故',\n",
       "       '列車脱線事故に関する上院公聴会で変更を約束', '別のノーフォーク', '別の列車が脱線', '卐army', '卐s',\n",
       "       '危険物処理班が現場に急行', '又一起', '另一列诺福克南方货运列车出轨', '另一列诺福克南部货运列车出轨了',\n",
       "       '同じ鉄道会社の貨物列車がまた脱線事故', '呼籲市民就地避難', '堆疊在路軌旁', '墮軌', '大規模な', '安全',\n",
       "       '导致北卡罗来纳州延误', '就在几天前', '居民被告知就地避难', '布蒂吉格说美国一年有一千', '幼児教育など', '意外',\n",
       "       '投稿時間', '據報這列載貨火車共有212卡車廂', '斯普林菲尔德', '日ミシシッピ川にフッ化水素酸が流出',\n",
       "       '汚染の規模が大きくて話題になっているけど', '海外科学', '火曜日',\n",
       "       '環境保護庁はオハイオ州の有毒列車事故による汚染廃棄物の出荷の一時停止を命令', '當中20節出軌',\n",
       "       '當局稱車上並無載有有毒化學物', '皮特', '看到有毒化學物品處理組的車輛出現', '米では年平均1700件の脱線事故が発生',\n",
       "       '羊民们别瞎担心了', '美国', '美国俄亥俄州再发生列车脱轨事故', '肇事的火車同樣屬於norfolk', '脱軌',\n",
       "       '脱轨', '自分たちが引き起こした混乱の後始末の代償を払うことになる', '诺福克南方声称居民没什么好担心的',\n",
       "       '这次是在北卡罗来纳州', '速報', '週末には', '都散了吧', '鐵路', '铁路', '阿拉巴马州',\n",
       "       '附近居民被要求就地避难', '열차', '탈선', '𝐃𝐢𝐬𝐚𝐬𝐭𝐞𝐫', '𝐄𝐚𝐬𝐭', '𝐄𝐱𝐩𝐨𝐬𝐢𝐧𝐠',\n",
       "       '𝐇𝐄𝐀𝐃𝐋𝐈𝐍𝐄𝐒', '𝐇𝐞𝐚𝐫𝐭', '𝐉𝐚𝐜𝐤', '𝐊𝐃𝐊𝐀', '𝐍𝐄𝐖𝐒𝐑𝐀𝐃𝐈𝐎', '𝐍𝐨𝐭',\n",
       "       '𝐏𝐚𝐥𝐞𝐬𝐭𝐢𝐧𝐞', '𝐓𝐫𝐚𝐢𝐧', '𝐕𝐓', '𝐛𝐮𝐲𝐢𝐧𝐠', '𝐢𝐭', '𝐰𝐢𝐭𝐡', '𝔸𝕄𝔼ℝ𝕀ℂ𝔸𝕊',\n",
       "       '𝗖𝗮𝗻', '𝗗𝗜𝗦𝗣𝗢𝗦𝗔𝗟', '𝗗𝗢', '𝗦𝗘𝗔𝗥𝗖𝗛', '𝗧𝗲𝘅𝗮𝘀', '𝗪𝗘𝗟𝗟𝗦', '𝘀𝗮𝘆', '𝘄𝗵𝘆',\n",
       "       '𝘆𝗼𝘂'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can get the whole vocabulary, sorted alphabetically\n",
    "tf_vectorizer.get_feature_names_out()[-1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also go in reverse: given a word, we can figure out which column index it corresponds to. To do this, we use the `vocabulary_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3596"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vectorizer.vocabulary_['car']\n",
    "# .vocabulary_ get the word index in the vocabulary\n",
    "# IN OTHER WORDS: what column do I have to look at to find the word car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out what the raw counts are for the 0-th post as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85220, 23665)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf[0].toarray()\n",
    "\n",
    "# I can index in to the zeroeth row of the matrix and convert into an array, I am anazlizing just the zeroeth row:\n",
    "# feature vector representation for the zeroeth document: \n",
    "# We should be able to write code to know what word corresponds each non zero number there\n",
    "\n",
    "# tf_vectorizer.get_feature_names_out()[(tf[0].toarray() != 0)[0]] # LIKE THIS!\n",
    "\n",
    "# run a for loop that goes thru the entries, and check to which indeces this corresponds in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitting procedure determines the every topic's distribution over words; this information is stored in the `components_` attribute. There's a catch: we actually have to normalize to get the probability distributions (without this normalization, instead what the model has are pseudocounts for how often different words appear per topic)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit an LDA model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=5, random_state=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now LDA. This will take a while ong\n",
    "\n",
    "num_topics = 5\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 23665)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape\n",
    "# The shaope of this is 16 by 1000, 10 topics and 1000 words\n",
    "# These are not probability distributions yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([212806.09756, 183229.36112, 226403.02355, 249360.46048,\n",
       "       196956.05727])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If we access: sum ACROSS COLUMNS (get rid of columns), I get 6 numbers: a sum for each row.\n",
    "# They dont sum uop to 1, because not probability distributions\n",
    "\n",
    "# .components_ is a raw count histogram, but why are there fractions?\n",
    "# Because of how it works: when it estimates how much a word belongs to a topic it does a prob assignment, not a deterministic assignment.\n",
    "lda.components_.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distributions = np.array([row / row.sum() for row in lda.components_])\n",
    "# We can get the proba distributions: take each row and divide by sum, then we get what was in the purple box in the lecture slide:\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 23665)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that each topic's word distribution sums to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_distributions.sum(axis=1)\n",
    "# then we have distributions: every row sums to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also print out what the probabilities for the different words are for a specific topic. This isn't very easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.      0.      0.      ... 0.00001 0.00001 0.00001]\n"
     ]
    }
   ],
   "source": [
    "print(topic_word_distributions[4])\n",
    "# lets look at 0th topic, it is saying that the 0th word appears 0.00011, word 1 appears with prob 0.00191\n",
    "# Lets view the data differently to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, usually people do something like looking at the most probable words per topic, and try to use these words to interpret what the different topics correspond to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[Topic 0]\n",
      "toxic : 0.022457791846392967\n",
      "ceo : 0.01909519770061295\n",
      "resident : 0.017347292501322095\n",
      "chemical : 0.016160113157076755\n",
      "epa : 0.013899063024322483\n",
      "via : 0.009893960751737617\n",
      "pay : 0.008269276069613581\n",
      "say : 0.008163893554262587\n",
      "senate : 0.008134594278846838\n",
      "tell : 0.007605117286419225\n",
      "congress : 0.006781515182579619\n",
      "order : 0.006306676179334696\n",
      "clean : 0.006226100852638858\n",
      "rail : 0.0062106600464542985\n",
      "crash : 0.006076923915123679\n",
      "site : 0.005999986680991488\n",
      "worker : 0.0059216764863732185\n",
      "animal : 0.005689835953484064\n",
      "company : 0.00549751936463235\n",
      "demand : 0.005352323604188377\n",
      "\n",
      "[Topic 1]\n",
      "derail : 0.041301288720680895\n",
      "another : 0.03180513917666337\n",
      "car : 0.026418350367137678\n",
      "derails : 0.022203092403190045\n",
      "hazardous : 0.019883504315243262\n",
      "material : 0.015784198361090688\n",
      "say : 0.013526961781535762\n",
      "springfield : 0.012509990870489616\n",
      "20 : 0.011057471751499764\n",
      "company : 0.010800731026674894\n",
      "alabama : 0.010715894642125327\n",
      "carry : 0.009308797231190406\n",
      "freight : 0.008695898991201418\n",
      "cargo : 0.008482252175005624\n",
      "place : 0.008091923299740364\n",
      "second : 0.007548877594248619\n",
      "county : 0.007522726704506382\n",
      "report : 0.007300660550027382\n",
      "month : 0.007231423910512517\n",
      "one : 0.007019434564381862\n",
      "\n",
      "[Topic 2]\n",
      "toxic : 0.024399433631634115\n",
      "chemical : 0.018556797501523944\n",
      "visit : 0.013117524876437109\n",
      "water : 0.012062389845854405\n",
      "say : 0.010973712439973757\n",
      "site : 0.00844431795530629\n",
      "biden : 0.007837146592019666\n",
      "town : 0.007748579055315083\n",
      "resident : 0.007402385870002908\n",
      "state : 0.007034684176878805\n",
      "federal : 0.007014036491315758\n",
      "release : 0.006943703134839403\n",
      "week : 0.006781308268996687\n",
      "epa : 0.006548100561755021\n",
      "air : 0.006254763588951667\n",
      "disaster : 0.0057565792934920115\n",
      "follow : 0.00573390107272309\n",
      "environmental : 0.005488453374713875\n",
      "response : 0.005398110140580032\n",
      "trump : 0.005321220579266696\n",
      "\n",
      "[Topic 3]\n",
      "trump : 0.018502068531496673\n",
      "biden : 0.013314674872412476\n",
      "people : 0.012179552557099477\n",
      "go : 0.009201033734639479\n",
      "get : 0.00865621546372934\n",
      "need : 0.007154970123360606\n",
      "say : 0.007040251673666007\n",
      "water : 0.006922296359066375\n",
      "disaster : 0.006480895117766147\n",
      "oh : 0.0057712524173923006\n",
      "buttigieg : 0.00563832997784419\n",
      "pete : 0.005540222921467095\n",
      "gop : 0.005460751698488064\n",
      "help : 0.005425819781321395\n",
      "regulation : 0.005069411637819183\n",
      "president : 0.005018373647870214\n",
      "blame : 0.004838856230112978\n",
      "republican : 0.004819385292446188\n",
      "like : 0.004649180333924542\n",
      "would : 0.004602272585002344\n",
      "\n",
      "[Topic 4]\n",
      "safety : 0.020854286101955024\n",
      "trump : 0.012096334684715907\n",
      "rail : 0.009905084947262426\n",
      "disaster : 0.009302870886439376\n",
      "regulation : 0.008105892834487346\n",
      "brake : 0.007738428284822817\n",
      "rule : 0.006814332639663208\n",
      "make : 0.0066215506193103195\n",
      "would : 0.0065813934167291074\n",
      "like : 0.006260418724992104\n",
      "railroad : 0.006026254068120426\n",
      "year : 0.005842938838686263\n",
      "derailment : 0.005839382265900528\n",
      "accident : 0.00567860726893937\n",
      "new : 0.005491509162680534\n",
      "cause : 0.004876663188700848\n",
      "ntsb : 0.004615220286816077\n",
      "happen : 0.004435757515635707\n",
      "buttigieg : 0.004175798070820346\n",
      "right : 0.004164262724009133\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we can sort the probability by big to small\n",
    "# for each topic, listing top 20 most popular words (by probability)\n",
    "\n",
    "num_top_words = 20\n",
    "\n",
    "def print_top_words(topic_word_distributions, num_top_words, vectorizer):\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    print('Displaying the top %d words per topic and their probabilities within the topic...' % num_top_words)\n",
    "    print()\n",
    "\n",
    "    for topic_idx in range(num_topics):\n",
    "        print('[Topic ', topic_idx, ']', sep='')\n",
    "        # This is the interesting part: take the topic_word_distributions only for the topic_idxth topic, and get the indexes sorted from highest to lowest\n",
    "        sort_indices = np.argsort(-topic_word_distributions[topic_idx])\n",
    "        # then for the range 0,num_words, print the vocab of each of the words and its entry in the topic_word_distribution matrix\n",
    "        for rank in range(num_top_words):\n",
    "            word_idx = sort_indices[rank]\n",
    "            print(vocab[word_idx], ':',\n",
    "                  topic_word_distributions[topic_idx, word_idx])\n",
    "        print()\n",
    "\n",
    "print_top_words(topic_word_distributions, num_top_words, tf_vectorizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do\n",
    "Present these results and then tweak:\n",
    "- Present this\n",
    "- Divid by govt people and public and present results\n",
    "- set a minimum frequency for the terms included and a max for vocabulary\n",
    "- Present overall topics\n",
    "- Divide the set by govt, media an public and redo analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `transform()` function to figure out for each document, what fraction of it is explained by each of the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider removing:\n",
    "terms_remove = ['train', 'https', 'rt', 'palestine', 'east', 'ohio', 'norfolk', 'southern']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = lda.transform(tf)\n",
    "# Ill get a low dimensional version of the data, the document topic matrix (10, 10)\n",
    "# each of the 10,000 represented as a distribution over 10 different topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The topic distribution of the 0th document is very small except for 0.96288 fior index 7\n",
    "# We saw from above, topic 7 is about religion, and we can check that document 0 is about religion\n",
    "doc_topic_matrix[0]\n",
    "\n",
    "\n",
    "# Note: for GMM when you do the .predict, itll give you k probabilites that sum to 1. \n",
    "# predict_proba of GMM \n",
    "# here: ... gives you a proba distr:\n",
    "# INTERPRETARATION DIFFERENT\n",
    "\n",
    "# FOR LDA: document can consist of a bunch of words, and different fraction of the words are truly in different topics: we get mixed membership, words are allowed to e in different topics\n",
    "# GMM: the above doesnt hold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix[0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this *could* be interpreted as a form of dimensionality reduction: document 0 is converted from its raw counts histogram representation to a 10-dimensional vector of probabilities, indicating estimated memberships to the 10 different topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **--------------------- Stop here by now ---------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word clouds\n",
    "\n",
    "Here's a fancier way to visualize. This requires installation of the wordcloud package:\n",
    "\n",
    "```\n",
    "pip install wordcloud\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "num_max_word_cloud_words = 100\n",
    "\n",
    "vocab = tf_vectorizer.get_feature_names()\n",
    "num_topics = len(topic_word_distributions)\n",
    "\n",
    "for topic_idx in range(num_topics):\n",
    "    wc = WordCloud(max_words=num_max_word_cloud_words)\n",
    "    wc.generate_from_frequencies(dict(zip(vocab, topic_word_distributions[topic_idx])))\n",
    "    plt.figure()\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.title('Topic %d' % topic_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing co-occurrences of words\n",
    "\n",
    "Here, we count the number of newsgroup posts in which two words both occur. This part of the demo should feel like a review of co-occurrence analysis from earlier in the course, except now we use scikit-learn's built-in CountVectorizer. Conceptually everything else in the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = 'year'\n",
    "word2 = 'team'\n",
    "\n",
    "word1_column_idx = tf_vectorizer.vocabulary_[word1]\n",
    "word2_column_idx = tf_vectorizer.vocabulary_[word2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(tf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf[:, word1_column_idx].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_with_both_word1_and_word2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we compute the log of the conditional probability of word 1 appearing given that word 2 appeared, where we add in a little bit of a fudge factor in the numerator (in this case, it's actually not needed but some times you do have two words that do not co-occur for which you run into a numerical issue due to taking the log of 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.1\n",
    "np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute log of prob of see one word given you see another, using count vectorizer!\n",
    "# Same content as in hw , but with a different tool\n",
    "# This uses np.arrays instead of counters. Needs to kep track of the indexes.\n",
    "# Understad how countvectorizer works\n",
    "\n",
    "def prob_see_word1_given_see_word2(word1, word2, vectorizer, eps=0.1):\n",
    "    word1_column_idx = vectorizer.vocabulary_[word1]\n",
    "    word2_column_idx = vectorizer.vocabulary_[word2]\n",
    "    documents_with_word1 = (tf[:, word1_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_word2 = (tf[:, word2_column_idx].toarray().flatten() > 0)\n",
    "    documents_with_both_word1_and_word2 = documents_with_word1 * documents_with_word2\n",
    "    return np.log2((documents_with_both_word1_and_word2.sum() + eps) / documents_with_word2.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(word1), type(word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic coherence\n",
    "\n",
    "The below code shows how one implements the topic coherence calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the cell below, vectorizer.get_feature_names() is just tokenizing the text, \n",
    "# eliminating stopwords, eliminating some other words, and then giving you back\n",
    "# a list of the tokens in string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic coherence\n",
    "\n",
    "def compute_average_coherence(topic_word_distributions, num_top_words, vectorizer, verbose=True):\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    average_coherence = 0\n",
    "    # foor loop thru different topics, for each topic double nested for loop, going thru each entry, ordering matters\n",
    "    # make sure words arent the same, then compute the log(prob)\n",
    "    # add a bunhc of them and divide by total number of topics\n",
    "    for topic_idx in range(num_topics):\n",
    "        if verbose:\n",
    "            print('[Topic ', topic_idx, ']', sep='')\n",
    "        \n",
    "        sort_indices = np.argsort(topic_word_distributions[topic_idx])[::-1]\n",
    "        coherence = 0.\n",
    "        for top_word_idx1 in sort_indices[:num_top_words]:\n",
    "            word1 = vocab[top_word_idx1]\n",
    "            for top_word_idx2 in sort_indices[:num_top_words]:\n",
    "                word2 = vocab[top_word_idx2]\n",
    "                if top_word_idx1 != top_word_idx2:\n",
    "                    coherence += prob_see_word1_given_see_word2(word1, word2, vectorizer, 0.1)\n",
    "        \n",
    "        if verbose:\n",
    "            print('Coherence:', coherence)\n",
    "            print()\n",
    "        average_coherence += coherence\n",
    "    average_coherence /= num_topics\n",
    "    if verbose:\n",
    "        print('Average coherence:', average_coherence)\n",
    "    return average_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_coherence(topic_word_distributions, num_top_words, tf_vectorizer, True)\n",
    "# These are negative, the highest possible is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of unique words\n",
    "\n",
    "The below code shows how one implements the number of unique words calculation from lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# brute force implementation\n",
    "# nothing clever to speed upmcalculation\n",
    "\n",
    "#fro loop for each topic\n",
    "# For loop over each top word\n",
    "# another for loop for other topics\n",
    "# lok for all other top words in those other topics\n",
    "# check for uniqueness\n",
    "\n",
    "def compute_average_num_unique_words(topic_word_distributions, num_top_words, vectorizer, verbose=True):\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    num_topics = len(topic_word_distributions)\n",
    "    average_number_of_unique_top_words = 0\n",
    "    for topic_idx1 in range(num_topics):\n",
    "        if verbose:\n",
    "            print('[Topic ', topic_idx1, ']', sep='')\n",
    "        \n",
    "        sort_indices1 = np.argsort(topic_word_distributions[topic_idx1])[::-1]\n",
    "        num_unique_top_words = 0\n",
    "        for top_word_idx1 in sort_indices1[:num_top_words]:\n",
    "            word1 = vocab[top_word_idx1]\n",
    "            break_ = False\n",
    "            for topic_idx2 in range(num_topics):\n",
    "                if topic_idx1 != topic_idx2:\n",
    "                    sort_indices2 = np.argsort(topic_word_distributions[topic_idx2])[::-1]\n",
    "                    for top_word_idx2 in sort_indices2[:num_top_words]:\n",
    "                        word2 = vocab[top_word_idx2]\n",
    "                        if word1 == word2:\n",
    "                            break_ = True\n",
    "                            break\n",
    "                    if break_:\n",
    "                        break\n",
    "            else:\n",
    "                num_unique_top_words += 1\n",
    "        if verbose:\n",
    "            print('Number of unique top words:', num_unique_top_words)\n",
    "            print()\n",
    "\n",
    "        average_number_of_unique_top_words += num_unique_top_words\n",
    "    average_number_of_unique_top_words /= num_topics\n",
    "    \n",
    "    if verbose:\n",
    "        print('Average number of unique top words:', average_number_of_unique_top_words)\n",
    "    \n",
    "    return average_number_of_unique_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_average_num_unique_words(topic_word_distributions, num_top_words, tf_vectorizer, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting average coherence vs k (number of topics), and average number of unique words vs k\n",
    "\n",
    "Next, we plot the average coherence vs k and the average number of unique words vs k. Note that these are *not* the only topic model metrics available (much like how CH index is not the only metric available for clustering).\n",
    "\n",
    "For both average coherence and average number of unique words, we would like these to be high. In this particular example, it turns out k=2 yields very high values for both but if you look at the topics learned for k=2, they are qualitatively quite bad (basically one topic is gibberish and the other is everything else!). This observation reinforces the important idea that while there exist topic modeling metrics (such as coherence and number of unique words), you should definitely still look at what the learned topics are (e.g., by printing the top words per topic) to help decide on what value of k to use.\n",
    "\n",
    "Also, keep in mind that the results are in some sense \"noisy\" since the LDA fitting procedure is random. We're choosing a specific `random_state` seed value but if we try different random seeds, we can get different results. For simplicity, because LDA fitting is quite computationally expensive, we are *not* doing what we did with GMM's where we did many different random initializations. Thus, the conclusions we draw regarding how many topics to use might actually be different with different random initializations.\n",
    "\n",
    "At least according to average coherence and average number of unique words for the random seed we use, the results below suggests that using k=4 yields average coherence and average number of unique words that are still reasonably high (as good as or almost as good as the k=2 result), and inspecting the topics learned for k=4, they are definitely more interesting than the ones learned for k=2.\n",
    "\n",
    "From qualitatively looking at topics, the k=5, k=6, and k=7 topics also look decent. When k gets too large (e.g., k=10), there start to be topics that look like there might be too much overlap (such as multiple topics that seem to be about computers).\n",
    "\n",
    "Note that one of the things to look out for is whether there are \"stable\" topics, where even for slightly different values of k and different random initializations, LDA keeps finding specific topics (e.g., one on gibberish, one on numbers, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = range(2, 11)\n",
    "avg_coherences = []\n",
    "avg_num_unique_words = []\n",
    "\n",
    "for k in k_values:\n",
    "    lda_candidate = LatentDirichletAllocation(n_components=k, random_state=0)\n",
    "    lda_candidate.fit(tf)\n",
    "    topic_word_distributions = np.array([row / row.sum() for row in lda_candidate.components_])\n",
    "    print('-' * 80)\n",
    "    print('Number of topics:', k)\n",
    "    print()\n",
    "    print_top_words(topic_word_distributions, num_top_words, tf_vectorizer)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    avg_coherences.append(compute_average_coherence(topic_word_distributions, num_top_words, tf_vectorizer, False))\n",
    "    avg_num_unique_words.append(compute_average_num_unique_words(topic_word_distributions, num_top_words, tf_vectorizer, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, avg_coherences)\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Average coherence')\n",
    "\n",
    "# For differnt number of topics, I fitted a different LDA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k_values, avg_num_unique_words)\n",
    "plt.xlabel('Number of topics')\n",
    "plt.ylabel('Average number of unique words')\n",
    "\n",
    "# For different topic models, the average number of unique words\n",
    "# We want coherence to be high and unique words to be high\n",
    "\n",
    "# Number of topic = 2, gets me high coherence and hogh unique words. BUT, for model 2, there are 2 topics: garbage and not garbage!\n",
    "# that doesnt help a lot\n",
    "\n",
    "# So, you want to tolerate a lower value for one of the metrics, but look at the actual topics to interpret what is going on\n",
    "# If we see resutlts of 4, starts to make more ssense but feels like still low\n",
    "# go to 5, starts to make a bit more sense\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.11 ('UDA_RIG')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1a526087eeeed7e3a2dd7dbe3d8582928a2e8e80d21671a2817819bcd9f426f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

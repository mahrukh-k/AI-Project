{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning (data source 1)\n",
    "\n",
    "**This code cleans up the tweets that come from data set no. 1, and prepares them to be merged to data set no. 2**  \n",
    "\n",
    "**This code will not run, as the raw data is not contained in the repository. The files were too large to host on Github; we are hosting only the cleaned data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('raw_data/DatasetOne.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with only nulls\n",
    "null_col_bool = [i for i in [data_1.isna().sum() != len(data_1)][0]]\n",
    "cols_keep = data_1.columns[null_col_bool]\n",
    "data_1 = data_1.loc[:, cols_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop this columns because all values are zero (these are related to Instagram and Linkedin attributes, and we ont have Instagram or Linknedin data)\n",
    "cols_drop = [col for col in data_1.select_dtypes(include=['int', 'float']) if data_1[col].sum() == 0]\n",
    "data_1.drop(columns = cols_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns because all entries are 0\n",
    "0 == data_1['Is Syndicated'].sum() == data_1['Linkedin Sponsored'].sum() == data_1['Starred'].sum() == data_1['Checked'].sum()\n",
    "\n",
    "# Drop columns becuase they only have one unique value (True for example)\n",
    "1 == data_1['Total Monthly Visitors'].nunique() == data_1['Content Source'].nunique() == data_1['Content Source Name'].nunique() == data_1['Page Type Name'].nunique() == data_1['Pub Type'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non useful columns \n",
    "non_useful_cols = ['Avatar', 'Latitude', 'Longitude', 'Total Monthly Visitors', 'Content Source', 'Is Syndicated', 'Linkedin Sponsored', 'Starred', 'Content Source Name', 'Page Type Name', 'Pub Type', 'Checked']\n",
    "data_1.drop(columns = non_useful_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2 = pd.read_csv('raw_data/DatasetTwo.csv')\n",
    "data_2 = data_2.loc[:, cols_keep]\n",
    "data_2.drop(columns = cols_drop, inplace=True)\n",
    "data_2.drop(columns = non_useful_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets from data_1 and data_2\n",
    "data_3 = pd.concat([data_2, data_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(814103, 753791, 814102)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_3), data_3.Title.duplicated().sum(), data_3['Query Id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Query Id', 'Query Name', 'Date', 'Title', 'Url', 'Domain', 'Sentiment',\n",
       "       'Page Type', 'Language', 'Country Code', 'Continent Code', 'Continent',\n",
       "       'Country', 'City Code', 'Account Type', 'Added', 'Author', 'City',\n",
       "       'Entity Info', 'Expanded URLs', 'Full Name', 'Full Text', 'Gender',\n",
       "       'Hashtags', 'Impact', 'Impressions', 'Interest', 'Location Name',\n",
       "       'Media URLs', 'Mentioned Authors', 'Original Url', 'Professions',\n",
       "       'Resource Id', 'Short URLs', 'Thread Author', 'Thread Created Date',\n",
       "       'Thread Entry Type', 'Thread Id', 'Twitter Author ID',\n",
       "       'Twitter Followers', 'Twitter Following', 'Twitter Reply Count',\n",
       "       'Twitter Reply to', 'Twitter Retweet of', 'Twitter Retweets',\n",
       "       'Twitter Likes', 'Twitter Tweets', 'Twitter Verified', 'Updated',\n",
       "       'Reach (new)', 'Engagement Type', 'Region', 'Region Code',\n",
       "       'Weblog Title'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3.drop_duplicates(subset = ['Title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60312"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classificaiton of accounts into media, government and public was an interative and slightly manual process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_news_org = [i for i in data_3['Author'][(data_3['Account Type'] == 'organisational') & (data_3['Twitter Verified'] == True)].value_counts().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_verified = [i for i in data_3['Author'][(data_3['Account Type'] == 'individual') & (data_3['Twitter Verified'] == True)].value_counts().index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USing regular expression to classify the greatest number of tweeter handles into media accounts\n",
    "import re\n",
    "count = 0\n",
    "news_org_list = []\n",
    "check_by_hand = []\n",
    "pattern = re.compile(r'news|abc|cnn|cbs|reuters|AP|times|fox|post|nbc|cnbc|bloomberg|gazette|hours|politico|cspan|politics|msn|journal|Tribune|daily|wsj|yahoo|tv|sun|time|radio|chronicle|herald|today', re.IGNORECASE) \n",
    "for item in potential_news_org:\n",
    "    if pattern.search(item):\n",
    "        news_org_list.append(item)\n",
    "    else:\n",
    "        check_by_hand.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MK NEWS ACCOUNTS\n",
    "mk_news_org = pd.read_csv('raw_data/news_accounts_dataset3.csv')\n",
    "mk_news_org = mk_news_org.SENDER.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_orgs_list2 = []\n",
    "check_by_hand_2 = []\n",
    "for account in check_by_hand:\n",
    "    if account in mk_news_org:\n",
    "      news_orgs_list2.append(account)\n",
    "    else:\n",
    "       check_by_hand_2.append(account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USing regular expression to classify the greatest number of tweeter handles into governemnt accounts\n",
    "political = []\n",
    "check_by_hand_3 = []\n",
    "pattern = re.compile(r'GOP|Gov|Senate|Senator|Rep', re.IGNORECASE) \n",
    "for item in check_by_hand_2:\n",
    "    if pattern.search(item):\n",
    "        political.append(item)\n",
    "    else:\n",
    "        check_by_hand_3.append(item)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "political + ['RudyGiuliani', 'USDOT', 'ChineseEmbinHU', 'TeamCavuto', 'TheDemCoalition', 'Ohio_OCJS', 'SenSchumer', 'FEMAregion3', 'EPAregion3', 'FDA_ORA']\n",
    "activists = ['PghProtests', 'CleanAirMoms', 'POGOwatchdog', 'genzforchange', 'PatrioticMills', 'TheUSASingers', 'OhioFarmBureau', 'gofundme', 'RevJJackson', 'AAF']\n",
    "other = ['RedCross', 'scifri', 'AmChemistry', 'RedCrossNOH', 'GoyaFoods']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_orgs_list3 = [i for i in check_by_hand_3 if i not in (political + activists + other)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_org = news_org_list + news_orgs_list2 + news_orgs_list3\n",
    "news_org = list(set(news_org))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, from all other accounts (non organizational , verified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_verified = [i for i in data_3['Author'][(data_3['Account Type'] == 'individual') & (data_3['Twitter Verified'] == True)].value_counts().index]\n",
    "individual_verified = [i for i in individual_verified if i not in (news_org + political + activists + other)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "count = 0\n",
    "news_ind_list = []\n",
    "check_by_hand_ind = []\n",
    "pattern = re.compile(r'news|abc|cnn|cbs|reuters|AP|times|fox|post|nbc|cnbc|bloomberg|chron|gazette|hours|politico|cspan|politics|msn|journal|Tribune|daily|wsj|yahoo|tv|sun|time|radio|chronicle|herald|today', re.IGNORECASE) \n",
    "for item in individual_verified:\n",
    "    if pattern.search(item):\n",
    "        news_ind_list.append(item)\n",
    "    else:\n",
    "        check_by_hand_ind.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news_org + news_ind_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "political_ind = []\n",
    "check_by_hand_ind2 = []\n",
    "pattern = re.compile(r'GOP|Gov|Senate|Senator|Rep|Sen|EPA', re.IGNORECASE) \n",
    "for item in check_by_hand_ind:\n",
    "    if pattern.search(item):\n",
    "        political_ind.append(item)\n",
    "    else:\n",
    "        check_by_hand_ind2.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics = political + political_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, reading the cleaned data from source 2\n",
    "MK_byhand = pd.read_csv('raw_data/check_by_hand_cleanMK.csv')\n",
    "MK_byhand.set_index('check_by_hand_ind2', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "MK_byhand.fillna(0, inplace=True)\n",
    "news_org_additional = MK_byhand.query('news == 1').index.to_list()\n",
    "government_additional = MK_byhand.query('political == 1').index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_by_hand_ind3 = [i for i in check_by_hand_ind2 if i not in (news_org_additional+government_additional)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "people = ['RobertKennedyJr', '_SemaHernandez_', 'CatoInstitute', 'ColMorrisDavis', 'mrtgr', 'LindseyBoylan', 'AJPennyfarthing', 'DrEricDing', 'mattkbh', 'AlBernstein', 'lootpress', 'DinahVP', 'iskandrah'\\\n",
    "          '_Zeets', 'vickorano', 'addedvalueth', 'IBDinvestors', 'SierraClub', 'StephenJ_Caruso', 'djrelentt', 'ErickFernandez', 'ceciliakcecilia', 'michaelmalice', 'court_bennett11'\\\n",
    "            'MsAConner', 'BrookingsEcon', 'MeredithLClark', 'drdavidmichaels', 'AWCities', 'laurenpeikoff', 'WajahatAli', 'MarkRuffalo', 'NewEconomics', 'EricMGarcia', 'GooRee']\n",
    "political = ['TPPatriots', 'PalimenoForGAD1', 'ForTexasHoujami', 'ProgressOhio', 'mh4oh', 'ElectMattDolan', 'Mike_Pence', 'Heritage', 'NorthCarolinaGP', 'HCEMA', 'MaxMillerOH', 'DaveYostOH', 'Miller_Congress'\\\n",
    "             'NewDemCoalition', 'DebbieLesko', 'ElissaSlotkin', 'PADEPSecretary', 'BriannaForCO', 'MikeStuartWV', 'justin4all2', 'voteSmitherman', 'CongressmanRaja', 'jessicalbenham'\\\n",
    "              'AlexPadilla4CA', 'RedState', 'DemSocialists', 'JaredEMoskowitz']\n",
    "journalists = [i for i in check_by_hand_ind3 if i not in (people + political)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_final = news + news_org_additional + journalists\n",
    "government_final = politics + government_additional + political\n",
    "people_final = people + activists + other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1821, 119, 48)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(news_final), len(government_final), len(people_final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parsing dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/9jkc42cj01gb_08dbbq9fcnw0000gn/T/ipykernel_96147/1648274890.py:7: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  tweet_agent[['Date', 'Time']] = tweet_agent['DATE'].str.split(' ', 1, expand=True)\n"
     ]
    }
   ],
   "source": [
    "tweet = pd.read_csv('raw_data/Tweet.csv')\n",
    "agent = pd.read_csv('raw_data/Agent.csv')\n",
    "hashtag = pd.read_csv('raw_data/Hashtag.csv')\n",
    "url = pd.read_csv('raw_data/Url.csv')\n",
    "agent.rename(columns = {'Node Label' : 'Sender'}, inplace=True)\n",
    "tweet_agent = pd.merge(tweet, agent, on=['Sender'])\n",
    "tweet_agent[['Date', 'Time']] = tweet_agent['DATE'].str.split(' ', 1, expand=True)\n",
    "tweet_agent.drop(columns=['Language_y','Location','DATE'], inplace = True)\n",
    "tweet_agent.rename(columns={'Node ID_x' : 'TWEET_ID', \n",
    "                            'Node ID_y' : 'AGENT_ID', \n",
    "                            'Language_x' : 'LANGUAGE',\n",
    "                            'conversation_id' : 'CONVERSATION_ID',\n",
    "                            'Latitude' : 'LATITUDE',\n",
    "                            'Longitude' : 'LONGITUDE',\n",
    "                            'Sender' : 'SENDER',\n",
    "                            'Date' : 'DATE',\n",
    "                            'Time' : 'TIME'\n",
    "                            }, inplace=True)\n",
    "tweet_agent['IS_RETWEET'] = tweet_agent['IS_RETWEET'].fillna(0)\n",
    "tweet_agent['HAS_URL'] = tweet_agent['HAS_URL'].fillna(0)\n",
    "tweet_agent['IS_REPLY'] = tweet_agent['IS_REPLY'].fillna(0)\n",
    "tweet_agent['IS_IN_REPLY_TO'] = tweet_agent['IS_IN_REPLY_TO'].fillna(0)\n",
    "tweet_agent['IS_QUOTE'] = tweet_agent['IS_QUOTE'].fillna(0)\n",
    "tweet_agent['IS_GEOTAGGED'] = tweet_agent['IS_GEOTAGGED'].fillna(0)\n",
    "tweet_agent['IS_NEWS_AGENCY'] = tweet_agent['IS_NEWS_AGENCY'].fillna(0)\n",
    "tweets_data = tweet_agent[tweet_agent.columns.difference(['CONVERSATION_ID','IS_GEOTAGGED','LATITUDE','LONGITUDE','AGENT_ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3['HAS_URL'] = data_3['Expanded URLs'].apply(lambda x: len(x.split()) if not isinstance(x, float) else 0)\n",
    "data_3['IS_REPLY'] = data_3['Engagement Type'].apply(lambda x : 1 if x == 'REPLY' else 0)\n",
    "data_3['IS_QUOTE'] = data_3['Engagement Type'].apply(lambda x : 1 if x == 'QUOTE' else 0)\n",
    "data_3['IS_RETWEET'] = data_3['Engagement Type'].apply(lambda x : 1 if x == 'RETWEET' else 0)\n",
    "data_3['Twitter Verified'] = data_3['Twitter Verified'].apply(lambda x: 1 if x == True else 0)\n",
    "drop_from_dataset3 = ['Query Id', 'Query Name', 'Title', 'Url', 'Domain', 'Page Type', 'Continent Code', 'Continent',\n",
    "       'Country', 'City Code', 'Added', 'City', 'Entity Info', 'Expanded URLs', 'Full Name', 'Location Name',\n",
    "       'Media URLs', 'Original Url', 'Professions',\n",
    "       'Resource Id', 'Short URLs', 'Thread Author', 'Thread Created Date',\n",
    "       'Thread Entry Type', 'Thread Id', 'Twitter Author ID', 'Twitter Reply to', 'Twitter Retweet of', 'Updated',\n",
    "       'Reach (new)', 'Region', 'Region Code', 'Weblog Title', 'Engagement Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are columns that we may need to bring back, as they are interesting\n",
    "drop_to_merge = ['Account Type', 'Sentiment', 'Hashtags', 'Impact', 'Interest', 'Mentioned Authors','Twitter Following',\t'Twitter Reply Count', 'Twitter Tweets', 'Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3.drop(columns=drop_from_dataset3, inplace=True)\n",
    "data_3.drop(columns=drop_to_merge, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/9jkc42cj01gb_08dbbq9fcnw0000gn/T/ipykernel_96147/3624984072.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_data['TIME'] = tweets_data['TIME'].apply(lambda x: re.findall(r'(.*)-', x)[0])\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "data_3['DATE'] = data_3['Date'].apply(lambda x: x.split()[0])\n",
    "data_3['TIME'] = data_3['Date'].apply(lambda x: x.split()[1])\n",
    "data_3['TIME'] = data_3['TIME'].apply(lambda x: re.findall(r'(.*)\\.', x)[0])\n",
    "tweets_data['TIME'] = tweets_data['TIME'].apply(lambda x: re.findall(r'(.*)-', x)[0])\n",
    "data_3.drop(columns='Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "data_3.rename(columns= {'Author' :  'SENDER', 'Country Code' : 'LOCATION', 'Language' : 'LANGUAGE', 'Full Text' : 'MESSAGE', \\\n",
    " 'Twitter Verified': 'IS_VERIFIED', 'Twitter Followers':'NUMBER_FOLLOWERS', 'Twitter Retweets':'RETWEET_COUNT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yv/9jkc42cj01gb_08dbbq9fcnw0000gn/T/ipykernel_96147/2771311703.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_data.drop(columns = 'TWEET_ID', inplace=True)\n",
      "/var/folders/yv/9jkc42cj01gb_08dbbq9fcnw0000gn/T/ipykernel_96147/2771311703.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tweets_data.drop_duplicates(subset = ['MESSAGE'], inplace = True)\n"
     ]
    }
   ],
   "source": [
    "tweets_data.drop(columns = 'TWEET_ID', inplace=True)\n",
    "tweets_data.drop_duplicates(subset = ['MESSAGE'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data_3, tweets_data], axis=0)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset=['MESSAGE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7406, 376, 77438)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, bring in the sender classifiers\n",
    "data['IS_MEDIA'] = data.SENDER.apply(lambda x : 1 if x in news_final else 0)\n",
    "data['IS_GOVT'] = data.SENDER.apply(lambda x : 1 if x in government_final else 0)\n",
    "data['IS_PEOPLE'] = data[['IS_GOVT', 'IS_MEDIA']].apply(lambda x : 1 if x[0] == 0 | x[1] == 0 else 0, axis = 1)\n",
    "data.SENDER[data.IS_MEDIA == 1].count(),  data.SENDER[data.IS_GOVT == 1].count(), data['IS_PEOPLE'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>SENDER</th>\n",
       "      <th>MESSAGE</th>\n",
       "      <th>Impressions</th>\n",
       "      <th>NUMBER_FOLLOWERS</th>\n",
       "      <th>RETWEET_COUNT</th>\n",
       "      <th>Twitter Likes</th>\n",
       "      <th>IS_VERIFIED</th>\n",
       "      <th>HAS_URL</th>\n",
       "      <th>...</th>\n",
       "      <th>IS_QUOTE</th>\n",
       "      <th>IS_RETWEET</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>IS_IN_REPLY_TO</th>\n",
       "      <th>IS_NEWS_AGENCY</th>\n",
       "      <th>TWEET_COUNT</th>\n",
       "      <th>IS_MEDIA</th>\n",
       "      <th>IS_GOVT</th>\n",
       "      <th>IS_PEOPLE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>USA</td>\n",
       "      <td>TurnKyBlue</td>\n",
       "      <td>RT @TristanSnell Get rid of train safety rules...</td>\n",
       "      <td>4414.0</td>\n",
       "      <td>4414.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>21:56:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jrbkjrbk</td>\n",
       "      <td>RT @realTuckFrumper Ohio Sues Norfolk Southern...</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>1011.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>21:56:03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>camoxendale</td>\n",
       "      <td>RT @SenJeffMerkley We need accountability from...</td>\n",
       "      <td>990.0</td>\n",
       "      <td>990.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>21:55:12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>NaN</td>\n",
       "      <td>strongfemchar</td>\n",
       "      <td>RT @nicksortor ðŸš¨ #BREAKING: The State of Ohio ...</td>\n",
       "      <td>64.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2023-03-14</td>\n",
       "      <td>21:54:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  LANGUAGE LOCATION         SENDER  \\\n",
       "0       en      USA     TurnKyBlue   \n",
       "1       en      NaN       jrbkjrbk   \n",
       "2       en      NaN    camoxendale   \n",
       "3       en      NaN  strongfemchar   \n",
       "\n",
       "                                             MESSAGE  Impressions  \\\n",
       "0  RT @TristanSnell Get rid of train safety rules...       4414.0   \n",
       "1  RT @realTuckFrumper Ohio Sues Norfolk Southern...       1011.0   \n",
       "2  RT @SenJeffMerkley We need accountability from...        990.0   \n",
       "3  RT @nicksortor ðŸš¨ #BREAKING: The State of Ohio ...         64.0   \n",
       "\n",
       "   NUMBER_FOLLOWERS  RETWEET_COUNT  Twitter Likes  IS_VERIFIED  HAS_URL  ...  \\\n",
       "0            4414.0            0.0            0.0          0.0      0.0  ...   \n",
       "1            1011.0            0.0            0.0          0.0      1.0  ...   \n",
       "2             990.0            0.0            0.0          0.0      0.0  ...   \n",
       "3              64.0            0.0            0.0          0.0      0.0  ...   \n",
       "\n",
       "   IS_QUOTE  IS_RETWEET        DATE      TIME IS_IN_REPLY_TO  IS_NEWS_AGENCY  \\\n",
       "0       0.0         1.0  2023-03-14  21:56:12            NaN             NaN   \n",
       "1       0.0         1.0  2023-03-14  21:56:03            NaN             NaN   \n",
       "2       0.0         1.0  2023-03-14  21:55:12            NaN             NaN   \n",
       "3       0.0         1.0  2023-03-14  21:54:42            NaN             NaN   \n",
       "\n",
       "   TWEET_COUNT  IS_MEDIA  IS_GOVT  IS_PEOPLE  \n",
       "0          NaN         0        0          1  \n",
       "1          NaN         0        0          1  \n",
       "2          NaN         0        0          1  \n",
       "3          NaN         0        0          1  \n",
       "\n",
       "[4 rows x 21 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the final cleaned data set for both LDA and sentiment analysis\n",
    "data.to_csv('data/data_tweet.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0       1845\n",
       "1.0        483\n",
       "2.0        354\n",
       "3.0        243\n",
       "4.0        159\n",
       "          ... \n",
       "781.0        1\n",
       "8806.0       1\n",
       "214.0        1\n",
       "2222.0       1\n",
       "1702.0       1\n",
       "Name: Twitter Likes, Length: 406, dtype: int64"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.query('IS_MEDIA == 1')['Twitter Likes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Intro_AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3eb23708133211dcd341a8aa35b90f924296bf92f5e22e78650e8d6fafcfea05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
